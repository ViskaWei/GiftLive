你现在要做的其实是 **action-value / 反事实结果预测**：
从“这条样本会不会送礼”升级到“**在这个时刻给了这个行动 a，会发生什么**”。这个目标非常容易踩坑——而且多数坑不是模型本身，是**数据口径、时间语义、选择偏差**。

下面给你一份“别再掉坑里”的指南（偏工程可落地 + 能支撑后续分配优化）。我会把你在短期收益里已经踩过的“公平对比/窗口/泄漏”教训也编进检查项里。

---

## 0) 先把“你在学什么”说清楚：相关性预测 vs 行动后果

### 两类问题完全不同

1. **相关性/识别金主**：
   “这个样本是否送礼/送多少”≈ 在现有策略日志里做监督学习。
   这类模型哪怕有偏，也常常能在同策略下离线排序很强（你们 Baseline 就是这样）。

2. **行动后果（你现在要的）**：
   要学的是
   [
   Q(x,a) = \mathbb{E}[r \mid x_t, a_t]
   ]
   甚至更进一步的 **增量效应**
   [
   \tau(x,a) = \mathbb{E}[r \mid x_t, do(a_t=a)] - \mathbb{E}[r \mid x_t, do(a_t=a_0)]
   ]

关键差别：日志里的 (a) 不是随机的，是“系统挑过的”。如果你不处理 **policy confounding**，模型很容易学到的是“系统更爱把高留存用户推给谁”，而不是“推给谁会让留存更高”。

### 一句硬规则

> **没有探索（随机化/已记录 propensity）时，离线学到的 retention Q(x,a) 更像“相关性评分”，只能当正则/护栏，不要当强优化目标直接拉满。**

---

## 1) 定义训练样本的“决策时刻”与时间轴：所有坑的源头

你必须把每条样本明确成一个 **decision event**：

* **t0**：决策发生时刻（比如用户一次 click 进入直播间的时间戳）
* **x(t0)**：t0 之前可见的状态/历史（严格 as-of）
* **a(t0)**：你做的行动（把 u 分配/曝光给 streamer s，或展示某个候选集排序）
* **y(t0→t0+H)**：结果窗口（留存、满意度代理、生态指标的变化）

> 你之前短期收益踩的“窗口”和“口径不一致”，本质都是 t0/x/a/y 没锁死。

**最常见的致命错误**：
你以为在预测 “action → consequence”，实际数据在回答 “consequence → action（或 post-treatment）”。

---

## 2) 留存标签怎么做才不泄漏：你们方案是对的，但要守住边界

你在实验设计里给的 retention 标签定义很标准，推荐先按这个做 MVP：

* `return_1d`: 在 ([day+1, day+2)) 是否有 click
* `return_7d`: 在 ([day+1, day+8)) 是否有 click
  并且 **test 末尾样本因未来不可见要 mask**（否则天然“未来缺失→当负例”会造假）。

### 留存标签最容易踩的 6 个坑（按危险程度排序）

1. **未来泄漏（hard leak）**
   任何特征直接/间接用到 ([t0, t0+H]) 甚至更后面的行为：

* “当天总观看时长/互动次数”如果是按自然日汇总，包含了 t0 之后的行为
* “近1天 click_count”如果计算时不做 as-of，会吃到 day+1 的 click（直接把 return_1d 喂进特征）

2. **窗口错位（off-by-one day）**
   你用 `click_day` 划天时，一定要统一时区/日界线。否则 return_1d 里混进同一天的点击，AUC 会虚高。

3. **样本删失（censoring）没处理**
   你已经写了：“Test 集最后几天无法算留存要 mask；7-7-7 切分下 test 只有前 1-2 天可算 return_7d”。这条必须硬执行，不做会让模型学到“在数据末尾的样本更不回访”（时间位置泄漏）。

4. **把 post-treatment 中介当特征**
   如果你的 action 是“给用户分配某个主播”，那么 **watch_time / engagement 在 t0 之后发生**，它们是 action 的结果（中介）。

* 你可以把它们当 **额外目标（labels）** 来预测
* 但不要在“决策时刻模型”里把 t0+Δ 的 watch_time 当特征，否则模型不是在预测 action 后果，而是在读答案

5. **多次曝光干扰（interference）**
   用户在未来 1 天内会看很多房间，你把 return_1d 归因到某一次曝光上，本质上是“最后一击/随机一击归因”。
   这在 MVP 阶段可以接受，但你要清楚：它会把 effect 稀释，模型上限下降。

6. **标签定义过宽/过窄导致不可学习**

* 过宽：return 定义成“任何 click”可能被“用户天然活跃度”主导，action 差异学不到。
* 过窄：只算回到同一主播，样本稀疏，冷启动更惨。
  你们用“回到任意直播间”的 return 作为第一版是合理折中。

---

## 3) 特征工程：最容易产生“隐形泄漏”的地方（尤其聚合特征）

### 核心原则：任何聚合都必须是 **as-of t0**

建议你给每条训练样本带 3 个字段并贯穿流水线：

* `t0`：决策时间戳（毫秒）
* `feature_cutoff_time = t0`：特征截止线
* `label_start_time = floor_day(t0)+1day`：标签起点（对于 return）

然后所有特征都写成：

> “在 `feature_cutoff_time` 之前发生的事件/统计”

### 你们项目里已经验证“交互历史特征极强”

像 `pair_gift_mean / pair_gift_sum / pair_gift_count` 在短期收益里是主导特征（pair_gift_mean 重要性远超第二名）。
这类特征对 retention 也可能很强，但要注意：**一定要按 t0 之前截断**，否则直接泄漏“本次/未来送礼”。

### 高危“看起来没问题”的泄漏样式（你可以当 code review checklist）

* `user_gift_mean_last7d`：如果 last7d 的定义是按自然日，且包含当天晚些时候的礼物 → 泄漏
* `streamer_exposure_today`：如果是 day-level 汇总，包含 t0 之后曝光 → 泄漏
* `is_new_user_today`：如果用“当天是否出现过 click”来定义，也可能把未来 click 吃进去（尤其跨时区）

**建议做一个自动化单测**：
随机抽 1k 条样本，把 `t0` 往前挪 24h 重新算特征，如果特征几乎不变，说明你可能在用 day-level 全量汇总（危险）。

---

## 4) 数据切分与对比：你已经踩过一次“不可比”，这次要一次性封死

你们在两段式 vs 直接回归上已经得到一个非常典型的教训：
**不同候选集/不同数据分布的对比会得出错误结论**，后来做了 click 全量的公平对比才关闭 DG1（Direct Reg Top-1% 54.5% vs Two-Stage 35.7%）。

在 retention/生态这里，“不可比”会以更隐蔽的方式出现：

### 必须遵守的 4 条对比规则

1. **同一候选集**：同一批 (u,s) 行为日志
2. **同一切分**：按天切分 + 同样的时间范围（你们用 7-7-7 by days）
3. **同一 label 可见性规则**：test 尾部 mask 的策略一致
4. **同一评估粒度**：不要一个用 impression-level AUC，另一个用 user-level retention rate 却直接比“谁更好”

---

## 5) “预测 action 后果”最容易被忽略的致命坑：选择偏差与政策学习

就算你完全没有时间泄漏，你仍然会遇到：

### 坑 A：日志策略 confounding（最常见）

系统更喜欢把某些主播给某类用户 → 你观察到 “给这个主播的人更回访”
模型会学成 “给这个主播能提升回访”，但这是错的。

**最实用的规避策略（按成本从低到高）**：

1. 把 retention 模型先当 **风险预警/护栏**：只做“不要把流量推到明显伤留存的动作”，而不是用它去 aggressively max。
2. 做 **分桶随机探索**：例如 1%-5% 流量做随机/ε-greedy，并把 propensity 记下来（后续才能 IPS/DR）。
3. 做 **uplift / CATE**：以随机桶为基础训练增量模型。

> 你们 Roadmap 里 Phase 3（OPE: IPS/DR）本来就是为了处理这类问题；如果未来要真用 retention 做优化目标，propensity 是刚需。

### 坑 B：post-treatment bias（把中介当特征）

上面提过：如果你在预测 “分配 → 次日回访”，就不能把“分配后的观看时长”当输入。否则你是在预测 “观看时长 → 回访”。这类模型离线 AUC 会非常漂亮，上线会非常脆。

### 坑 C：干预后分布漂移（policy shift）

你用旧策略日志训练的 (Q(x,a)) 只在旧策略覆盖的 action 分布上可靠。
一旦你用它来做分配优化，它会把系统带到 “训练没见过的区域”，效果可能崩。

**解法**：

* 限制动作改变幅度（例如只在 Top-N 候选里重排 + 频控/上限）
* 逐步加权（先小 w_usr）再放大
* 保留探索流量持续校正

---

## 6) 生态健康：不要硬把它做成逐条监督学习，先当“分配层约束/惩罚”更稳

你写的生态指标方向是对的：

* 按天算 `Gini(exposure)` / `Gini(revenue)`
* `tail_coverage`（收到曝光的主播占比）
  并把生态当约束项或惩罚项。

### 为什么不建议第一天就做“每条样本的 eco label”

因为生态指标本质上是 **全局耦合**：
某个动作对 Gini 的影响取决于“当天其他所有动作怎么分配”，这不是一个稳定的逐条监督信号。

### 推荐落地方式（从易到难）

1. **先算生态基线**：每天 Gini / tail_coverage（只观察）
2. **上线策略用约束优化/惩罚**：在 EV 排序上加：

   * 单主播曝光上限/频控
   * 长尾最低曝光
   * 过载惩罚
3. **用凹函数做“边际递减”**：
   把分配目标写成 (\max \sum_s g_s(V_s))，用 (g'_s(V_s)) 做影子价格（你们 Hub 里已经把它列为决策层核心方向）。

这一步的好处：你不需要给每条样本一个“eco label”，只需要在分配时算“边际收益折扣”。

---

## 7) 评估：先验证“可预测性”，再谈“可用于优化”

你们的实验计划里对 retention 的成功标准是：

* AUC(return_1d) > 0.60 才认为有可用信号
* 否则要重做标签/特征。

我建议把评估分成两层：

### Layer 1：纯预测质量（你现在就能做）

* AUC / PR-AUC（看类不平衡）
* 分天/分人群稳定性（防止只靠 “时间位置/活跃度”）
* 校准（ECE/Brier）——如果你要把它当 reward 权重进优化，校准很重要

### Layer 2：可用于分配优化的证据（需要探索或策略评估）

* 在线小流量 A/B（最靠谱）
* 或 OPE（IPS/SNIPS/DR），但前提是有 propensity / 随机桶。

---

## 8) 你可以直接照抄的“防踩坑清单”

### A. 时间与窗口（强制过）

* [ ] 每条样本明确 t0，并保存 `feature_cutoff_time=t0`
* [ ] 所有聚合特征都按 cutoff 截断（as-of join）
* [ ] return_1d/7d 的 label 窗口严格按定义构造
* [ ] test 尾部不可见 label 全部 mask（尤其 return_7d）
* [ ] 时区/日界线统一（否则 off-by-one）

### B. 反事实/行动后果（强制想一遍）

* [ ] action a 的定义是否真的是“系统可控的开关”？（比如曝光给哪个主播/排序）
* [ ] 训练数据里 action 是否有足够多样性？还是几乎确定性？
* [ ] 是否把 post-treatment 的 watch_time/engagement 当了特征？（如果是，删）
* [ ] 是否至少有一个探索桶记录 propensity？（没有就别把模型当可优化目标）

### C. 公平对比（你们已经踩过）

* [ ] 对比任何两个模型：同候选集、同切分、同 label mask、同评估口径
  （你们 DG1 的经验就是最好的警示牌）

### D. 生态指标（别误用）

* [ ] Gini / tail_coverage 先作为日级观测基线
* [ ] 分配层用约束/凹效用做边际递减，不强行监督学习“每条样本的生态标签”

---

## 9) 最推荐的落地路线（能最快产出“对分配有用”的东西）

按你们的 MVP-4.0 计划，我会建议这样走（顺序很重要）：

1. **先把 return_1d/return_7d 标签管道做对 + mask 做对**（这是地基）
2. 用最简单的模型验证“是否可预测”（LR/小树），把泄漏风险降到最低
3. 同时算生态基线（Gini/tail_coverage），把护栏指标固化
4. 如果 AUC 达标，再考虑更强模型/多任务；否则先回头改标签/特征，而不是加深网络
5. 真要用于分配优化：尽早要一个小探索桶 + propensity，不然你只能把 retention 当弱护栏

---

如果你愿意，我也可以把上面的清单进一步“工程化”成：

* 一份数据集 schema（每列的时间语义：pre/post、窗口定义）
* 一组自动化泄漏单测（比如：只用时间位置特征是否就能上 0.7 AUC；如果能，肯定泄漏/口径问题）
* 以及一个“把 r_rev / r_usr / r_eco 拼成可优化目标”的标准化归一化方案（避免 scale 互相碾压）。
