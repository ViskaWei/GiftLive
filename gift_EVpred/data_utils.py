#!/usr/bin/env python3
"""
Gift EV Prediction - Leakage-Free Data Utilities (Day-Frozen Version)
======================================================================

ç»Ÿä¸€çš„æ— æ³„æ¼æ•°æ®å¤„ç†æ¨¡å—ï¼Œæ‰€æœ‰ gift_EVpred å®éªŒå¿…é¡»ä½¿ç”¨æ­¤æ¨¡å—ã€‚

æ ¸å¿ƒè®¾è®¡ï¼šæŒ‰å¤©å†»ç»“ï¼ˆDay-Frozen / Day-Snapshotï¼‰
- å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
- å®Œå…¨ä¸ä¼šç”¨åˆ°"æœªæ¥"ï¼Œä½†ä¼šä¸¢æ‰"åŒä¸€å¤©æ›´æ—©å‘ç”Ÿçš„å†å²"ï¼ˆä¿å®ˆä½†å®‰å…¨ï¼‰
- è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—æ„é€ é€»è¾‘

Usage:
    from gift_EVpred.data_utils import (
        load_raw_data,
        prepare_dataset,
        get_feature_columns,
        verify_no_leakage
    )

    # æ ‡å‡†ç”¨æ³•
    train_df, val_df, test_df = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)

Author: Viska Wei
Date: 2026-01-18
Version: 2.0 (Day-Frozen)
"""

import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR = Path("/home/swei20/GiftLive")
DATA_DIR = BASE_DIR / "data" / "KuaiLive"
OUTPUT_DIR = BASE_DIR / "gift_EVpred"

# =============================================================================
# å¸¸é‡é…ç½®
# =============================================================================
SEED = 42
np.random.seed(SEED)

# æ³„æ¼ç‰¹å¾é»‘åå•ï¼ˆç»å¯¹ç¦æ­¢ä½¿ç”¨ï¼‰
FORBIDDEN_FEATURES = [
    'watch_live_time',      # ç»“æœæ³„æ¼ï¼šåŒ…å«æ‰“èµåçš„è§‚çœ‹æ—¶é•¿
    'watch_time_log',       # åŒä¸Š
    'watch_time_ratio',     # åŒä¸Š
]

# å¿…é¡»æ’é™¤çš„åˆ—ï¼ˆéç‰¹å¾ï¼‰
EXCLUDE_COLUMNS = [
    'user_id', 'live_id', 'streamer_id', 'timestamp', 'timestamp_dt',
    'gift_price_label', 'target', 'target_raw', 'is_gift',
    'label_end_dt', '_datetime', '_date', 'day',
] + FORBIDDEN_FEATURES


# =============================================================================
# æ—¥å¿—å·¥å…·
# =============================================================================
def log(msg: str, level: str = "INFO"):
    """æ‰“å°æ—¥å¿—"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    prefix = {"INFO": "ğŸ“", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}.get(level, "")
    print(f"[{timestamp}] {prefix} {msg}")


# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================
def load_raw_data():
    """
    åŠ è½½åŸå§‹æ•°æ®

    Returns:
        tuple: (gift, click, user, streamer, room) DataFrames
    """
    log("Loading raw data...")

    gift = pd.read_csv(DATA_DIR / "gift.csv")
    click = pd.read_csv(DATA_DIR / "click.csv")
    user = pd.read_csv(DATA_DIR / "user.csv")
    streamer = pd.read_csv(DATA_DIR / "streamer.csv")
    room = pd.read_csv(DATA_DIR / "room.csv")

    log(f"Loaded: gift={len(gift):,}, click={len(click):,}, "
        f"user={len(user):,}, streamer={len(streamer):,}, room={len(room):,}")

    return gift, click, user, streamer, room


# =============================================================================
# Click-Level æ ‡ç­¾æ„å»º
# =============================================================================
def prepare_click_level_labels(gift, click, label_window_minutes=1):
    """
    æ„å»º Click-level æ ‡ç­¾ï¼ˆLast-Touch Attributionï¼‰

    Attribution Model: Last-touch (Last-click) within lookback window
    Dedup Rule: æ¯ä¸ª gift åªèƒ½å½’å› ç»™ 1 æ¡ clickï¼ˆæœ€è¿‘çš„ä¸€æ¡ï¼‰ï¼ŒæŒ‰ gift_id å»é‡
    Aggregation: å†æŠŠ gift é‡‘é¢ sum åˆ° click-level label

    Args:
        gift: gift DataFrame
        click: click DataFrame
        label_window_minutes: æ ‡ç­¾çª—å£ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤ 1 åˆ†é’Ÿ
            - æ•°æ®åˆ†ææ˜¾ç¤º 98.2% çš„ gift åœ¨ click åŒä¸€æ¯«ç§’å†…å‘ç”Ÿ
            - 1 åˆ†é’Ÿçª—å£å·²è¦†ç›– 92.6% çš„ giftï¼ˆ90.0% çš„é‡‘é¢ï¼‰
            - è¯¦è§ exp/exp_label_window_analysis_20260119.md

    Returns:
        tuple: (click DataFrame with gift_price_label, orphan_stats dict)

    æ³¨æ„:
        - ä¼šè‡ªåŠ¨åˆ é™¤ watch_live_time åˆ—ï¼ˆæ³„æ¼ç‰¹å¾ï¼‰
        - æ¯ä¸ª gift æŒ‰ gift_id å»é‡ï¼Œåªå½’å› ç»™æœ€è¿‘çš„ä¸€æ¡ click
    """
    log(f"Preparing click-level labels (Last-Touch, window={label_window_minutes}min)...")

    click = click.copy()
    gift = gift.copy()

    # åˆ é™¤æ³„æ¼ç‰¹å¾
    if 'watch_live_time' in click.columns:
        click = click.drop(columns=['watch_live_time'])
        log("Removed watch_live_time (leakage feature)", "WARNING")

    # ==========================================================================
    # Step 0: æ·»åŠ  gift_idï¼ˆä½¿ç”¨è¡Œå·ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
    # ==========================================================================
    gift = gift.reset_index(drop=True)
    gift['gift_id'] = gift.index
    total_gift_count = len(gift)
    total_gift_value = gift['gift_price'].sum()

    # ==========================================================================
    # Step 1: ä» gift è§’åº¦ merge clickï¼ˆåå‘æ€è·¯ï¼šå…ˆå½’å› ï¼Œå†èšåˆï¼‰
    # ==========================================================================
    window_ms = label_window_minutes * 60 * 1000  # è½¬æ¢ä¸ºæ¯«ç§’

    # å…ˆæ‰¾å‡ºå“ªäº› gift æœ‰å¯¹åº”çš„ click
    gift_with_click_keys = gift.merge(
        click[['user_id', 'streamer_id', 'live_id']].drop_duplicates(),
        on=['user_id', 'streamer_id', 'live_id'],
        how='inner'
    )['gift_id'].unique()

    orphan_no_click = gift[~gift['gift_id'].isin(gift_with_click_keys)]
    orphan_no_click_count = len(orphan_no_click)
    orphan_no_click_value = orphan_no_click['gift_price'].sum()

    merged = gift[gift['gift_id'].isin(gift_with_click_keys)].merge(
        click[['user_id', 'streamer_id', 'live_id', 'timestamp']].rename(
            columns={'timestamp': 'click_ts'}
        ),
        on=['user_id', 'streamer_id', 'live_id'],
        how='inner'
    )

    # ==========================================================================
    # Step 2: ç­›é€‰ gift åœ¨ click çš„çª—å£å†…ï¼ˆclick_ts <= gift_ts <= click_ts + windowï¼‰
    # ==========================================================================
    in_window = (merged['timestamp'] >= merged['click_ts']) & \
                (merged['timestamp'] <= merged['click_ts'] + window_ms)
    merged_in_window = merged[in_window]
    merged_outside = merged[~in_window]

    # ç»Ÿè®¡çª—å£å¤–çš„ giftï¼ˆå»é‡åï¼‰
    outside_gift_ids = set(merged['gift_id']) - set(merged_in_window['gift_id'])
    orphan_outside_window = gift[gift['gift_id'].isin(outside_gift_ids)]
    orphan_outside_count = len(orphan_outside_window)
    orphan_outside_value = orphan_outside_window['gift_price'].sum()

    log(f"  Gift-Click pairs in window: {len(merged_in_window):,}")

    # ==========================================================================
    # Step 3: Last-Touch - æ¯ä¸ª gift_id åªä¿ç•™æœ€è¿‘çš„ clickï¼ˆclick_ts æœ€å¤§ï¼‰
    # ==========================================================================
    if len(merged_in_window) > 0:
        # æŒ‰ gift_id å»é‡ï¼ˆä¸æ˜¯æŒ‰ gift_tsï¼‰ï¼Œç¡®ä¿æ¯ä¸ª gift åªå½’å› ä¸€æ¬¡
        merged_dedup = merged_in_window.loc[
            merged_in_window.groupby('gift_id')['click_ts'].idxmax()
        ]
    else:
        merged_dedup = merged_in_window

    attributed_count = len(merged_dedup)
    attributed_value = merged_dedup['gift_price'].sum() if len(merged_dedup) > 0 else 0

    log(f"  Attributed gifts: {attributed_count:,} (dedup by gift_id)")

    # ==========================================================================
    # Step 4: èšåˆåˆ° click çº§åˆ«
    # ==========================================================================
    if len(merged_dedup) > 0:
        gift_agg = merged_dedup.groupby(
            ['user_id', 'streamer_id', 'live_id', 'click_ts']
        )['gift_price'].sum().reset_index().rename(columns={
            'click_ts': 'timestamp',
            'gift_price': 'gift_price_label'
        })
    else:
        gift_agg = pd.DataFrame(columns=['user_id', 'streamer_id', 'live_id', 'timestamp', 'gift_price_label'])

    # ==========================================================================
    # Step 5: Merge å› click
    # ==========================================================================
    click = click.merge(
        gift_agg,
        on=['user_id', 'streamer_id', 'live_id', 'timestamp'],
        how='left'
    )
    click['gift_price_label'] = click['gift_price_label'].fillna(0)

    # ==========================================================================
    # Step 6: è¦†ç›–ç‡ç»Ÿè®¡ + Orphan Breakdownï¼ˆä¸å†ç”¨"å®ˆæ’"è¯¯å¯¼ï¼‰
    # ==========================================================================
    orphan_stats = {
        'total_gift_count': total_gift_count,
        'total_gift_value': total_gift_value,
        'attributed_count': attributed_count,
        'attributed_value': attributed_value,
        'orphan_no_click_count': orphan_no_click_count,
        'orphan_no_click_value': orphan_no_click_value,
        'orphan_outside_window_count': orphan_outside_count,
        'orphan_outside_window_value': orphan_outside_value,
        'count_coverage': attributed_count / total_gift_count if total_gift_count > 0 else 0,
        'value_coverage': attributed_value / total_gift_value if total_gift_value > 0 else 0,
    }

    log(f"  Attribution Coverage:")
    log(f"    Count: {orphan_stats['count_coverage']:.1%} ({attributed_count:,}/{total_gift_count:,})")
    log(f"    Value: {orphan_stats['value_coverage']:.1%} ({attributed_value:,.0f}/{total_gift_value:,.0f})")
    log(f"  Orphan Breakdown:")
    log(f"    No click:       {orphan_no_click_count:,} gifts ({orphan_no_click_value:,.0f} yuan, {orphan_no_click_value/total_gift_value*100:.1f}%)")
    log(f"    Outside window: {orphan_outside_count:,} gifts ({orphan_outside_value:,.0f} yuan, {orphan_outside_value/total_gift_value*100:.1f}%)")

    # æ£€æŸ¥æ˜¯å¦æœ‰é‡‘é¢è†¨èƒ€ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
    if attributed_value > total_gift_value * 1.001:
        log(f"ERROR: é‡‘é¢è†¨èƒ€! attributed={attributed_value:,.0f} > total={total_gift_value:,.0f}", "ERROR")

    log(f"Click-level data: {len(click):,} records, gift_rate={click['gift_price_label'].gt(0).mean()*100:.2f}%")

    return click, orphan_stats


# =============================================================================
# 7-7-7 æ•°æ®åˆ’åˆ†
# =============================================================================
def split_by_days(df, train_days=7, val_days=7, test_days=7, gap_days=0):
    """
    æŒ‰å¤©åˆ’åˆ†æ•°æ®é›†ï¼ˆ7-7-7ï¼‰

    Args:
        df: DataFrame with timestamp column (milliseconds)
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val å’Œ Val-Test ä¹‹é—´çš„ gap å¤©æ•° (default: 0)

    Returns:
        tuple: (train_df, val_df, test_df)
    """
    log(f"Splitting data: {train_days}-{val_days}-{test_days} days (gap={gap_days})...")

    df = df.copy()
    df['_datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    dates = sorted(df['_date'].unique())
    min_date = dates[0]
    max_date = dates[-1]

    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_end = min_date + timedelta(days=train_days - 1)
    val_start = train_end + timedelta(days=gap_days + 1)
    val_end = val_start + timedelta(days=val_days - 1)
    test_start = val_end + timedelta(days=gap_days + 1)
    test_end = test_start + timedelta(days=test_days - 1)

    log(f"Data range: {min_date} ~ {max_date} ({len(dates)} days)")
    log(f"  Train: {min_date} ~ {train_end} ({train_days} days)")
    log(f"  Val:   {val_start} ~ {val_end} ({val_days} days)")
    log(f"  Test:  {test_start} ~ {test_end} ({test_days} days)")

    # åˆ’åˆ†
    train_df = df[df['_date'] <= train_end].copy()
    val_df = df[(df['_date'] >= val_start) & (df['_date'] <= val_end)].copy()
    test_df = df[(df['_date'] >= test_start) & (df['_date'] <= test_end)].copy()

    # æ¸…ç†ä¸´æ—¶åˆ—
    for d in [train_df, val_df, test_df]:
        d.drop(columns=['_datetime', '_date'], inplace=True, errors='ignore')

    log(f"Split result: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}")

    return train_df, val_df, test_df


# =============================================================================
# Day-Frozen å†å²ç‰¹å¾ï¼ˆæ— æ³„æ¼ï¼‰
# =============================================================================
def create_day_frozen_features(gift, click):
    """
    åˆ›å»ºæŒ‰å¤©å†»ç»“çš„å†å²ç‰¹å¾ï¼ˆDay-Frozen / Day-Snapshotï¼‰

    æ ¸å¿ƒè®¾è®¡ï¼š
    - å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
    - ä½¿ç”¨ pd.merge_asof(..., by=..., allow_exact_matches=False) é«˜æ•ˆå®ç°
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—é€»è¾‘
    - æ— æ³„æ¼ã€å£å¾„ä¸€è‡´

    Args:
        gift: å…¨é‡ gift DataFrame
        click: å…¨é‡ click DataFrame (å·²åˆ é™¤ watch_live_time)

    Returns:
        DataFrame: click æ•°æ® + å†å²ç‰¹å¾
    """
    log("Creating day-frozen historical features...")

    click = click.copy()
    gift = gift.copy()

    # æ·»åŠ  day åˆ—ï¼ˆè½¬æ¢ä¸º datetime ä»¥ä¾¿ merge_asofï¼‰
    click['day'] = pd.to_datetime(click['timestamp'], unit='ms').dt.normalize()
    gift['day'] = pd.to_datetime(gift['timestamp'], unit='ms').dt.normalize()

    # =========================================================================
    # Pair-level å†å²ç‰¹å¾ (user, streamer)
    # =========================================================================
    log("  Building pair-level features...")

    # æŒ‰å¤©èšåˆ
    pair_day = gift.groupby(['day', 'user_id', 'streamer_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº (merge_asof è¦æ±‚ on åˆ—å…¨å±€æ’åº)
    pair_day = pair_day.sort_values('day').reset_index(drop=True)

    # Cumsum: æˆªè‡³å½“å¤©ï¼ˆå«ï¼‰çš„ç´¯è®¡
    pair_day[['pair_gift_cnt_hist', 'pair_gift_sum_hist']] = pair_day.groupby(
        ['user_id', 'streamer_id']
    )[['gift_cnt_day', 'gift_sum_day']].cumsum()

    # å‡†å¤‡ click æ•°æ®ç”¨äº merge_asof (æŒ‰ day å…¨å±€æ’åº)
    click_sorted = click.sort_values('day').reset_index(drop=True)

    # merge_asof with by: æŒ‰ (user, streamer) åˆ†ç»„ï¼ŒæŸ¥æ‰¾ strictly before çš„æœ€è¿‘è®°å½•
    click_with_pair = pd.merge_asof(
        click_sorted,
        pair_day[['day', 'user_id', 'streamer_id', 'pair_gift_cnt_hist', 'pair_gift_sum_hist']],
        on='day',
        by=['user_id', 'streamer_id'],
        direction='backward',
        allow_exact_matches=False  # ä¸¥æ ¼ < å½“å‰å¤©
    )

    # å¡«å…… NaN ä¸º 0
    click_with_pair[['pair_gift_cnt_hist', 'pair_gift_sum_hist']] = \
        click_with_pair[['pair_gift_cnt_hist', 'pair_gift_sum_hist']].fillna(0)

    # è®¡ç®—å‡å€¼
    click_with_pair['pair_gift_mean_hist'] = (
        click_with_pair['pair_gift_sum_hist'] / click_with_pair['pair_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    Pair features: {(click_with_pair['pair_gift_cnt_hist'] > 0).sum():,} samples have history")

    # =========================================================================
    # User-level å†å²ç‰¹å¾
    # =========================================================================
    log("  Building user-level features...")

    user_day = gift.groupby(['day', 'user_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº
    user_day = user_day.sort_values('day').reset_index(drop=True)
    user_day[['user_gift_cnt_hist', 'user_gift_sum_hist']] = user_day.groupby('user_id')[
        ['gift_cnt_day', 'gift_sum_day']
    ].cumsum()

    # merge_asof by user_id (click_with_pair å·²æŒ‰ day æ’åº)
    click_with_user = pd.merge_asof(
        click_with_pair,
        user_day[['day', 'user_id', 'user_gift_cnt_hist', 'user_gift_sum_hist']],
        on='day',
        by='user_id',
        direction='backward',
        allow_exact_matches=False
    )

    click_with_user[['user_gift_cnt_hist', 'user_gift_sum_hist']] = \
        click_with_user[['user_gift_cnt_hist', 'user_gift_sum_hist']].fillna(0)
    click_with_user['user_gift_mean_hist'] = (
        click_with_user['user_gift_sum_hist'] / click_with_user['user_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    User features: {(click_with_user['user_gift_cnt_hist'] > 0).sum():,} samples have history")

    # =========================================================================
    # Streamer-level å†å²ç‰¹å¾
    # =========================================================================
    log("  Building streamer-level features...")

    str_day = gift.groupby(['day', 'streamer_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº
    str_day = str_day.sort_values('day').reset_index(drop=True)
    str_day[['str_gift_cnt_hist', 'str_gift_sum_hist']] = str_day.groupby('streamer_id')[
        ['gift_cnt_day', 'gift_sum_day']
    ].cumsum()

    # merge_asof by streamer_id (click_with_user å·²æŒ‰ day æ’åº)
    click_with_str = pd.merge_asof(
        click_with_user,
        str_day[['day', 'streamer_id', 'str_gift_cnt_hist', 'str_gift_sum_hist']],
        on='day',
        by='streamer_id',
        direction='backward',
        allow_exact_matches=False
    )

    click_with_str[['str_gift_cnt_hist', 'str_gift_sum_hist']] = \
        click_with_str[['str_gift_cnt_hist', 'str_gift_sum_hist']].fillna(0)
    click_with_str['str_gift_mean_hist'] = (
        click_with_str['str_gift_sum_hist'] / click_with_str['str_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    Streamer features: {(click_with_str['str_gift_cnt_hist'] > 0).sum():,} samples have history")

    click = click_with_str

    # =========================================================================
    # å†å²è§‚çœ‹æ—¶é•¿ç‰¹å¾ï¼ˆå¯é€‰ï¼Œä½¿ç”¨è¿‡å»å¤©çš„ click/watch_timeï¼‰
    # =========================================================================
    # æ³¨æ„ï¼šå½“å‰å®ç°ä¸ä½¿ç”¨ watch_time ç‰¹å¾ï¼Œå› ä¸ºéœ€è¦åŸå§‹ click ä¿ç•™ watch_live_time
    # å¦‚éœ€æ·»åŠ ï¼Œå¯ä»¥åœ¨æ­¤å¤„å®ç°ç±»ä¼¼çš„ cumsum + shift é€»è¾‘

    log("Day-frozen features created!", "SUCCESS")
    log(f"  Total historical features: 9 (pair: 3, user: 3, streamer: 3)")

    return click


# =============================================================================
# é™æ€ç‰¹å¾
# =============================================================================

# Snapshot ç‰¹å¾ï¼ˆKuaiLive ç”¨ May 25 å¿«ç…§ï¼Œå­˜åœ¨æ—¶é—´æ³„æ¼é£é™©ï¼‰
SNAPSHOT_FEATURES = [
    # User snapshot features
    'fans_num', 'follow_num', 'accu_watch_live_cnt', 'accu_watch_live_duration',
    # Streamer snapshot features (will be prefixed with str_)
    'fans_user_num', 'fans_group_fans_num', 'follow_user_num',
    'accu_live_cnt', 'accu_live_duration', 'accu_play_cnt', 'accu_play_duration',
]


def add_static_features(df, user, streamer, room, strict_mode=True):
    """
    æ·»åŠ é™æ€ç‰¹å¾

    Args:
        df: DataFrame
        user: user DataFrame
        streamer: streamer DataFrame
        room: room DataFrame
        strict_mode: bool, æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆé»˜è®¤ Trueï¼‰
            - True (Strict): åªä¿ç•™çœŸæ­£é™æ€/åœ¨çº¿å¯å¾—å­—æ®µï¼Œdrop æ‰€æœ‰å¿«ç…§ç´¯è®¡ç‰¹å¾
            - False (Benchmark): ä¿ç•™ KuaiLive å¿«ç…§ç‰¹å¾ï¼ˆfans/follow/accu_*ï¼‰
              æ³¨æ„ï¼šè¿™äº›æ˜¯ May 25, 2025 å¿«ç…§ï¼Œå­˜åœ¨æ—¶é—´æ³„æ¼é£é™©

    Returns:
        DataFrame with static features added
    """
    mode_str = "Strict" if strict_mode else "Benchmark"
    log(f"Adding static features (mode={mode_str})...")

    df = df.copy()

    # -------------------------------------------------------------------------
    # User profile features
    # -------------------------------------------------------------------------
    if strict_mode:
        # Strict: åªä¿ç•™çœŸæ­£é™æ€å­—æ®µ
        user_cols = ['user_id', 'age', 'gender', 'device_brand', 'device_price',
                     'is_live_streamer', 'is_photo_author']
    else:
        # Benchmark: åŒ…å«å¿«ç…§ç‰¹å¾ï¼ˆå­˜åœ¨æ³„æ¼é£é™©ï¼‰
        user_cols = ['user_id', 'age', 'gender', 'device_brand', 'device_price',
                     'fans_num', 'follow_num', 'accu_watch_live_cnt', 'accu_watch_live_duration',
                     'is_live_streamer', 'is_photo_author']

    user_cols = [c for c in user_cols if c in user.columns]
    df = df.merge(user[user_cols], on='user_id', how='left')

    # -------------------------------------------------------------------------
    # Streamer profile features
    # -------------------------------------------------------------------------
    if strict_mode:
        # Strict: åªä¿ç•™ streamer_idï¼ˆå†å²ç‰¹å¾å·²åœ¨ day-frozen é‡Œï¼‰
        str_cols = ['streamer_id']
    else:
        # Benchmark: åŒ…å«å¿«ç…§ç‰¹å¾
        str_cols = ['streamer_id', 'fans_user_num', 'fans_group_fans_num',
                    'follow_user_num', 'accu_live_cnt', 'accu_live_duration',
                    'accu_play_cnt', 'accu_play_duration']

    str_cols = [c for c in str_cols if c in streamer.columns]

    # Rename to avoid conflict with user features
    streamer_subset = streamer[str_cols].copy()
    rename_map = {c: f'str_{c}' for c in str_cols if c != 'streamer_id'}
    streamer_subset = streamer_subset.rename(columns=rename_map)

    df = df.merge(streamer_subset, on='streamer_id', how='left')

    # -------------------------------------------------------------------------
    # Room features
    # -------------------------------------------------------------------------
    room_cols = ['live_id', 'live_type', 'live_content_category']
    room_cols = [c for c in room_cols if c in room.columns]
    room_dedup = room[room_cols].drop_duplicates('live_id')
    df = df.merge(room_dedup, on='live_id', how='left')

    # -------------------------------------------------------------------------
    # Time features (ä» timestamp æå–)
    # -------------------------------------------------------------------------
    ts_dt = pd.to_datetime(df['timestamp'], unit='ms')
    df['hour'] = ts_dt.dt.hour
    df['day_of_week'] = ts_dt.dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)

    log(f"  Added static features (mode={mode_str}), total columns: {df.shape[1]}", "SUCCESS")

    return df


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================
def prepare_dataset(train_days=7, val_days=7, test_days=7, gap_days=0,
                    label_window_minutes=1, use_cache=True, strict_mode=True):
    """
    å‡†å¤‡æ— æ³„æ¼çš„æ•°æ®é›†ï¼ˆä¸»å‡½æ•°ï¼‰

    è¿™æ˜¯æ‰€æœ‰ gift_EVpred å®éªŒçš„æ ‡å‡†å…¥å£ã€‚

    æ ¸å¿ƒè®¾è®¡ï¼šDay-Frozenï¼ˆæŒ‰å¤©å†»ç»“ï¼‰
    - å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—æ„é€ é€»è¾‘
    - æ— æ³„æ¼ã€å£å¾„ä¸€è‡´

    Args:
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val/Val-Test gap å¤©æ•° (default: 0)
        label_window_minutes: æ ‡ç­¾çª—å£ï¼ˆåˆ†é’Ÿï¼‰(default: 1)
            - æ•°æ®åˆ†ææ˜¾ç¤º 98.2% çš„ gift åœ¨ click åŒä¸€æ¯«ç§’å†…å‘ç”Ÿ
            - 1 åˆ†é’Ÿçª—å£å·²è¦†ç›– 92.6% çš„ giftï¼ˆ90.0% çš„é‡‘é¢ï¼‰
            - å¦‚éœ€æ›´å¤§çª—å£ï¼Œå¯è®¾ä¸º 5/10/60 åˆ†é’Ÿ
            - è¯¦è§ exp/exp_label_window_analysis_20260119.md
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (default: True)
        strict_mode: æ˜¯å¦ä½¿ç”¨ä¸¥æ ¼æ— æ³„æ¼æ¨¡å¼ (default: True)
            - True (Strict): åªä¿ç•™çœŸæ­£é™æ€å­—æ®µï¼Œdrop å¿«ç…§ç´¯è®¡ç‰¹å¾
            - False (Benchmark): ä¿ç•™ KuaiLive å¿«ç…§ç‰¹å¾ï¼ˆå­˜åœ¨æ—¶é—´æ³„æ¼é£é™©ï¼‰

    Returns:
        tuple: (train_df, val_df, test_df)

    Example:
        >>> from gift_EVpred.data_utils import prepare_dataset, get_feature_columns
        >>> train_df, val_df, test_df = prepare_dataset()  # é»˜è®¤ 1 åˆ†é’Ÿçª—å£ + Strict
        >>> train_df, val_df, test_df = prepare_dataset(strict_mode=False)  # Benchmark æ¨¡å¼
        >>> feature_cols = get_feature_columns(train_df)
    """
    mode_str = "Strict" if strict_mode else "Benchmark"
    log("=" * 60)
    log(f"Preparing Leakage-Free Dataset (Day-Frozen, {mode_str} Mode)")
    log("=" * 60)

    # ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«çª—å£é•¿åº¦ä»¥åŒºåˆ†ä¸åŒé…ç½®ï¼‰
    CACHE_DIR = OUTPUT_DIR / "features_cache"
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_file = CACHE_DIR / f"day_frozen_features_lw{label_window_minutes}min.parquet"

    # 1. Load raw data
    gift, click, user, streamer, room = load_raw_data()

    # 2. å°è¯•ä»ç¼“å­˜åŠ è½½
    if use_cache and cache_file.exists():
        log(f"Loading cached features from {cache_file}")
        click_with_features = pd.read_parquet(cache_file)
        log(f"  Loaded {len(click_with_features):,} records from cache", "SUCCESS")

        # ç¼“å­˜åæ¸…ç†ï¼šç¡®ä¿ forbidden features ä¸ä¼šä»æ—§ç¼“å­˜å¸¦å›æ¥
        for col in FORBIDDEN_FEATURES:
            if col in click_with_features.columns:
                click_with_features = click_with_features.drop(columns=[col])
                log(f"  Removed forbidden feature from cache: {col}", "WARNING")
    else:
        # 2. Click-level labels (removes watch_live_time automatically)
        click_with_labels, orphan_stats = prepare_click_level_labels(gift, click, label_window_minutes)

        # 3. Create day-frozen historical features (before split!)
        # è¿™æ ·è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—é€»è¾‘
        click_with_features = create_day_frozen_features(gift, click_with_labels)

        # ä¿å­˜ç¼“å­˜
        if use_cache:
            click_with_features.to_parquet(cache_file)
            log(f"Saved features to cache: {cache_file}", "SUCCESS")

    # 4. 7-7-7 split
    train_df, val_df, test_df = split_by_days(
        click_with_features, train_days, val_days, test_days, gap_days
    )

    # 5. Add static features (with strict_mode control)
    train_df = add_static_features(train_df, user, streamer, room, strict_mode=strict_mode)
    val_df = add_static_features(val_df, user, streamer, room, strict_mode=strict_mode)
    test_df = add_static_features(test_df, user, streamer, room, strict_mode=strict_mode)

    # 6. Create targets
    for df in [train_df, val_df, test_df]:
        df['target'] = np.log1p(df['gift_price_label'])
        df['target_raw'] = df['gift_price_label']
        df['is_gift'] = (df['gift_price_label'] > 0).astype(int)

    # 7. Encode all object/category columns (Train æ‹Ÿåˆï¼ŒVal/Test å¤ç”¨)
    # å…ˆè¯†åˆ«éœ€è¦ç¼–ç çš„åˆ—
    cat_cols = []
    for col in train_df.columns:
        if train_df[col].dtype == 'object' or str(train_df[col].dtype) == 'category':
            cat_cols.append(col)

    # Train æ‹Ÿåˆ categoriesï¼ŒVal/Test å¤ç”¨åŒä¸€æ˜ å°„
    for col in cat_cols:
        # å¡«å…… NaN
        train_df[col] = train_df[col].fillna('unknown').astype(str)
        val_df[col] = val_df[col].fillna('unknown').astype(str)
        test_df[col] = test_df[col].fillna('unknown').astype(str)

        # ç”¨ Train çš„å”¯ä¸€å€¼å»ºç«‹ categoriesï¼ˆåŠ ä¸Š 'unknown' å…œåº•ï¼‰
        train_categories = list(train_df[col].unique())
        if 'unknown' not in train_categories:
            train_categories.append('unknown')

        # åˆ›å»ºå¸¦å›ºå®š categories çš„ Categoricalï¼ŒæœªçŸ¥å€¼æ˜ å°„ä¸º 'unknown'
        for df in [train_df, val_df, test_df]:
            # å°† Val/Test ä¸­ Train æ²¡è§è¿‡çš„å€¼æ›¿æ¢ä¸º 'unknown'
            df[col] = df[col].apply(lambda x: x if x in train_categories else 'unknown')
            df[col] = pd.Categorical(df[col], categories=train_categories).codes

    log(f"  Encoded {len(cat_cols)} categorical columns (train-fitted)", "SUCCESS")

    # 8. Fill NaN for numeric columns
    for df in [train_df, val_df, test_df]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)

    log("=" * 60)
    log("Dataset preparation complete!", "SUCCESS")
    log(f"  Train: {len(train_df):,} (gift_rate={train_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Val:   {len(val_df):,} (gift_rate={val_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Test:  {len(test_df):,} (gift_rate={test_df['is_gift'].mean()*100:.2f}%)")
    log("=" * 60)

    return train_df, val_df, test_df


# =============================================================================
# ç‰¹å¾åˆ—å·¥å…·
# =============================================================================
def get_feature_columns(df):
    """
    è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ metadataã€target å’Œæ³„æ¼ç‰¹å¾ï¼‰

    Args:
        df: DataFrame

    Returns:
        list: feature column names
    """
    exclude = set(EXCLUDE_COLUMNS)
    features = [c for c in df.columns if c not in exclude]

    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰æ³„æ¼ç‰¹å¾
    for f in FORBIDDEN_FEATURES:
        if f in features:
            features.remove(f)
            log(f"Removed forbidden feature: {f}", "WARNING")

    return features


def verify_feature_columns(feature_cols):
    """
    éªŒè¯ç‰¹å¾åˆ—ä¸åŒ…å«æ³„æ¼ç‰¹å¾

    Args:
        feature_cols: list of feature column names

    Raises:
        AssertionError if forbidden features found
    """
    for f in FORBIDDEN_FEATURES:
        assert f not in feature_cols, f"Forbidden feature found: {f}"

    log("Feature column verification: PASSED", "SUCCESS")


# =============================================================================
# éªŒè¯å‡½æ•°
# =============================================================================
def verify_time_split(train_df, val_df, test_df):
    """
    éªŒè¯æ—¶é—´åˆ’åˆ†æ­£ç¡®æ€§ï¼ˆæ— æ—¶é—´ç©¿è¶Šï¼‰

    Args:
        train_df, val_df, test_df: DataFrames

    Raises:
        AssertionError if time overlap detected
    """
    train_max = train_df['timestamp'].max()
    val_min = val_df['timestamp'].min()
    val_max = val_df['timestamp'].max()
    test_min = test_df['timestamp'].min()

    assert train_max < val_min, f"Train/Val overlap: {train_max} >= {val_min}"
    assert val_max < test_min, f"Val/Test overlap: {val_max} >= {test_min}"

    log("Time split verification: PASSED", "SUCCESS")
    log(f"  Train max: {pd.to_datetime(train_max, unit='ms')}")
    log(f"  Val min:   {pd.to_datetime(val_min, unit='ms')}")
    log(f"  Test min:  {pd.to_datetime(test_min, unit='ms')}")


def verify_no_leakage(df, gift, n_samples=100):
    """
    éªŒè¯ç‰¹å¾æ— æ³„æ¼ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰

    å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼ŒéªŒè¯ pair_gift_cnt_hist ç­‰äºçœŸå®çš„ past-day countã€‚
    Day-Frozen: åªç”¨ day < å½“å‰ day çš„å†å²

    Args:
        df: DataFrame with features
        gift: original gift DataFrame
        n_samples: number of samples to check

    Returns:
        bool: True if passed, False if leakage detected
    """
    log(f"Verifying no leakage ({n_samples} samples)...")

    gift = gift.copy()
    gift['day'] = pd.to_datetime(gift['timestamp'], unit='ms').dt.normalize()

    errors = []

    sample_idx = np.random.choice(len(df), min(n_samples, len(df)), replace=False)

    for idx in sample_idx:
        row = df.iloc[idx]
        click_day = pd.to_datetime(row['timestamp'], unit='ms').normalize()
        user_id = row['user_id']
        streamer_id = row['streamer_id']

        # è®¡ç®—çœŸå®çš„ past-day count (day < click_day)
        true_past = gift[
            (gift['user_id'] == user_id) &
            (gift['streamer_id'] == streamer_id) &
            (gift['day'] < click_day)  # ä¸¥æ ¼ < å½“å‰å¤©
        ]
        true_count = len(true_past)

        # å¯¹æ¯”ç‰¹å¾å€¼
        feature_count = row['pair_gift_cnt_hist']

        if feature_count != true_count:
            errors.append({
                'idx': idx,
                'day': click_day,
                'expected': true_count,
                'got': feature_count,
                'diff': feature_count - true_count
            })

    if errors:
        log(f"Leakage verification: FAILED ({len(errors)}/{n_samples} samples)", "ERROR")
        for e in errors[:3]:
            log(f"  idx={e['idx']}, day={e['day']}: expected={e['expected']}, got={e['got']}, diff={e['diff']}")
        return False
    else:
        log(f"Leakage verification: PASSED ({n_samples}/{n_samples} samples)", "SUCCESS")
        return True


def run_full_verification(train_df, val_df, test_df, gift, feature_cols):
    """
    è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹

    Args:
        train_df, val_df, test_df: DataFrames
        gift: original gift DataFrame
        feature_cols: list of feature column names

    Returns:
        bool: True if all verifications passed
    """
    log("=" * 60)
    log("Running Full Verification")
    log("=" * 60)

    all_passed = True

    # 1. æ—¶é—´åˆ’åˆ†
    try:
        verify_time_split(train_df, val_df, test_df)
    except AssertionError as e:
        log(f"Time split verification FAILED: {e}", "ERROR")
        all_passed = False

    # 2. ç‰¹å¾åˆ—
    try:
        verify_feature_columns(feature_cols)
    except AssertionError as e:
        log(f"Feature column verification FAILED: {e}", "ERROR")
        all_passed = False

    # 3. æ³„æ¼æ£€æŸ¥
    log("\nVerifying train set...")
    if not verify_no_leakage(train_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying val set...")
    if not verify_no_leakage(val_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying test set...")
    if not verify_no_leakage(test_df, gift, n_samples=100):
        all_passed = False

    log("=" * 60)
    if all_passed:
        log("All verifications PASSED!", "SUCCESS")
    else:
        log("Some verifications FAILED!", "ERROR")
    log("=" * 60)

    return all_passed


# =============================================================================
# CLI å…¥å£
# =============================================================================
if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    print("Gift EVpred Data Utils - Day-Frozen Version")
    print("=" * 60)

    # å‡†å¤‡æ•°æ®
    train_df, val_df, test_df = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)
    print(f"\nFeature columns ({len(feature_cols)}):")
    print(feature_cols[:10], "...")

    # è¿è¡ŒéªŒè¯
    gift, _, _, _, _ = load_raw_data()
    run_full_verification(train_df, val_df, test_df, gift, feature_cols)
