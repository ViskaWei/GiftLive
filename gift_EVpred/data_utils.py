#!/usr/bin/env python3
"""
Gift EV Prediction - Leakage-Free Data Utilities
=================================================

ç»Ÿä¸€çš„æ— æ³„æ¼æ•°æ®å¤„ç†æ¨¡å—ï¼Œæ‰€æœ‰ gift_EVpred å®éªŒå¿…é¡»ä½¿ç”¨æ­¤æ¨¡å—ã€‚

Usage:
    from gift_EVpred.data_utils import (
        load_raw_data,
        prepare_dataset,
        get_feature_columns,
        verify_no_leakage
    )

    # æ ‡å‡†ç”¨æ³•
    train_df, val_df, test_df, lookups = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)

    # éªŒè¯æ— æ³„æ¼
    verify_no_leakage(train_df, gift_df)

Author: Viska Wei
Date: 2026-01-18
Version: 1.0
"""

import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import pickle
import warnings

warnings.filterwarnings('ignore')

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR = Path("/home/swei20/GiftLive")
DATA_DIR = BASE_DIR / "data" / "KuaiLive"
OUTPUT_DIR = BASE_DIR / "gift_EVpred"
CACHE_DIR = OUTPUT_DIR / "features_cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# =============================================================================
# å¸¸é‡é…ç½®
# =============================================================================
SEED = 42
np.random.seed(SEED)

# æ³„æ¼ç‰¹å¾é»‘åå•ï¼ˆç»å¯¹ç¦æ­¢ä½¿ç”¨ï¼‰
FORBIDDEN_FEATURES = [
    'watch_live_time',      # ç»“æœæ³„æ¼ï¼šåŒ…å«æ‰“èµåçš„è§‚çœ‹æ—¶é•¿
    'watch_time_log',       # åŒä¸Š
    'watch_time_ratio',     # åŒä¸Š
]

# å¿…é¡»æ’é™¤çš„åˆ—ï¼ˆéç‰¹å¾ï¼‰
EXCLUDE_COLUMNS = [
    'user_id', 'live_id', 'streamer_id', 'timestamp', 'timestamp_dt',
    'gift_price_label', 'target', 'target_raw', 'is_gift',
    'label_end_dt', '_datetime', '_date',
] + FORBIDDEN_FEATURES


# =============================================================================
# æ—¥å¿—å·¥å…·
# =============================================================================
def log(msg: str, level: str = "INFO"):
    """æ‰“å°æ—¥å¿—"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    prefix = {"INFO": "ğŸ“", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}.get(level, "")
    print(f"[{timestamp}] {prefix} {msg}")


# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================
def load_raw_data():
    """
    åŠ è½½åŸå§‹æ•°æ®

    Returns:
        tuple: (gift, click, user, streamer, room) DataFrames
    """
    log("Loading raw data...")

    gift = pd.read_csv(DATA_DIR / "gift.csv")
    click = pd.read_csv(DATA_DIR / "click.csv")
    user = pd.read_csv(DATA_DIR / "user.csv")
    streamer = pd.read_csv(DATA_DIR / "streamer.csv")
    room = pd.read_csv(DATA_DIR / "room.csv")

    log(f"Loaded: gift={len(gift):,}, click={len(click):,}, "
        f"user={len(user):,}, streamer={len(streamer):,}, room={len(room):,}")

    return gift, click, user, streamer, room


# =============================================================================
# Click-Level æ ‡ç­¾æ„å»º
# =============================================================================
def prepare_click_level_labels(gift, click, label_window_hours=1):
    """
    æ„å»º Click-level æ ‡ç­¾

    Args:
        gift: gift DataFrame
        click: click DataFrame
        label_window_hours: æ ‡ç­¾çª—å£ï¼ˆå°æ—¶ï¼‰

    Returns:
        DataFrame: click æ•°æ® + gift_price_label åˆ—ï¼ˆ0 æˆ–æ­£æ•°ï¼‰

    æ³¨æ„:
        - ä¼šè‡ªåŠ¨åˆ é™¤ watch_live_time åˆ—ï¼ˆæ³„æ¼ç‰¹å¾ï¼‰
        - Label = click å label_window_hours å†…çš„ gift æ€»é¢
    """
    log(f"Preparing click-level labels (window={label_window_hours}h)...")

    click = click.copy()
    gift = gift.copy()

    # åˆ é™¤æ³„æ¼ç‰¹å¾
    if 'watch_live_time' in click.columns:
        click = click.drop(columns=['watch_live_time'])
        log("Removed watch_live_time (leakage feature)", "WARNING")

    # è½¬æ¢æ—¶é—´
    click['timestamp_dt'] = pd.to_datetime(click['timestamp'], unit='ms')
    gift['timestamp_dt'] = pd.to_datetime(gift['timestamp'], unit='ms')

    # Label window
    click['label_end_dt'] = click['timestamp_dt'] + pd.Timedelta(hours=label_window_hours)

    # Merge and filter
    merged = click[['user_id', 'streamer_id', 'live_id', 'timestamp', 'timestamp_dt', 'label_end_dt']].merge(
        gift[['user_id', 'streamer_id', 'live_id', 'timestamp_dt', 'gift_price']],
        on=['user_id', 'streamer_id', 'live_id'],
        how='left',
        suffixes=('_click', '_gift')
    )

    # Filter: gift within window
    merged = merged[
        (merged['timestamp_dt_gift'] >= merged['timestamp_dt_click']) &
        (merged['timestamp_dt_gift'] <= merged['label_end_dt'])
    ]

    # Aggregate
    gift_agg = merged.groupby(
        ['user_id', 'streamer_id', 'live_id', 'timestamp']
    )['gift_price'].sum().reset_index().rename(columns={'gift_price': 'gift_price_label'})

    # Merge back
    click = click.merge(gift_agg, on=['user_id', 'streamer_id', 'live_id', 'timestamp'], how='left')
    click['gift_price_label'] = click['gift_price_label'].fillna(0)

    # æ¸…ç†ä¸´æ—¶åˆ—
    click = click.drop(columns=['label_end_dt'], errors='ignore')

    log(f"Click-level data: {len(click):,} records, gift_rate={click['gift_price_label'].gt(0).mean()*100:.2f}%")

    return click


# =============================================================================
# 7-7-7 æ•°æ®åˆ’åˆ†
# =============================================================================
def split_by_days(df, train_days=7, val_days=7, test_days=7, gap_days=0):
    """
    æŒ‰å¤©åˆ’åˆ†æ•°æ®é›†ï¼ˆ7-7-7ï¼‰

    Args:
        df: DataFrame with timestamp column (milliseconds)
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val å’Œ Val-Test ä¹‹é—´çš„ gap å¤©æ•° (default: 0)

    Returns:
        tuple: (train_df, val_df, test_df)
    """
    log(f"Splitting data: {train_days}-{val_days}-{test_days} days (gap={gap_days})...")

    df = df.copy()
    df['_datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    dates = sorted(df['_date'].unique())
    min_date = dates[0]
    max_date = dates[-1]

    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_end = min_date + timedelta(days=train_days - 1)
    val_start = train_end + timedelta(days=gap_days + 1)
    val_end = val_start + timedelta(days=val_days - 1)
    test_start = val_end + timedelta(days=gap_days + 1)
    test_end = test_start + timedelta(days=test_days - 1)

    log(f"Data range: {min_date} ~ {max_date} ({len(dates)} days)")
    log(f"  Train: {min_date} ~ {train_end} ({train_days} days)")
    log(f"  Val:   {val_start} ~ {val_end} ({val_days} days)")
    log(f"  Test:  {test_start} ~ {test_end} ({test_days} days)")

    # åˆ’åˆ†
    train_df = df[df['_date'] <= train_end].copy()
    val_df = df[(df['_date'] >= val_start) & (df['_date'] <= val_end)].copy()
    test_df = df[(df['_date'] >= test_start) & (df['_date'] <= test_end)].copy()

    # æ¸…ç†ä¸´æ—¶åˆ—
    for d in [train_df, val_df, test_df]:
        d.drop(columns=['_datetime', '_date'], inplace=True, errors='ignore')

    log(f"Split result: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}")

    return train_df, val_df, test_df


# =============================================================================
# Frozen ç‰¹å¾ï¼ˆæ— æ³„æ¼ï¼‰
# =============================================================================
def create_frozen_lookups(gift, train_end_ts, click=None):
    """
    åˆ›å»º Frozen ç‰ˆæœ¬çš„ç‰¹å¾æŸ¥æ‰¾è¡¨

    åªä½¿ç”¨ Train æ—¶é—´çª—å£å†…çš„æ•°æ®è®¡ç®—ç»Ÿè®¡é‡ï¼ŒVal/Test åªæŸ¥è¡¨ã€‚

    Args:
        gift: å…¨é‡ gift DataFrame
        train_end_ts: Train ç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        click: å…¨é‡ click DataFrameï¼ˆå¯é€‰ï¼Œç”¨äºè®¡ç®—å†å²è§‚çœ‹æ—¶é•¿ï¼‰

    Returns:
        dict: {'pair': {...}, 'user': {...}, 'streamer': {...}, 'watch_time': {...}}
    """
    log("Creating frozen lookups (train window only)...")

    # åªç”¨ train æ—¶é—´çª—å£å†…çš„ gifts
    gift_train = gift[gift['timestamp'] <= train_end_ts].copy()
    log(f"  Train gifts: {len(gift_train):,} (before {pd.to_datetime(train_end_ts, unit='ms')})")

    lookups = {}

    # -------------------------------------------------------------------------
    # Pair-level features (user, streamer)
    # -------------------------------------------------------------------------
    pair_stats = gift_train.groupby(['user_id', 'streamer_id']).agg({
        'gift_price': ['count', 'sum', 'mean', 'std', 'max'],
        'timestamp': 'max'  # æœ€åä¸€æ¬¡æ‰“èµæ—¶é—´
    }).reset_index()
    pair_stats.columns = ['user_id', 'streamer_id',
                          'count', 'sum', 'mean', 'std', 'max', 'last_ts']
    pair_stats['std'] = pair_stats['std'].fillna(0)

    lookups['pair'] = {}
    for _, row in pair_stats.iterrows():
        key = (row['user_id'], row['streamer_id'])
        lookups['pair'][key] = {
            'count': row['count'],
            'sum': row['sum'],
            'mean': row['mean'],
            'std': row['std'],
            'max': row['max'],
            'last_ts': row['last_ts']
        }

    # -------------------------------------------------------------------------
    # User-level features
    # -------------------------------------------------------------------------
    user_stats = gift_train.groupby('user_id').agg({
        'gift_price': ['count', 'sum', 'mean'],
        'streamer_id': 'nunique'
    }).reset_index()
    user_stats.columns = ['user_id', 'count', 'sum', 'mean', 'unique_streamers']

    lookups['user'] = {}
    for _, row in user_stats.iterrows():
        lookups['user'][row['user_id']] = {
            'count': row['count'],
            'sum': row['sum'],
            'mean': row['mean'],
            'unique_streamers': row['unique_streamers']
        }

    # -------------------------------------------------------------------------
    # Streamer-level features
    # -------------------------------------------------------------------------
    str_stats = gift_train.groupby('streamer_id').agg({
        'gift_price': ['count', 'sum', 'mean'],
        'user_id': 'nunique'
    }).reset_index()
    str_stats.columns = ['streamer_id', 'count', 'sum', 'mean', 'unique_givers']

    lookups['streamer'] = {}
    for _, row in str_stats.iterrows():
        lookups['streamer'][row['streamer_id']] = {
            'count': row['count'],
            'sum': row['sum'],
            'mean': row['mean'],
            'unique_givers': row['unique_givers']
        }

    # -------------------------------------------------------------------------
    # Watch time features (å†å²è§‚çœ‹æ—¶é•¿ï¼Œæ— æ³„æ¼)
    # æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯ train æœŸé—´çš„ click æ•°æ®çš„ watch_live_time
    # -------------------------------------------------------------------------
    lookups['watch_time'] = {'user': {}, 'pair': {}, 'streamer': {}}

    if click is not None and 'watch_live_time' in click.columns:
        click_train = click[click['timestamp'] <= train_end_ts].copy()
        log(f"  Train clicks for watch_time: {len(click_train):,}")

        # User-level: ç”¨æˆ·å†å²å¹³å‡è§‚çœ‹æ—¶é•¿
        user_watch = click_train.groupby('user_id')['watch_live_time'].agg(['mean', 'sum', 'count']).reset_index()
        user_watch.columns = ['user_id', 'mean', 'sum', 'count']
        for _, row in user_watch.iterrows():
            lookups['watch_time']['user'][row['user_id']] = {
                'mean': row['mean'],
                'sum': row['sum'],
                'count': row['count']
            }

        # Pair-level: user-streamer å†å²å¹³å‡è§‚çœ‹æ—¶é•¿
        pair_watch = click_train.groupby(['user_id', 'streamer_id'])['watch_live_time'].agg(['mean', 'sum', 'count']).reset_index()
        pair_watch.columns = ['user_id', 'streamer_id', 'mean', 'sum', 'count']
        for _, row in pair_watch.iterrows():
            key = (row['user_id'], row['streamer_id'])
            lookups['watch_time']['pair'][key] = {
                'mean': row['mean'],
                'sum': row['sum'],
                'count': row['count']
            }

        # Streamer-level: ä¸»æ’­å†å²å¹³å‡è¢«è§‚çœ‹æ—¶é•¿
        str_watch = click_train.groupby('streamer_id')['watch_live_time'].agg(['mean', 'sum', 'count']).reset_index()
        str_watch.columns = ['streamer_id', 'mean', 'sum', 'count']
        for _, row in str_watch.iterrows():
            lookups['watch_time']['streamer'][row['streamer_id']] = {
                'mean': row['mean'],
                'sum': row['sum'],
                'count': row['count']
            }

        log(f"  Watch time lookups: {len(lookups['watch_time']['pair']):,} pairs, "
            f"{len(lookups['watch_time']['user']):,} users", "SUCCESS")

    log(f"  Lookups created: {len(lookups['pair']):,} pairs, "
        f"{len(lookups['user']):,} users, {len(lookups['streamer']):,} streamers", "SUCCESS")

    return lookups


def apply_frozen_features(df, lookups, is_train=False):
    """
    åº”ç”¨ Frozen ç‰¹å¾åˆ° DataFrame

    å¯¹äº Val/Testï¼šç›´æ¥ç”¨ lookup è¡¨ï¼ˆtrain æœŸé—´çš„ç»Ÿè®¡ï¼‰
    å¯¹äº Trainï¼šéœ€è¦æ£€æŸ¥ last_ts < click_tsï¼Œé¿å…çœ‹åˆ°æœªæ¥æ•°æ®

    Args:
        df: DataFrame with user_id, streamer_id, timestamp columns
        lookups: dict from create_frozen_lookups()
        is_train: æ˜¯å¦æ˜¯è®­ç»ƒé›†ï¼ˆéœ€è¦é¢å¤–çš„ past-only æ£€æŸ¥ï¼‰

    Returns:
        DataFrame with past-only features added
    """
    log(f"Applying frozen features (is_train={is_train})...")

    df = df.copy()
    n = len(df)

    # åˆ›å»º pair keys
    pair_keys = list(zip(df['user_id'], df['streamer_id']))
    timestamps = df['timestamp'].values

    # -------------------------------------------------------------------------
    # Pair features (å¸¦ _past åç¼€ï¼Œè¡¨ç¤ºæ— æ³„æ¼)
    # å¯¹äº Train é›†ï¼šéœ€è¦æ£€æŸ¥ last_ts < click_ts
    # -------------------------------------------------------------------------
    pair_count = np.zeros(n)
    pair_sum = np.zeros(n)
    pair_mean = np.zeros(n)
    pair_std = np.zeros(n)
    pair_max = np.zeros(n)
    pair_last_gap = np.full(n, 999.0)

    for i, (key, click_ts) in enumerate(zip(pair_keys, timestamps)):
        if key in lookups['pair']:
            info = lookups['pair'][key]
            last_ts = info.get('last_ts', np.nan)

            # å…³é”®æ£€æŸ¥ï¼šå¯¹äº Train é›†ï¼Œåªæœ‰ last_ts < click_ts æ‰èƒ½ä½¿ç”¨
            if is_train and not np.isnan(last_ts) and last_ts >= click_ts:
                # è¿™ä¸ª gift å‘ç”Ÿåœ¨ click ä¹‹åï¼Œä¸èƒ½ä½¿ç”¨
                # ä¿æŒé»˜è®¤å€¼ 0
                continue

            pair_count[i] = info.get('count', 0)
            pair_sum[i] = info.get('sum', 0.0)
            pair_mean[i] = info.get('mean', 0.0)
            pair_std[i] = info.get('std', 0.0)
            pair_max[i] = info.get('max', 0.0)

            if not np.isnan(last_ts) and last_ts < click_ts:
                pair_last_gap[i] = (click_ts - last_ts) / (1000 * 3600)

    df['pair_gift_count_past'] = pair_count
    df['pair_gift_sum_past'] = pair_sum
    df['pair_gift_mean_past'] = pair_mean
    df['pair_gift_std_past'] = pair_std
    df['pair_gift_max_past'] = pair_max
    df['pair_last_gift_gap_hours'] = pair_last_gap

    # -------------------------------------------------------------------------
    # User featuresï¼ˆç®€åŒ–ï¼šç›´æ¥ç”¨ lookupï¼Œå› ä¸ºæ˜¯å…¨å±€ç»Ÿè®¡ï¼‰
    # -------------------------------------------------------------------------
    df['user_gift_count_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('count', 0)
    )
    df['user_gift_sum_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('sum', 0.0)
    )
    df['user_gift_mean_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('mean', 0.0)
    )
    df['user_unique_streamers_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('unique_streamers', 0)
    )

    # -------------------------------------------------------------------------
    # Streamer featuresï¼ˆç®€åŒ–ï¼šç›´æ¥ç”¨ lookupï¼‰
    # -------------------------------------------------------------------------
    df['str_gift_count_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('count', 0)
    )
    df['str_gift_sum_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('sum', 0.0)
    )
    df['str_gift_mean_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('mean', 0.0)
    )
    df['str_unique_givers_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('unique_givers', 0)
    )

    # -------------------------------------------------------------------------
    # Watch time features (å†å²è§‚çœ‹æ—¶é•¿)
    # -------------------------------------------------------------------------
    if 'watch_time' in lookups and lookups['watch_time']['user']:
        # User-level å†å²è§‚çœ‹æ—¶é•¿
        df['user_avg_watch_time_past'] = df['user_id'].map(
            lambda x: lookups['watch_time']['user'].get(x, {}).get('mean', 0)
        )
        df['user_total_watch_time_past'] = df['user_id'].map(
            lambda x: lookups['watch_time']['user'].get(x, {}).get('sum', 0)
        )

        # Pair-level å†å²è§‚çœ‹æ—¶é•¿
        pair_watch_mean = np.zeros(n)
        pair_watch_count = np.zeros(n)
        for i, key in enumerate(pair_keys):
            if key in lookups['watch_time']['pair']:
                info = lookups['watch_time']['pair'][key]
                # å¯¹äº Train é›†ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰æœªæ¥æ•°æ®ï¼ˆä½†è§‚çœ‹æ—¶é•¿æ²¡æœ‰æ—¶é—´æˆ³ï¼Œç®€åŒ–å¤„ç†ï¼‰
                pair_watch_mean[i] = info.get('mean', 0)
                pair_watch_count[i] = info.get('count', 0)

        df['pair_avg_watch_time_past'] = pair_watch_mean
        df['pair_watch_count_past'] = pair_watch_count

        # Streamer-level å†å²è¢«è§‚çœ‹æ—¶é•¿
        df['str_avg_watch_time_past'] = df['streamer_id'].map(
            lambda x: lookups['watch_time']['streamer'].get(x, {}).get('mean', 0)
        )

        n_watch_features = 5
    else:
        n_watch_features = 0

    log(f"  Applied {14 + n_watch_features} past-only features", "SUCCESS")

    return df


# =============================================================================
# é™æ€ç‰¹å¾ï¼ˆæ— æ³„æ¼é£é™©ï¼‰
# =============================================================================
def add_static_features(df, user, streamer, room):
    """
    æ·»åŠ é™æ€ç‰¹å¾ï¼ˆprofile ç‰¹å¾ï¼Œæ— æ³„æ¼é£é™©ï¼‰

    Args:
        df: DataFrame
        user: user DataFrame
        streamer: streamer DataFrame
        room: room DataFrame

    Returns:
        DataFrame with static features added
    """
    log("Adding static features...")

    df = df.copy()

    # -------------------------------------------------------------------------
    # User profile features
    # -------------------------------------------------------------------------
    user_cols = ['user_id', 'age', 'gender', 'device_brand', 'device_price',
                 'fans_num', 'follow_num', 'accu_watch_live_cnt', 'accu_watch_live_duration',
                 'is_live_streamer', 'is_photo_author']
    user_cols = [c for c in user_cols if c in user.columns]
    df = df.merge(user[user_cols], on='user_id', how='left')

    # -------------------------------------------------------------------------
    # Streamer profile features
    # -------------------------------------------------------------------------
    str_cols = ['streamer_id', 'fans_user_num', 'fans_group_fans_num',
                'follow_user_num', 'accu_live_cnt', 'accu_live_duration',
                'accu_play_cnt', 'accu_play_duration']
    str_cols = [c for c in str_cols if c in streamer.columns]

    # Rename to avoid conflict with user features
    streamer_subset = streamer[str_cols].copy()
    rename_map = {c: f'str_{c}' for c in str_cols if c != 'streamer_id'}
    streamer_subset = streamer_subset.rename(columns=rename_map)

    df = df.merge(streamer_subset, on='streamer_id', how='left')

    # -------------------------------------------------------------------------
    # Room features
    # -------------------------------------------------------------------------
    room_cols = ['live_id', 'live_type', 'live_content_category']
    room_cols = [c for c in room_cols if c in room.columns]
    room_dedup = room[room_cols].drop_duplicates('live_id')
    df = df.merge(room_dedup, on='live_id', how='left')

    # -------------------------------------------------------------------------
    # Time features (ä» timestamp æå–)
    # -------------------------------------------------------------------------
    ts_dt = pd.to_datetime(df['timestamp'], unit='ms')
    df['hour'] = ts_dt.dt.hour
    df['day_of_week'] = ts_dt.dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)

    log(f"  Added static features, total columns: {df.shape[1]}", "SUCCESS")

    return df


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================
def prepare_dataset(train_days=7, val_days=7, test_days=7, gap_days=0,
                    label_window_hours=1, use_cache=True):
    """
    å‡†å¤‡æ— æ³„æ¼çš„æ•°æ®é›†ï¼ˆä¸»å‡½æ•°ï¼‰

    è¿™æ˜¯æ‰€æœ‰ gift_EVpred å®éªŒçš„æ ‡å‡†å…¥å£ã€‚

    Args:
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val/Val-Test gap å¤©æ•° (default: 0)
        label_window_hours: æ ‡ç­¾çª—å£ï¼ˆå°æ—¶ï¼‰(default: 1)
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (default: True)

    Returns:
        tuple: (train_df, val_df, test_df, lookups)

    Example:
        >>> from gift_EVpred.data_utils import prepare_dataset, get_feature_columns
        >>> train_df, val_df, test_df, lookups = prepare_dataset()
        >>> feature_cols = get_feature_columns(train_df)
    """
    log("=" * 60)
    log("Preparing Leakage-Free Dataset")
    log("=" * 60)

    # 1. Load raw data
    gift, click, user, streamer, room = load_raw_data()

    # 2. Click-level labels (removes watch_live_time automatically)
    click_with_labels = prepare_click_level_labels(gift, click, label_window_hours)

    # 3. 7-7-7 split
    train_df, val_df, test_df = split_by_days(
        click_with_labels, train_days, val_days, test_days, gap_days
    )

    # 4. Get train end timestamp for frozen features
    train_end_ts = train_df['timestamp'].max()

    # 5. Create or load frozen lookups
    cache_key = f"frozen_{train_days}_{val_days}_{test_days}_{gap_days}_{label_window_hours}.pkl"
    cache_path = CACHE_DIR / cache_key

    if use_cache and cache_path.exists():
        log(f"Loading cached lookups from {cache_path}")
        with open(cache_path, 'rb') as f:
            lookups = pickle.load(f)
    else:
        # ä¼ é€’åŸå§‹ click æ•°æ®ï¼ˆå« watch_live_timeï¼‰ç”¨äºè®¡ç®—å†å²è§‚çœ‹æ—¶é•¿
        lookups = create_frozen_lookups(gift, train_end_ts, click=click)
        with open(cache_path, 'wb') as f:
            pickle.dump(lookups, f)
        log(f"Cached lookups to {cache_path}")

    # 6. Apply frozen features (Train éœ€è¦ past-only æ£€æŸ¥)
    train_df = apply_frozen_features(train_df, lookups, is_train=True)
    val_df = apply_frozen_features(val_df, lookups, is_train=False)
    test_df = apply_frozen_features(test_df, lookups, is_train=False)

    # 7. Add static features
    train_df = add_static_features(train_df, user, streamer, room)
    val_df = add_static_features(val_df, user, streamer, room)
    test_df = add_static_features(test_df, user, streamer, room)

    # 8. Create targets
    for df in [train_df, val_df, test_df]:
        df['target'] = np.log1p(df['gift_price_label'])
        df['target_raw'] = df['gift_price_label']
        df['is_gift'] = (df['gift_price_label'] > 0).astype(int)

    # 9. Encode all object/category columns
    for df in [train_df, val_df, test_df]:
        for col in df.columns:
            if df[col].dtype == 'object' or str(df[col].dtype) == 'category':
                df[col] = df[col].fillna('unknown').astype(str)
                df[col] = pd.Categorical(df[col]).codes

    # 10. Fill NaN for numeric columns
    for df in [train_df, val_df, test_df]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)

    log("=" * 60)
    log("Dataset preparation complete!", "SUCCESS")
    log(f"  Train: {len(train_df):,} (gift_rate={train_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Val:   {len(val_df):,} (gift_rate={val_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Test:  {len(test_df):,} (gift_rate={test_df['is_gift'].mean()*100:.2f}%)")
    log("=" * 60)

    return train_df, val_df, test_df, lookups


# =============================================================================
# ç‰¹å¾åˆ—å·¥å…·
# =============================================================================
def get_feature_columns(df):
    """
    è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ metadataã€target å’Œæ³„æ¼ç‰¹å¾ï¼‰

    Args:
        df: DataFrame

    Returns:
        list: feature column names
    """
    exclude = set(EXCLUDE_COLUMNS)
    features = [c for c in df.columns if c not in exclude]

    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰æ³„æ¼ç‰¹å¾
    for f in FORBIDDEN_FEATURES:
        if f in features:
            features.remove(f)
            log(f"Removed forbidden feature: {f}", "WARNING")

    return features


def verify_feature_columns(feature_cols):
    """
    éªŒè¯ç‰¹å¾åˆ—ä¸åŒ…å«æ³„æ¼ç‰¹å¾

    Args:
        feature_cols: list of feature column names

    Raises:
        AssertionError if forbidden features found
    """
    for f in FORBIDDEN_FEATURES:
        assert f not in feature_cols, f"Forbidden feature found: {f}"

    # æ£€æŸ¥ gift ç›¸å…³ç‰¹å¾å¿…é¡»å¸¦ _past åç¼€
    gift_features = [f for f in feature_cols if 'gift' in f.lower() and 'label' not in f]
    for f in gift_features:
        if not f.endswith('_past'):
            log(f"WARNING: Gift feature without _past suffix: {f}", "WARNING")

    log("Feature column verification: PASSED", "SUCCESS")


# =============================================================================
# éªŒè¯å‡½æ•°
# =============================================================================
def verify_time_split(train_df, val_df, test_df):
    """
    éªŒè¯æ—¶é—´åˆ’åˆ†æ­£ç¡®æ€§ï¼ˆæ— æ—¶é—´ç©¿è¶Šï¼‰

    Args:
        train_df, val_df, test_df: DataFrames

    Raises:
        AssertionError if time overlap detected
    """
    train_max = train_df['timestamp'].max()
    val_min = val_df['timestamp'].min()
    val_max = val_df['timestamp'].max()
    test_min = test_df['timestamp'].min()

    assert train_max < val_min, f"Train/Val overlap: {train_max} >= {val_min}"
    assert val_max < test_min, f"Val/Test overlap: {val_max} >= {test_min}"

    log("Time split verification: PASSED", "SUCCESS")
    log(f"  Train max: {pd.to_datetime(train_max, unit='ms')}")
    log(f"  Val min:   {pd.to_datetime(val_min, unit='ms')}")
    log(f"  Test min:  {pd.to_datetime(test_min, unit='ms')}")


def verify_no_leakage(df, gift, n_samples=100):
    """
    éªŒè¯ç‰¹å¾æ— æ³„æ¼ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰

    å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼ŒéªŒè¯ pair_gift_count_past ç­‰äºçœŸå®çš„ past-only countã€‚

    Args:
        df: DataFrame with features
        gift: original gift DataFrame
        n_samples: number of samples to check

    Returns:
        bool: True if passed, False if leakage detected
    """
    log(f"Verifying no leakage ({n_samples} samples)...")

    gift_sorted = gift.sort_values('timestamp')
    errors = []

    sample_idx = np.random.choice(len(df), min(n_samples, len(df)), replace=False)

    for idx in sample_idx:
        row = df.iloc[idx]
        click_ts = row['timestamp']
        user_id = row['user_id']
        streamer_id = row['streamer_id']

        # è®¡ç®—çœŸå®çš„ past-only count
        true_past = gift_sorted[
            (gift_sorted['user_id'] == user_id) &
            (gift_sorted['streamer_id'] == streamer_id) &
            (gift_sorted['timestamp'] < click_ts)  # ä¸¥æ ¼ <
        ]
        true_count = len(true_past)

        # å¯¹æ¯”ç‰¹å¾å€¼
        feature_count = row['pair_gift_count_past']

        if feature_count != true_count:
            errors.append({
                'idx': idx,
                'expected': true_count,
                'got': feature_count,
                'diff': feature_count - true_count
            })

    if errors:
        log(f"Leakage verification: FAILED ({len(errors)}/{n_samples} samples)", "ERROR")
        for e in errors[:3]:
            log(f"  idx={e['idx']}: expected={e['expected']}, got={e['got']}, diff={e['diff']}")
        return False
    else:
        log(f"Leakage verification: PASSED ({n_samples}/{n_samples} samples)", "SUCCESS")
        return True


def run_full_verification(train_df, val_df, test_df, gift, feature_cols):
    """
    è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹

    Args:
        train_df, val_df, test_df: DataFrames
        gift: original gift DataFrame
        feature_cols: list of feature column names

    Returns:
        bool: True if all verifications passed
    """
    log("=" * 60)
    log("Running Full Verification")
    log("=" * 60)

    all_passed = True

    # 1. æ—¶é—´åˆ’åˆ†
    try:
        verify_time_split(train_df, val_df, test_df)
    except AssertionError as e:
        log(f"Time split verification FAILED: {e}", "ERROR")
        all_passed = False

    # 2. ç‰¹å¾åˆ—
    try:
        verify_feature_columns(feature_cols)
    except AssertionError as e:
        log(f"Feature column verification FAILED: {e}", "ERROR")
        all_passed = False

    # 3. æ³„æ¼æ£€æŸ¥
    log("\nVerifying train set...")
    if not verify_no_leakage(train_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying val set...")
    if not verify_no_leakage(val_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying test set...")
    if not verify_no_leakage(test_df, gift, n_samples=50):
        all_passed = False

    log("=" * 60)
    if all_passed:
        log("All verifications PASSED!", "SUCCESS")
    else:
        log("Some verifications FAILED!", "ERROR")
    log("=" * 60)

    return all_passed


# =============================================================================
# CLI å…¥å£
# =============================================================================
if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    print("Gift EVpred Data Utils - Example Usage")
    print("=" * 60)

    # å‡†å¤‡æ•°æ®
    train_df, val_df, test_df, lookups = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)
    print(f"\nFeature columns ({len(feature_cols)}):")
    print(feature_cols[:10], "...")

    # è¿è¡ŒéªŒè¯
    gift, _, _, _, _ = load_raw_data()
    run_full_verification(train_df, val_df, test_df, gift, feature_cols)
