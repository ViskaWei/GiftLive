#!/usr/bin/env python3
"""
Gift EV Prediction - Leakage-Free Data Utilities (Day-Frozen Version)
======================================================================

ç»Ÿä¸€çš„æ— æ³„æ¼æ•°æ®å¤„ç†æ¨¡å—ï¼Œæ‰€æœ‰ gift_EVpred å®éªŒå¿…é¡»ä½¿ç”¨æ­¤æ¨¡å—ã€‚

æ ¸å¿ƒè®¾è®¡ï¼šæŒ‰å¤©å†»ç»“ï¼ˆDay-Frozen / Day-Snapshotï¼‰
- å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
- å®Œå…¨ä¸ä¼šç”¨åˆ°"æœªæ¥"ï¼Œä½†ä¼šä¸¢æ‰"åŒä¸€å¤©æ›´æ—©å‘ç”Ÿçš„å†å²"ï¼ˆä¿å®ˆä½†å®‰å…¨ï¼‰
- è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—æ„é€ é€»è¾‘

Usage:
    from gift_EVpred.data_utils import (
        load_raw_data,
        prepare_dataset,
        get_feature_columns,
        verify_no_leakage
    )

    # æ ‡å‡†ç”¨æ³•
    train_df, val_df, test_df = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)

Author: Viska Wei
Date: 2026-01-18
Version: 2.0 (Day-Frozen)
"""

import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR = Path("/home/swei20/GiftLive")
DATA_DIR = BASE_DIR / "data" / "KuaiLive"
OUTPUT_DIR = BASE_DIR / "gift_EVpred"

# =============================================================================
# å¸¸é‡é…ç½®
# =============================================================================
SEED = 42
np.random.seed(SEED)

# æ³„æ¼ç‰¹å¾é»‘åå•ï¼ˆç»å¯¹ç¦æ­¢ä½¿ç”¨ï¼‰
FORBIDDEN_FEATURES = [
    'watch_live_time',      # ç»“æœæ³„æ¼ï¼šåŒ…å«æ‰“èµåçš„è§‚çœ‹æ—¶é•¿
    'watch_time_log',       # åŒä¸Š
    'watch_time_ratio',     # åŒä¸Š
]

# å¿…é¡»æ’é™¤çš„åˆ—ï¼ˆéç‰¹å¾ï¼‰
EXCLUDE_COLUMNS = [
    'user_id', 'live_id', 'streamer_id', 'timestamp', 'timestamp_dt',
    'gift_price_label', 'target', 'target_raw', 'is_gift',
    'label_end_dt', '_datetime', '_date', 'day',
] + FORBIDDEN_FEATURES


# =============================================================================
# æ—¥å¿—å·¥å…·
# =============================================================================
def log(msg: str, level: str = "INFO"):
    """æ‰“å°æ—¥å¿—"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    prefix = {"INFO": "ğŸ“", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}.get(level, "")
    print(f"[{timestamp}] {prefix} {msg}")


# =============================================================================
# æ•°æ®åŠ è½½
# =============================================================================
def load_raw_data():
    """
    åŠ è½½åŸå§‹æ•°æ®

    Returns:
        tuple: (gift, click, user, streamer, room) DataFrames
    """
    log("Loading raw data...")

    gift = pd.read_csv(DATA_DIR / "gift.csv")
    click = pd.read_csv(DATA_DIR / "click.csv")
    user = pd.read_csv(DATA_DIR / "user.csv")
    streamer = pd.read_csv(DATA_DIR / "streamer.csv")
    room = pd.read_csv(DATA_DIR / "room.csv")

    log(f"Loaded: gift={len(gift):,}, click={len(click):,}, "
        f"user={len(user):,}, streamer={len(streamer):,}, room={len(room):,}")

    return gift, click, user, streamer, room


# =============================================================================
# Click-Level æ ‡ç­¾æ„å»º
# =============================================================================
def prepare_click_level_labels(gift, click, label_window_hours=1):
    """
    æ„å»º Click-level æ ‡ç­¾

    Args:
        gift: gift DataFrame
        click: click DataFrame
        label_window_hours: æ ‡ç­¾çª—å£ï¼ˆå°æ—¶ï¼‰

    Returns:
        DataFrame: click æ•°æ® + gift_price_label åˆ—ï¼ˆ0 æˆ–æ­£æ•°ï¼‰

    æ³¨æ„:
        - ä¼šè‡ªåŠ¨åˆ é™¤ watch_live_time åˆ—ï¼ˆæ³„æ¼ç‰¹å¾ï¼‰
        - Label = click å label_window_hours å†…çš„ gift æ€»é¢
    """
    log(f"Preparing click-level labels (window={label_window_hours}h)...")

    click = click.copy()
    gift = gift.copy()

    # åˆ é™¤æ³„æ¼ç‰¹å¾
    if 'watch_live_time' in click.columns:
        click = click.drop(columns=['watch_live_time'])
        log("Removed watch_live_time (leakage feature)", "WARNING")

    # è½¬æ¢æ—¶é—´
    click['timestamp_dt'] = pd.to_datetime(click['timestamp'], unit='ms')
    gift['timestamp_dt'] = pd.to_datetime(gift['timestamp'], unit='ms')

    # Label window
    click['label_end_dt'] = click['timestamp_dt'] + pd.Timedelta(hours=label_window_hours)

    # Merge and filter
    merged = click[['user_id', 'streamer_id', 'live_id', 'timestamp', 'timestamp_dt', 'label_end_dt']].merge(
        gift[['user_id', 'streamer_id', 'live_id', 'timestamp_dt', 'gift_price']],
        on=['user_id', 'streamer_id', 'live_id'],
        how='left',
        suffixes=('_click', '_gift')
    )

    # Filter: gift within window
    merged = merged[
        (merged['timestamp_dt_gift'] >= merged['timestamp_dt_click']) &
        (merged['timestamp_dt_gift'] <= merged['label_end_dt'])
    ]

    # Aggregate
    gift_agg = merged.groupby(
        ['user_id', 'streamer_id', 'live_id', 'timestamp']
    )['gift_price'].sum().reset_index().rename(columns={'gift_price': 'gift_price_label'})

    # Merge back
    click = click.merge(gift_agg, on=['user_id', 'streamer_id', 'live_id', 'timestamp'], how='left')
    click['gift_price_label'] = click['gift_price_label'].fillna(0)

    # æ¸…ç†ä¸´æ—¶åˆ—
    click = click.drop(columns=['label_end_dt'], errors='ignore')

    log(f"Click-level data: {len(click):,} records, gift_rate={click['gift_price_label'].gt(0).mean()*100:.2f}%")

    return click


# =============================================================================
# 7-7-7 æ•°æ®åˆ’åˆ†
# =============================================================================
def split_by_days(df, train_days=7, val_days=7, test_days=7, gap_days=0):
    """
    æŒ‰å¤©åˆ’åˆ†æ•°æ®é›†ï¼ˆ7-7-7ï¼‰

    Args:
        df: DataFrame with timestamp column (milliseconds)
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val å’Œ Val-Test ä¹‹é—´çš„ gap å¤©æ•° (default: 0)

    Returns:
        tuple: (train_df, val_df, test_df)
    """
    log(f"Splitting data: {train_days}-{val_days}-{test_days} days (gap={gap_days})...")

    df = df.copy()
    df['_datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    dates = sorted(df['_date'].unique())
    min_date = dates[0]
    max_date = dates[-1]

    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_end = min_date + timedelta(days=train_days - 1)
    val_start = train_end + timedelta(days=gap_days + 1)
    val_end = val_start + timedelta(days=val_days - 1)
    test_start = val_end + timedelta(days=gap_days + 1)
    test_end = test_start + timedelta(days=test_days - 1)

    log(f"Data range: {min_date} ~ {max_date} ({len(dates)} days)")
    log(f"  Train: {min_date} ~ {train_end} ({train_days} days)")
    log(f"  Val:   {val_start} ~ {val_end} ({val_days} days)")
    log(f"  Test:  {test_start} ~ {test_end} ({test_days} days)")

    # åˆ’åˆ†
    train_df = df[df['_date'] <= train_end].copy()
    val_df = df[(df['_date'] >= val_start) & (df['_date'] <= val_end)].copy()
    test_df = df[(df['_date'] >= test_start) & (df['_date'] <= test_end)].copy()

    # æ¸…ç†ä¸´æ—¶åˆ—
    for d in [train_df, val_df, test_df]:
        d.drop(columns=['_datetime', '_date'], inplace=True, errors='ignore')

    log(f"Split result: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}")

    return train_df, val_df, test_df


# =============================================================================
# Day-Frozen å†å²ç‰¹å¾ï¼ˆæ— æ³„æ¼ï¼‰
# =============================================================================
def create_day_frozen_features(gift, click):
    """
    åˆ›å»ºæŒ‰å¤©å†»ç»“çš„å†å²ç‰¹å¾ï¼ˆDay-Frozen / Day-Snapshotï¼‰

    æ ¸å¿ƒè®¾è®¡ï¼š
    - å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
    - ä½¿ç”¨ pd.merge_asof(..., by=..., allow_exact_matches=False) é«˜æ•ˆå®ç°
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—é€»è¾‘
    - æ— æ³„æ¼ã€å£å¾„ä¸€è‡´

    Args:
        gift: å…¨é‡ gift DataFrame
        click: å…¨é‡ click DataFrame (å·²åˆ é™¤ watch_live_time)

    Returns:
        DataFrame: click æ•°æ® + å†å²ç‰¹å¾
    """
    log("Creating day-frozen historical features...")

    click = click.copy()
    gift = gift.copy()

    # æ·»åŠ  day åˆ—ï¼ˆè½¬æ¢ä¸º datetime ä»¥ä¾¿ merge_asofï¼‰
    click['day'] = pd.to_datetime(click['timestamp'], unit='ms').dt.normalize()
    gift['day'] = pd.to_datetime(gift['timestamp'], unit='ms').dt.normalize()

    # =========================================================================
    # Pair-level å†å²ç‰¹å¾ (user, streamer)
    # =========================================================================
    log("  Building pair-level features...")

    # æŒ‰å¤©èšåˆ
    pair_day = gift.groupby(['day', 'user_id', 'streamer_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº (merge_asof è¦æ±‚ on åˆ—å…¨å±€æ’åº)
    pair_day = pair_day.sort_values('day').reset_index(drop=True)

    # Cumsum: æˆªè‡³å½“å¤©ï¼ˆå«ï¼‰çš„ç´¯è®¡
    pair_day[['pair_gift_cnt_hist', 'pair_gift_sum_hist']] = pair_day.groupby(
        ['user_id', 'streamer_id']
    )[['gift_cnt_day', 'gift_sum_day']].cumsum()

    # å‡†å¤‡ click æ•°æ®ç”¨äº merge_asof (æŒ‰ day å…¨å±€æ’åº)
    click_sorted = click.sort_values('day').reset_index(drop=True)

    # merge_asof with by: æŒ‰ (user, streamer) åˆ†ç»„ï¼ŒæŸ¥æ‰¾ strictly before çš„æœ€è¿‘è®°å½•
    click_with_pair = pd.merge_asof(
        click_sorted,
        pair_day[['day', 'user_id', 'streamer_id', 'pair_gift_cnt_hist', 'pair_gift_sum_hist']],
        on='day',
        by=['user_id', 'streamer_id'],
        direction='backward',
        allow_exact_matches=False  # ä¸¥æ ¼ < å½“å‰å¤©
    )

    # å¡«å…… NaN ä¸º 0
    click_with_pair[['pair_gift_cnt_hist', 'pair_gift_sum_hist']] = \
        click_with_pair[['pair_gift_cnt_hist', 'pair_gift_sum_hist']].fillna(0)

    # è®¡ç®—å‡å€¼
    click_with_pair['pair_gift_mean_hist'] = (
        click_with_pair['pair_gift_sum_hist'] / click_with_pair['pair_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    Pair features: {(click_with_pair['pair_gift_cnt_hist'] > 0).sum():,} samples have history")

    # =========================================================================
    # User-level å†å²ç‰¹å¾
    # =========================================================================
    log("  Building user-level features...")

    user_day = gift.groupby(['day', 'user_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº
    user_day = user_day.sort_values('day').reset_index(drop=True)
    user_day[['user_gift_cnt_hist', 'user_gift_sum_hist']] = user_day.groupby('user_id')[
        ['gift_cnt_day', 'gift_sum_day']
    ].cumsum()

    # merge_asof by user_id (click_with_pair å·²æŒ‰ day æ’åº)
    click_with_user = pd.merge_asof(
        click_with_pair,
        user_day[['day', 'user_id', 'user_gift_cnt_hist', 'user_gift_sum_hist']],
        on='day',
        by='user_id',
        direction='backward',
        allow_exact_matches=False
    )

    click_with_user[['user_gift_cnt_hist', 'user_gift_sum_hist']] = \
        click_with_user[['user_gift_cnt_hist', 'user_gift_sum_hist']].fillna(0)
    click_with_user['user_gift_mean_hist'] = (
        click_with_user['user_gift_sum_hist'] / click_with_user['user_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    User features: {(click_with_user['user_gift_cnt_hist'] > 0).sum():,} samples have history")

    # =========================================================================
    # Streamer-level å†å²ç‰¹å¾
    # =========================================================================
    log("  Building streamer-level features...")

    str_day = gift.groupby(['day', 'streamer_id'])['gift_price'].agg(
        gift_cnt_day='count',
        gift_sum_day='sum'
    ).reset_index()

    # æŒ‰ day å…¨å±€æ’åº
    str_day = str_day.sort_values('day').reset_index(drop=True)
    str_day[['str_gift_cnt_hist', 'str_gift_sum_hist']] = str_day.groupby('streamer_id')[
        ['gift_cnt_day', 'gift_sum_day']
    ].cumsum()

    # merge_asof by streamer_id (click_with_user å·²æŒ‰ day æ’åº)
    click_with_str = pd.merge_asof(
        click_with_user,
        str_day[['day', 'streamer_id', 'str_gift_cnt_hist', 'str_gift_sum_hist']],
        on='day',
        by='streamer_id',
        direction='backward',
        allow_exact_matches=False
    )

    click_with_str[['str_gift_cnt_hist', 'str_gift_sum_hist']] = \
        click_with_str[['str_gift_cnt_hist', 'str_gift_sum_hist']].fillna(0)
    click_with_str['str_gift_mean_hist'] = (
        click_with_str['str_gift_sum_hist'] / click_with_str['str_gift_cnt_hist'].replace(0, np.nan)
    ).fillna(0)

    log(f"    Streamer features: {(click_with_str['str_gift_cnt_hist'] > 0).sum():,} samples have history")

    click = click_with_str

    # =========================================================================
    # å†å²è§‚çœ‹æ—¶é•¿ç‰¹å¾ï¼ˆå¯é€‰ï¼Œä½¿ç”¨è¿‡å»å¤©çš„ click/watch_timeï¼‰
    # =========================================================================
    # æ³¨æ„ï¼šå½“å‰å®ç°ä¸ä½¿ç”¨ watch_time ç‰¹å¾ï¼Œå› ä¸ºéœ€è¦åŸå§‹ click ä¿ç•™ watch_live_time
    # å¦‚éœ€æ·»åŠ ï¼Œå¯ä»¥åœ¨æ­¤å¤„å®ç°ç±»ä¼¼çš„ cumsum + shift é€»è¾‘

    log("Day-frozen features created!", "SUCCESS")
    log(f"  Total historical features: 9 (pair: 3, user: 3, streamer: 3)")

    return click


# =============================================================================
# é™æ€ç‰¹å¾ï¼ˆæ— æ³„æ¼é£é™©ï¼‰
# =============================================================================
def add_static_features(df, user, streamer, room):
    """
    æ·»åŠ é™æ€ç‰¹å¾ï¼ˆprofile ç‰¹å¾ï¼Œæ— æ³„æ¼é£é™©ï¼‰

    Args:
        df: DataFrame
        user: user DataFrame
        streamer: streamer DataFrame
        room: room DataFrame

    Returns:
        DataFrame with static features added
    """
    log("Adding static features...")

    df = df.copy()

    # -------------------------------------------------------------------------
    # User profile features
    # -------------------------------------------------------------------------
    user_cols = ['user_id', 'age', 'gender', 'device_brand', 'device_price',
                 'fans_num', 'follow_num', 'accu_watch_live_cnt', 'accu_watch_live_duration',
                 'is_live_streamer', 'is_photo_author']
    user_cols = [c for c in user_cols if c in user.columns]
    df = df.merge(user[user_cols], on='user_id', how='left')

    # -------------------------------------------------------------------------
    # Streamer profile features
    # -------------------------------------------------------------------------
    str_cols = ['streamer_id', 'fans_user_num', 'fans_group_fans_num',
                'follow_user_num', 'accu_live_cnt', 'accu_live_duration',
                'accu_play_cnt', 'accu_play_duration']
    str_cols = [c for c in str_cols if c in streamer.columns]

    # Rename to avoid conflict with user features
    streamer_subset = streamer[str_cols].copy()
    rename_map = {c: f'str_{c}' for c in str_cols if c != 'streamer_id'}
    streamer_subset = streamer_subset.rename(columns=rename_map)

    df = df.merge(streamer_subset, on='streamer_id', how='left')

    # -------------------------------------------------------------------------
    # Room features
    # -------------------------------------------------------------------------
    room_cols = ['live_id', 'live_type', 'live_content_category']
    room_cols = [c for c in room_cols if c in room.columns]
    room_dedup = room[room_cols].drop_duplicates('live_id')
    df = df.merge(room_dedup, on='live_id', how='left')

    # -------------------------------------------------------------------------
    # Time features (ä» timestamp æå–)
    # -------------------------------------------------------------------------
    ts_dt = pd.to_datetime(df['timestamp'], unit='ms')
    df['hour'] = ts_dt.dt.hour
    df['day_of_week'] = ts_dt.dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)

    log(f"  Added static features, total columns: {df.shape[1]}", "SUCCESS")

    return df


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================
def prepare_dataset(train_days=7, val_days=7, test_days=7, gap_days=0,
                    label_window_hours=1, use_cache=True):
    """
    å‡†å¤‡æ— æ³„æ¼çš„æ•°æ®é›†ï¼ˆä¸»å‡½æ•°ï¼‰

    è¿™æ˜¯æ‰€æœ‰ gift_EVpred å®éªŒçš„æ ‡å‡†å…¥å£ã€‚

    æ ¸å¿ƒè®¾è®¡ï¼šDay-Frozenï¼ˆæŒ‰å¤©å†»ç»“ï¼‰
    - å¯¹æ¯ä¸ª click çš„ç‰¹å¾ï¼Œåªå…è®¸ç”¨ **ä¹‹å‰çš„å¤©ï¼ˆday < å½“å‰ dayï¼‰**çš„å†å²
    - è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—æ„é€ é€»è¾‘
    - æ— æ³„æ¼ã€å£å¾„ä¸€è‡´

    Args:
        train_days: è®­ç»ƒé›†å¤©æ•° (default: 7)
        val_days: éªŒè¯é›†å¤©æ•° (default: 7)
        test_days: æµ‹è¯•é›†å¤©æ•° (default: 7)
        gap_days: Train-Val/Val-Test gap å¤©æ•° (default: 0)
        label_window_hours: æ ‡ç­¾çª—å£ï¼ˆå°æ—¶ï¼‰(default: 1)
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (default: True)

    Returns:
        tuple: (train_df, val_df, test_df)

    Example:
        >>> from gift_EVpred.data_utils import prepare_dataset, get_feature_columns
        >>> train_df, val_df, test_df = prepare_dataset()
        >>> feature_cols = get_feature_columns(train_df)
    """
    log("=" * 60)
    log("Preparing Leakage-Free Dataset (Day-Frozen Version)")
    log("=" * 60)

    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    CACHE_DIR = OUTPUT_DIR / "features_cache"
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_file = CACHE_DIR / f"day_frozen_features_lw{label_window_hours}h.parquet"

    # 1. Load raw data
    gift, click, user, streamer, room = load_raw_data()

    # 2. å°è¯•ä»ç¼“å­˜åŠ è½½
    if use_cache and cache_file.exists():
        log(f"Loading cached features from {cache_file}")
        click_with_features = pd.read_parquet(cache_file)
        log(f"  Loaded {len(click_with_features):,} records from cache", "SUCCESS")
    else:
        # 2. Click-level labels (removes watch_live_time automatically)
        click_with_labels = prepare_click_level_labels(gift, click, label_window_hours)

        # 3. Create day-frozen historical features (before split!)
        # è¿™æ ·è®­ç»ƒ/éªŒè¯/æµ‹è¯•éƒ½ç”¨åŒä¸€å¥—é€»è¾‘
        click_with_features = create_day_frozen_features(gift, click_with_labels)

        # ä¿å­˜ç¼“å­˜
        if use_cache:
            click_with_features.to_parquet(cache_file)
            log(f"Saved features to cache: {cache_file}", "SUCCESS")

    # 4. 7-7-7 split
    train_df, val_df, test_df = split_by_days(
        click_with_features, train_days, val_days, test_days, gap_days
    )

    # 5. Add static features
    train_df = add_static_features(train_df, user, streamer, room)
    val_df = add_static_features(val_df, user, streamer, room)
    test_df = add_static_features(test_df, user, streamer, room)

    # 6. Create targets
    for df in [train_df, val_df, test_df]:
        df['target'] = np.log1p(df['gift_price_label'])
        df['target_raw'] = df['gift_price_label']
        df['is_gift'] = (df['gift_price_label'] > 0).astype(int)

    # 7. Encode all object/category columns (Train æ‹Ÿåˆï¼ŒVal/Test å¤ç”¨)
    # å…ˆè¯†åˆ«éœ€è¦ç¼–ç çš„åˆ—
    cat_cols = []
    for col in train_df.columns:
        if train_df[col].dtype == 'object' or str(train_df[col].dtype) == 'category':
            cat_cols.append(col)

    # Train æ‹Ÿåˆ categoriesï¼ŒVal/Test å¤ç”¨åŒä¸€æ˜ å°„
    for col in cat_cols:
        # å¡«å…… NaN
        train_df[col] = train_df[col].fillna('unknown').astype(str)
        val_df[col] = val_df[col].fillna('unknown').astype(str)
        test_df[col] = test_df[col].fillna('unknown').astype(str)

        # ç”¨ Train çš„å”¯ä¸€å€¼å»ºç«‹ categoriesï¼ˆåŠ ä¸Š 'unknown' å…œåº•ï¼‰
        train_categories = list(train_df[col].unique())
        if 'unknown' not in train_categories:
            train_categories.append('unknown')

        # åˆ›å»ºå¸¦å›ºå®š categories çš„ Categoricalï¼ŒæœªçŸ¥å€¼æ˜ å°„ä¸º 'unknown'
        for df in [train_df, val_df, test_df]:
            # å°† Val/Test ä¸­ Train æ²¡è§è¿‡çš„å€¼æ›¿æ¢ä¸º 'unknown'
            df[col] = df[col].apply(lambda x: x if x in train_categories else 'unknown')
            df[col] = pd.Categorical(df[col], categories=train_categories).codes

    log(f"  Encoded {len(cat_cols)} categorical columns (train-fitted)", "SUCCESS")

    # 8. Fill NaN for numeric columns
    for df in [train_df, val_df, test_df]:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(0)

    log("=" * 60)
    log("Dataset preparation complete!", "SUCCESS")
    log(f"  Train: {len(train_df):,} (gift_rate={train_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Val:   {len(val_df):,} (gift_rate={val_df['is_gift'].mean()*100:.2f}%)")
    log(f"  Test:  {len(test_df):,} (gift_rate={test_df['is_gift'].mean()*100:.2f}%)")
    log("=" * 60)

    return train_df, val_df, test_df


# =============================================================================
# ç‰¹å¾åˆ—å·¥å…·
# =============================================================================
def get_feature_columns(df):
    """
    è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ metadataã€target å’Œæ³„æ¼ç‰¹å¾ï¼‰

    Args:
        df: DataFrame

    Returns:
        list: feature column names
    """
    exclude = set(EXCLUDE_COLUMNS)
    features = [c for c in df.columns if c not in exclude]

    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰æ³„æ¼ç‰¹å¾
    for f in FORBIDDEN_FEATURES:
        if f in features:
            features.remove(f)
            log(f"Removed forbidden feature: {f}", "WARNING")

    return features


def verify_feature_columns(feature_cols):
    """
    éªŒè¯ç‰¹å¾åˆ—ä¸åŒ…å«æ³„æ¼ç‰¹å¾

    Args:
        feature_cols: list of feature column names

    Raises:
        AssertionError if forbidden features found
    """
    for f in FORBIDDEN_FEATURES:
        assert f not in feature_cols, f"Forbidden feature found: {f}"

    log("Feature column verification: PASSED", "SUCCESS")


# =============================================================================
# éªŒè¯å‡½æ•°
# =============================================================================
def verify_time_split(train_df, val_df, test_df):
    """
    éªŒè¯æ—¶é—´åˆ’åˆ†æ­£ç¡®æ€§ï¼ˆæ— æ—¶é—´ç©¿è¶Šï¼‰

    Args:
        train_df, val_df, test_df: DataFrames

    Raises:
        AssertionError if time overlap detected
    """
    train_max = train_df['timestamp'].max()
    val_min = val_df['timestamp'].min()
    val_max = val_df['timestamp'].max()
    test_min = test_df['timestamp'].min()

    assert train_max < val_min, f"Train/Val overlap: {train_max} >= {val_min}"
    assert val_max < test_min, f"Val/Test overlap: {val_max} >= {test_min}"

    log("Time split verification: PASSED", "SUCCESS")
    log(f"  Train max: {pd.to_datetime(train_max, unit='ms')}")
    log(f"  Val min:   {pd.to_datetime(val_min, unit='ms')}")
    log(f"  Test min:  {pd.to_datetime(test_min, unit='ms')}")


def verify_no_leakage(df, gift, n_samples=100):
    """
    éªŒè¯ç‰¹å¾æ— æ³„æ¼ï¼ˆæŠ½æ ·æ£€æŸ¥ï¼‰

    å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼ŒéªŒè¯ pair_gift_cnt_hist ç­‰äºçœŸå®çš„ past-day countã€‚
    Day-Frozen: åªç”¨ day < å½“å‰ day çš„å†å²

    Args:
        df: DataFrame with features
        gift: original gift DataFrame
        n_samples: number of samples to check

    Returns:
        bool: True if passed, False if leakage detected
    """
    log(f"Verifying no leakage ({n_samples} samples)...")

    gift = gift.copy()
    gift['day'] = pd.to_datetime(gift['timestamp'], unit='ms').dt.normalize()

    errors = []

    sample_idx = np.random.choice(len(df), min(n_samples, len(df)), replace=False)

    for idx in sample_idx:
        row = df.iloc[idx]
        click_day = pd.to_datetime(row['timestamp'], unit='ms').normalize()
        user_id = row['user_id']
        streamer_id = row['streamer_id']

        # è®¡ç®—çœŸå®çš„ past-day count (day < click_day)
        true_past = gift[
            (gift['user_id'] == user_id) &
            (gift['streamer_id'] == streamer_id) &
            (gift['day'] < click_day)  # ä¸¥æ ¼ < å½“å‰å¤©
        ]
        true_count = len(true_past)

        # å¯¹æ¯”ç‰¹å¾å€¼
        feature_count = row['pair_gift_cnt_hist']

        if feature_count != true_count:
            errors.append({
                'idx': idx,
                'day': click_day,
                'expected': true_count,
                'got': feature_count,
                'diff': feature_count - true_count
            })

    if errors:
        log(f"Leakage verification: FAILED ({len(errors)}/{n_samples} samples)", "ERROR")
        for e in errors[:3]:
            log(f"  idx={e['idx']}, day={e['day']}: expected={e['expected']}, got={e['got']}, diff={e['diff']}")
        return False
    else:
        log(f"Leakage verification: PASSED ({n_samples}/{n_samples} samples)", "SUCCESS")
        return True


def run_full_verification(train_df, val_df, test_df, gift, feature_cols):
    """
    è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹

    Args:
        train_df, val_df, test_df: DataFrames
        gift: original gift DataFrame
        feature_cols: list of feature column names

    Returns:
        bool: True if all verifications passed
    """
    log("=" * 60)
    log("Running Full Verification")
    log("=" * 60)

    all_passed = True

    # 1. æ—¶é—´åˆ’åˆ†
    try:
        verify_time_split(train_df, val_df, test_df)
    except AssertionError as e:
        log(f"Time split verification FAILED: {e}", "ERROR")
        all_passed = False

    # 2. ç‰¹å¾åˆ—
    try:
        verify_feature_columns(feature_cols)
    except AssertionError as e:
        log(f"Feature column verification FAILED: {e}", "ERROR")
        all_passed = False

    # 3. æ³„æ¼æ£€æŸ¥
    log("\nVerifying train set...")
    if not verify_no_leakage(train_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying val set...")
    if not verify_no_leakage(val_df, gift, n_samples=50):
        all_passed = False

    log("\nVerifying test set...")
    if not verify_no_leakage(test_df, gift, n_samples=100):
        all_passed = False

    log("=" * 60)
    if all_passed:
        log("All verifications PASSED!", "SUCCESS")
    else:
        log("Some verifications FAILED!", "ERROR")
    log("=" * 60)

    return all_passed


# =============================================================================
# CLI å…¥å£
# =============================================================================
if __name__ == '__main__':
    # ç¤ºä¾‹ç”¨æ³•
    print("Gift EVpred Data Utils - Day-Frozen Version")
    print("=" * 60)

    # å‡†å¤‡æ•°æ®
    train_df, val_df, test_df = prepare_dataset()

    # è·å–ç‰¹å¾åˆ—
    feature_cols = get_feature_columns(train_df)
    print(f"\nFeature columns ({len(feature_cols)}):")
    print(feature_cols[:10], "...")

    # è¿è¡ŒéªŒè¯
    gift, _, _, _, _ = load_raw_data()
    run_full_verification(train_df, val_df, test_df, gift, feature_cols)
