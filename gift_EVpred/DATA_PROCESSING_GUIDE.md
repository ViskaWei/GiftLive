# æ•°æ®å¤„ç†æŒ‡å—ï¼šæ— æ³„æ¼ç‰¹å¾å·¥ç¨‹ä¸ 7-7-7 æ•°æ®åˆ’åˆ†

> **é€‚ç”¨èŒƒå›´**: æ‰€æœ‰ gift_EVpred ç›¸å…³å®éªŒ
> **åˆ›å»ºæ—¥æœŸ**: 2026-01-18
> **çŠ¶æ€**: å¼ºåˆ¶æ‰§è¡Œ

---

## ç›®å½•

1. [æ•°æ®æ³„æ¼é—®é¢˜æ±‡æ€»](#1-æ•°æ®æ³„æ¼é—®é¢˜æ±‡æ€»)
2. [7-7-7 æ•°æ®åˆ’åˆ†è§„èŒƒ](#2-7-7-7-æ•°æ®åˆ’åˆ†è§„èŒƒ)
3. [æ— æ³„æ¼ç‰¹å¾æ„å»ºè§„èŒƒ](#3-æ— æ³„æ¼ç‰¹å¾æ„å»ºè§„èŒƒ)
4. [ç»Ÿä¸€æ•°æ®å¤„ç†ä»£ç ](#4-ç»Ÿä¸€æ•°æ®å¤„ç†ä»£ç )
5. [éªŒè¯æ¸…å•](#5-éªŒè¯æ¸…å•)

---

## 1. æ•°æ®æ³„æ¼é—®é¢˜æ±‡æ€»

### 1.1 å·²ç¡®è®¤çš„æ³„æ¼é—®é¢˜

| é—®é¢˜ç¼–å· | æ³„æ¼ç‰¹å¾ | æ³„æ¼ç±»å‹ | ä¸¥é‡æ€§ | è¯æ® |
|---------|---------|---------|--------|------|
| **P1** | `watch_live_time` | ç»“æœæ³„æ¼ | ğŸ”´ è‡´å‘½ | åŒ…å«æ‰“èµåçš„è§‚çœ‹æ—¶é•¿ |
| **P2** | `pair_gift_mean/sum/count` | æœªæ¥æ³„æ¼ | ğŸ”´ è‡´å‘½ | Feature Importance = 1.4Mï¼ˆå¼‚å¸¸é«˜ï¼‰ |
| **P3** | `pair_seq_N_mean` | æœªæ¥æ³„æ¼ | ğŸ”´ è‡´å‘½ | ä½¿ç”¨å…¨é‡æ•°æ®è®¡ç®—æœ€è¿‘ N æ¬¡ |
| **P4** | `user_total_gift_7d` | æœªæ¥æ³„æ¼ | ğŸ”´ è‡´å‘½ | groupby åŒ…å«æœªæ¥æ ·æœ¬ |
| **P5** | `streamer_recent_revenue` | æœªæ¥æ³„æ¼ | ğŸ”´ è‡´å‘½ | groupby åŒ…å«æœªæ¥æ ·æœ¬ |

### 1.2 æ³„æ¼é—®é¢˜è¯¦è§£

#### P1: watch_live_time ç»“æœæ³„æ¼

**æ•°æ®æ¥æº**: `click.csv`
```csv
user_id,live_id,streamer_id,timestamp,watch_live_time
8505,9342705,392199,1746374400022,2852
```

**é—®é¢˜**:
- `watch_live_time = 2852ms` æ˜¯è¯¥ session çš„**æ€»è§‚çœ‹æ—¶é•¿**
- åŒ…å«ç”¨æˆ·è¿›å…¥ç›´æ’­é—´åçš„**æ‰€æœ‰æ—¶é—´**ï¼ŒåŒ…æ‹¬æ‰“èµåçš„æ—¶é—´
- å› æœå…³ç³»ï¼šé•¿è§‚çœ‹ â†’ æ‰“èµï¼Œè€Œéåè¿‡æ¥
- æ¨¡å‹å¯ä»¥ä» watch_time ç›´æ¥æ¨æ–­æ‰“èµæ¦‚ç‡ï¼ˆæ³„æ¼ï¼‰

**å½±å“**:
- å¦‚æœç”¨æˆ·çœ‹äº† 5 åˆ†é’Ÿåæ‰“èµï¼Œwatch_live_time åŒ…å«æ‰“èµåçš„æ—¶é—´
- è¿™æ˜¯**ç»“æœæ³„æ¼**ï¼šwatch_time æ˜¯æ‰“èµè¡Œä¸ºçš„ç»“æœï¼Œè€ŒéåŸå› 

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ 1: å®Œå…¨ç§»é™¤ï¼ˆæ¨èï¼‰
# ä¸ä½¿ç”¨ watch_live_time ç›¸å…³ç‰¹å¾

# æ–¹æ¡ˆ 2: æˆªæ–­åˆ°é¢„æµ‹æ—¶åˆ»
# éœ€è¦é¢å¤–æ•°æ®æ”¯æŒï¼ˆçŸ¥é“æ¯æ¬¡æ‰“èµå‘ç”Ÿçš„å…·ä½“æ—¶é—´ï¼‰
# ç›®å‰æ•°æ®ä¸æ”¯æŒï¼Œå› æ­¤é€‰æ‹©æ–¹æ¡ˆ 1
```

#### P2-P5: èšåˆç‰¹å¾æœªæ¥æ³„æ¼

**é—®é¢˜ä»£ç **ï¼ˆé”™è¯¯ç¤ºä¾‹ï¼‰:
```python
# âŒ é”™è¯¯ï¼šä½¿ç”¨ train æ—¶é—´èŒƒå›´å†…çš„ ALL gifts
gift_train = gift[
    (gift['timestamp'] >= train_min_ts) &
    (gift['timestamp'] <= train_max_ts)
].copy()

# âŒ é”™è¯¯ï¼šgroupby åŒ…å«å½“å‰å’Œæœªæ¥æ ·æœ¬
pair_stats = gift_train.groupby(['user_id', 'streamer_id']).agg({
    'gift_price': ['count', 'sum', 'mean']
})

# âŒ é”™è¯¯ï¼šç›´æ¥ merge å›å»
df = df.merge(pair_stats, on=['user_id', 'streamer_id'], how='left')
```

**ä¸ºä»€ä¹ˆé”™è¯¯**:
å¯¹äº train_df ä¸­æ—¶é—´ä¸º T çš„æ ·æœ¬ï¼Œpair_gift_mean åŒ…å«äº†ï¼š
1. T ä¹‹å‰çš„å†å²ç¤¼ç‰© âœ… åˆæ³•
2. **T æ—¶åˆ»çš„å½“å‰ç¤¼ç‰©ï¼ˆå¦‚æœæ˜¯æ­£æ ·æœ¬ï¼‰âŒ æ³„æ¼ï¼**
3. **T ä¹‹åã€train_max_ts ä¹‹å‰çš„æœªæ¥ç¤¼ç‰© âŒ æ³„æ¼ï¼**

**è¯Šæ–­è¯æ®**:
```
é¦–æ¬¡æ‰“èµæ ·æœ¬æ£€æŸ¥:
- é¦–æ¬¡æ‰“èµï¼ˆåº”è¯¥ count=0ï¼‰çš„æ ·æœ¬ä¸­ï¼Œ16.8% æœ‰ count > 1
- 100% çš„æ ·æœ¬å­˜åœ¨æ—¶é—´ç©¿è¶Šï¼ˆç‰¹å¾å€¼åŒ…å«æœªæ¥ä¿¡æ¯ï¼‰
- å¹³å‡ count å·®å¼‚ï¼š1.3ï¼Œæœ€å¤§å·®å¼‚ï¼š8
```

---

## 2. 7-7-7 æ•°æ®åˆ’åˆ†è§„èŒƒ

### 2.1 åŸºæœ¬åŸåˆ™

| åŸåˆ™ | è¯´æ˜ |
|------|------|
| **æŒ‰å¤©åˆ’åˆ†** | æŒ‰è‡ªç„¶æ—¥åˆ’åˆ†ï¼Œè€ŒéæŒ‰æ ·æœ¬æ¯”ä¾‹ |
| **æ—¶é—´é¡ºåº** | Train < Val < Testï¼Œä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåº |
| **æ— é‡å ** | ä¸‰ä¸ªé›†åˆçš„æ—¶é—´èŒƒå›´å®Œå…¨ä¸é‡å  |
| **Gap å¯é€‰** | å¯åœ¨ Train/Val å’Œ Val/Test ä¹‹é—´æ·»åŠ  gap é˜²æ­¢è¾¹ç•Œæ³„æ¼ |

### 2.2 KuaiLive æ•°æ®æ—¶é—´èŒƒå›´

```
æ•°æ®æ—¶é—´èŒƒå›´: 2025-05-04 ~ 2025-05-25 (å…± 22 å¤©)

7-7-7 åˆ’åˆ† (æ—  gap):
- Train: Day 1-7  (2025-05-04 ~ 2025-05-10) â†’ 7 å¤©
- Val:   Day 8-14 (2025-05-11 ~ 2025-05-17) â†’ 7 å¤©
- Test:  Day 15-21 (2025-05-18 ~ 2025-05-24) â†’ 7 å¤©
- å‰©ä½™:  Day 22 (2025-05-25) â†’ ä¸ä½¿ç”¨æˆ–ä½œä¸º buffer

7-7-7 åˆ’åˆ† (å¸¦ 1 å¤© gap):
- Train: Day 1-7  (2025-05-04 ~ 2025-05-10) â†’ 7 å¤©
- Gap:   Day 8    (2025-05-11)              â†’ 1 å¤©ï¼ˆä¸ä½¿ç”¨ï¼‰
- Val:   Day 9-15 (2025-05-12 ~ 2025-05-18) â†’ 7 å¤©
- Gap:   Day 16   (2025-05-19)              â†’ 1 å¤©ï¼ˆä¸ä½¿ç”¨ï¼‰
- Test:  Day 17-22 (2025-05-20 ~ 2025-05-25) â†’ 6 å¤©ï¼ˆæœ€åä¸€å¤©ä¸å®Œæ•´å¯èƒ½ï¼‰
```

### 2.3 æ—¶é—´åˆ’åˆ†ä»£ç 

```python
import pandas as pd
from datetime import datetime, timedelta

def split_by_days(df, train_days=7, val_days=7, test_days=7, gap_days=0,
                  timestamp_col='timestamp'):
    """
    æŒ‰å¤©åˆ’åˆ†æ•°æ®é›†

    Args:
        df: DataFrame with timestamp column (in milliseconds)
        train_days: è®­ç»ƒé›†å¤©æ•°
        val_days: éªŒè¯é›†å¤©æ•°
        test_days: æµ‹è¯•é›†å¤©æ•°
        gap_days: Train-Val å’Œ Val-Test ä¹‹é—´çš„ gap å¤©æ•°
        timestamp_col: æ—¶é—´æˆ³åˆ—åï¼ˆæ¯«ç§’ï¼‰

    Returns:
        train_df, val_df, test_df
    """
    # è½¬æ¢ä¸º datetime
    df = df.copy()
    df['_datetime'] = pd.to_datetime(df[timestamp_col], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    # è·å–æ—¥æœŸèŒƒå›´
    dates = sorted(df['_date'].unique())
    min_date = dates[0]

    # è®¡ç®—åˆ‡åˆ†ç‚¹
    train_end_date = min_date + timedelta(days=train_days - 1)
    val_start_date = train_end_date + timedelta(days=gap_days + 1)
    val_end_date = val_start_date + timedelta(days=val_days - 1)
    test_start_date = val_end_date + timedelta(days=gap_days + 1)
    test_end_date = test_start_date + timedelta(days=test_days - 1)

    print(f"Data range: {dates[0]} ~ {dates[-1]} ({len(dates)} days)")
    print(f"Train: {min_date} ~ {train_end_date}")
    print(f"Val:   {val_start_date} ~ {val_end_date}")
    print(f"Test:  {test_start_date} ~ {test_end_date}")

    # åˆ’åˆ†
    train_df = df[df['_date'] <= train_end_date].copy()
    val_df = df[(df['_date'] >= val_start_date) & (df['_date'] <= val_end_date)].copy()
    test_df = df[(df['_date'] >= test_start_date) & (df['_date'] <= test_end_date)].copy()

    # æ¸…ç†ä¸´æ—¶åˆ—
    for d in [train_df, val_df, test_df]:
        d.drop(columns=['_datetime', '_date'], inplace=True)

    print(f"Split result: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}")

    return train_df, val_df, test_df


def get_date_boundaries(df, train_days=7, val_days=7, gap_days=0,
                        timestamp_col='timestamp'):
    """
    è·å–æ—¥æœŸè¾¹ç•Œï¼ˆç”¨äº frozen ç‰¹å¾è®¡ç®—ï¼‰

    Returns:
        dict with train_end_ts, val_start_ts, val_end_ts, test_start_ts
    """
    df = df.copy()
    df['_datetime'] = pd.to_datetime(df[timestamp_col], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    min_date = df['_date'].min()

    train_end_date = min_date + timedelta(days=train_days - 1)
    val_start_date = train_end_date + timedelta(days=gap_days + 1)
    val_end_date = val_start_date + timedelta(days=val_days - 1)
    test_start_date = val_end_date + timedelta(days=gap_days + 1)

    # è½¬æ¢ä¸º timestampï¼ˆæ¯«ç§’ï¼‰
    def date_to_ts_start(d):
        """æ—¥æœŸè½¬æ¢ä¸ºå½“å¤© 00:00:00 çš„æ¯«ç§’æ—¶é—´æˆ³"""
        dt = datetime.combine(d, datetime.min.time())
        return int(dt.timestamp() * 1000)

    def date_to_ts_end(d):
        """æ—¥æœŸè½¬æ¢ä¸ºå½“å¤© 23:59:59.999 çš„æ¯«ç§’æ—¶é—´æˆ³"""
        dt = datetime.combine(d, datetime.max.time())
        return int(dt.timestamp() * 1000)

    return {
        'train_start_ts': date_to_ts_start(min_date),
        'train_end_ts': date_to_ts_end(train_end_date),
        'val_start_ts': date_to_ts_start(val_start_date),
        'val_end_ts': date_to_ts_end(val_end_date),
        'test_start_ts': date_to_ts_start(test_start_date),
    }
```

---

## 3. æ— æ³„æ¼ç‰¹å¾æ„å»ºè§„èŒƒ

### 3.1 ç‰¹å¾æ„å»ºåŸåˆ™

| åŸåˆ™ | è¯´æ˜ | å®ç°æ–¹å¼ |
|------|------|---------|
| **Past-Only** | åªç”¨ t < t_current çš„å†å²æ•°æ® | cumsum + shift æˆ– frozen lookup |
| **ä¸¥æ ¼ä¸ç­‰å¼** | gift_timestamp < click_timestamp | searchsorted(side='left') - 1 |
| **Train éš”ç¦»** | Val/Test åªèƒ½ç”¨ Train æœŸé—´çš„ç»Ÿè®¡ | Frozen lookup table |
| **æ—  watch_time** | å®Œå…¨ç§»é™¤ watch_live_time | ä»ç‰¹å¾åˆ—è¡¨ä¸­åˆ é™¤ |

### 3.2 ä¸¤ç§ç‰¹å¾æ„å»ºæ–¹å¼

#### æ–¹å¼ 1: Frozenï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰

**åŸç†**: åªç”¨ Train æ—¶é—´çª—å£å†…çš„æ•°æ®è®¡ç®—ç»Ÿè®¡é‡ï¼ŒVal/Test åªæŸ¥è¡¨

**ä¼˜ç‚¹**:
- ç®€å•ã€æ— æ³„æ¼é£é™©
- ç¬¦åˆçº¿ä¸Šæ¨ç†åœºæ™¯ï¼ˆæ¨¡å‹éƒ¨ç½²åä¸ä¼šå®æ—¶æ›´æ–°ç»Ÿè®¡ï¼‰
- è®¡ç®—æ•ˆç‡é«˜ï¼ˆé¢„è®¡ç®— lookup tableï¼‰

**ç¼ºç‚¹**:
- Val/Test æœŸé—´çš„æ–° pair æ²¡æœ‰å†å²ä¿¡æ¯ï¼ˆå†·å¯åŠ¨ï¼‰

```python
def create_frozen_lookups(gift, train_end_ts):
    """
    åˆ›å»º Frozen ç‰ˆæœ¬çš„ç‰¹å¾æŸ¥æ‰¾è¡¨

    Args:
        gift: å…¨é‡ gift æ•°æ®
        train_end_ts: Train ç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        dict of lookup tables
    """
    # åªç”¨ train æ—¶é—´çª—å£å†…çš„ gifts
    gift_train = gift[gift['timestamp'] <= train_end_ts].copy()

    lookups = {}

    # 1. Pair-level features
    pair_stats = gift_train.groupby(['user_id', 'streamer_id']).agg({
        'gift_price': ['count', 'sum', 'mean', 'std', 'max'],
        'timestamp': 'max'  # æœ€åä¸€æ¬¡æ‰“èµæ—¶é—´
    }).reset_index()
    pair_stats.columns = ['user_id', 'streamer_id',
                          'pair_gift_count', 'pair_gift_sum', 'pair_gift_mean',
                          'pair_gift_std', 'pair_gift_max', 'pair_last_gift_ts']
    pair_stats['pair_gift_std'] = pair_stats['pair_gift_std'].fillna(0)

    lookups['pair'] = {}
    for _, row in pair_stats.iterrows():
        key = (row['user_id'], row['streamer_id'])
        lookups['pair'][key] = {
            'count': row['pair_gift_count'],
            'sum': row['pair_gift_sum'],
            'mean': row['pair_gift_mean'],
            'std': row['pair_gift_std'],
            'max': row['pair_gift_max'],
            'last_ts': row['pair_last_gift_ts']
        }

    # 2. User-level features
    user_stats = gift_train.groupby('user_id').agg({
        'gift_price': ['count', 'sum', 'mean'],
        'streamer_id': 'nunique'
    }).reset_index()
    user_stats.columns = ['user_id', 'user_gift_count', 'user_gift_sum',
                          'user_gift_mean', 'user_unique_streamers']

    lookups['user'] = {}
    for _, row in user_stats.iterrows():
        lookups['user'][row['user_id']] = {
            'count': row['user_gift_count'],
            'sum': row['user_gift_sum'],
            'mean': row['user_gift_mean'],
            'unique_streamers': row['user_unique_streamers']
        }

    # 3. Streamer-level features
    streamer_stats = gift_train.groupby('streamer_id').agg({
        'gift_price': ['count', 'sum', 'mean'],
        'user_id': 'nunique'
    }).reset_index()
    streamer_stats.columns = ['streamer_id', 'streamer_gift_count', 'streamer_gift_sum',
                              'streamer_gift_mean', 'streamer_unique_givers']

    lookups['streamer'] = {}
    for _, row in streamer_stats.iterrows():
        lookups['streamer'][row['streamer_id']] = {
            'count': row['streamer_gift_count'],
            'sum': row['streamer_gift_sum'],
            'mean': row['streamer_gift_mean'],
            'unique_givers': row['streamer_unique_givers']
        }

    print(f"Created frozen lookups: {len(lookups['pair']):,} pairs, "
          f"{len(lookups['user']):,} users, {len(lookups['streamer']):,} streamers")

    return lookups


def apply_frozen_features(df, lookups, timestamp_col='timestamp'):
    """
    åº”ç”¨ Frozen ç‰¹å¾åˆ° DataFrame

    ä½¿ç”¨å‘é‡åŒ–æ“ä½œåŠ é€Ÿï¼ˆé¿å… iterrowsï¼‰
    """
    df = df.copy()

    # åˆ›å»º lookup æ˜ å°„
    pair_keys = list(zip(df['user_id'], df['streamer_id']))

    # Pair features
    df['pair_gift_count_past'] = [
        lookups['pair'].get(k, {}).get('count', 0) for k in pair_keys
    ]
    df['pair_gift_sum_past'] = [
        lookups['pair'].get(k, {}).get('sum', 0) for k in pair_keys
    ]
    df['pair_gift_mean_past'] = [
        lookups['pair'].get(k, {}).get('mean', 0) for k in pair_keys
    ]

    # Time gap from last gift
    last_ts = np.array([
        lookups['pair'].get(k, {}).get('last_ts', np.nan) for k in pair_keys
    ])
    df['pair_last_gift_gap_hours'] = np.where(
        ~np.isnan(last_ts),
        (df[timestamp_col].values - last_ts) / (1000 * 3600),  # ms to hours
        999  # æ— å†å²
    )

    # User features
    df['user_gift_count_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('count', 0)
    )
    df['user_gift_sum_past'] = df['user_id'].map(
        lambda x: lookups['user'].get(x, {}).get('sum', 0)
    )

    # Streamer features
    df['streamer_gift_count_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('count', 0)
    )
    df['streamer_gift_sum_past'] = df['streamer_id'].map(
        lambda x: lookups['streamer'].get(x, {}).get('sum', 0)
    )

    return df
```

#### æ–¹å¼ 2: Rollingï¼ˆå®éªŒç”¨ï¼Œéœ€ä¸¥æ ¼éªŒè¯ï¼‰

**åŸç†**: æ¯ä¸ªæ ·æœ¬ä½¿ç”¨ t < t_current çš„å†å²æ•°æ®

**ä¼˜ç‚¹**:
- ä¿¡æ¯æ›´ä¸°å¯Œï¼ˆTrain ä¸­åæœŸæ ·æœ¬èƒ½çœ‹åˆ°æ›´å¤šå†å²ï¼‰
- æ›´æ¥è¿‘å®æ—¶æ›´æ–°åœºæ™¯

**ç¼ºç‚¹**:
- å®ç°å¤æ‚ï¼Œå®¹æ˜“å‡ºé”™ï¼ˆ100% æ ·æœ¬æ›¾å‡ºç°æ³„æ¼ï¼‰
- è®¡ç®—æ•ˆç‡ä½ï¼ˆéœ€è¦ per-sample è®¡ç®—æˆ–å¤æ‚çš„ cumsum+searchsortedï¼‰

```python
import numpy as np

def create_rolling_features_vectorized(gift, df):
    """
    ä½¿ç”¨ binary search åˆ›å»º Rolling ç‰¹å¾ï¼ˆæ— æ³„æ¼ï¼‰

    å…³é”®: searchsorted(side='left') - 1 ç¡®ä¿ä¸¥æ ¼ <
    """
    df = df.copy().sort_values('timestamp').reset_index(drop=True)
    gift_sorted = gift.sort_values('timestamp').copy()

    # è®¡ç®—ç´¯ç§¯ç»Ÿè®¡
    gift_sorted['pair_gift_count_cum'] = gift_sorted.groupby(
        ['user_id', 'streamer_id']
    ).cumcount() + 1
    gift_sorted['pair_gift_sum_cum'] = gift_sorted.groupby(
        ['user_id', 'streamer_id']
    )['gift_price'].cumsum()
    gift_sorted['pair_gift_mean_cum'] = (
        gift_sorted['pair_gift_sum_cum'] / gift_sorted['pair_gift_count_cum']
    )

    # æ„å»º lookup ç»“æ„
    pair_lookup = {}
    for (user_id, streamer_id), grp in gift_sorted.groupby(['user_id', 'streamer_id']):
        grp = grp.sort_values('timestamp')
        pair_lookup[(user_id, streamer_id)] = {
            'ts': grp['timestamp'].values,
            'count': grp['pair_gift_count_cum'].values,
            'sum': grp['pair_gift_sum_cum'].values,
            'mean': grp['pair_gift_mean_cum'].values
        }

    # å‘é‡åŒ–æŸ¥æ‰¾
    n = len(df)
    pair_count = np.zeros(n)
    pair_sum = np.zeros(n)
    pair_mean = np.zeros(n)
    pair_last_ts = np.full(n, np.nan)

    for idx in range(n):
        row = df.iloc[idx]
        key = (row['user_id'], row['streamer_id'])
        click_ts = row['timestamp']

        if key in pair_lookup:
            lookup = pair_lookup[key]
            ts_arr = lookup['ts']

            # å…³é”®: searchsorted(side='left') æ‰¾ç¬¬ä¸€ä¸ª >= click_ts çš„ä½ç½®
            # ç„¶å -1 å¾—åˆ°æœ€åä¸€ä¸ª < click_ts çš„ä½ç½®
            pos = np.searchsorted(ts_arr, click_ts, side='left') - 1

            if pos >= 0:  # æœ‰å†å²è®°å½•
                pair_count[idx] = lookup['count'][pos]
                pair_sum[idx] = lookup['sum'][pos]
                pair_mean[idx] = lookup['mean'][pos]
                pair_last_ts[idx] = ts_arr[pos]

    df['pair_gift_count_past'] = pair_count
    df['pair_gift_sum_past'] = pair_sum
    df['pair_gift_mean_past'] = pair_mean
    df['pair_last_gift_gap_hours'] = np.where(
        ~np.isnan(pair_last_ts),
        (df['timestamp'].values - pair_last_ts) / (1000 * 3600),
        999
    )

    return df
```

### 3.3 ç¦æ­¢ä½¿ç”¨çš„ç‰¹å¾

| ç‰¹å¾ | åŸå›  | æ›¿ä»£æ–¹æ¡ˆ |
|------|------|---------|
| `watch_live_time` | ç»“æœæ³„æ¼ | å®Œå…¨ç§»é™¤ |
| `watch_time_log` | åŒä¸Š | å®Œå…¨ç§»é™¤ |
| `watch_time_ratio` | åŒä¸Š | å®Œå…¨ç§»é™¤ |
| `pair_gift_mean` (é past-only) | æœªæ¥æ³„æ¼ | `pair_gift_mean_past` |
| `user_total_gift_7d` (é past-only) | æœªæ¥æ³„æ¼ | `user_gift_sum_past` |

---

## 4. ç»Ÿä¸€æ•°æ®å¤„ç†ä»£ç 

### 4.1 ä¸»å‡½æ•°

```python
#!/usr/bin/env python3
"""
Leakage-Free Data Processing Pipeline
=====================================

ç»Ÿä¸€çš„æ— æ³„æ¼æ•°æ®å¤„ç†æµç¨‹ï¼Œæ‰€æœ‰å®éªŒå¿…é¡»ä½¿ç”¨æ­¤ä»£ç ã€‚

Usage:
    from data_processing import prepare_leakage_free_dataset

    train_df, val_df, test_df = prepare_leakage_free_dataset(
        train_days=7, val_days=7, test_days=7
    )
"""

import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import pickle

DATA_DIR = Path("/home/swei20/GiftLive/data/KuaiLive")
CACHE_DIR = Path("/home/swei20/GiftLive/gift_EVpred/features_cache")
CACHE_DIR.mkdir(exist_ok=True)


def load_raw_data():
    """åŠ è½½åŸå§‹æ•°æ®"""
    gift = pd.read_csv(DATA_DIR / "gift.csv")
    click = pd.read_csv(DATA_DIR / "click.csv")
    user = pd.read_csv(DATA_DIR / "user.csv")
    streamer = pd.read_csv(DATA_DIR / "streamer.csv")
    room = pd.read_csv(DATA_DIR / "room.csv")

    print(f"Loaded: gift={len(gift):,}, click={len(click):,}")
    return gift, click, user, streamer, room


def prepare_click_level_labels(gift, click, label_window_hours=1):
    """
    æ„å»º Click-level æ ‡ç­¾

    Label = click å label_window_hours å†…çš„ gift æ€»é¢ï¼ˆ0 æˆ–æ­£æ•°ï¼‰
    """
    click = click.copy()
    gift = gift.copy()

    # è½¬æ¢æ—¶é—´
    click['timestamp_dt'] = pd.to_datetime(click['timestamp'], unit='ms')
    gift['timestamp_dt'] = pd.to_datetime(gift['timestamp'], unit='ms')

    # Label window
    click['label_end_dt'] = click['timestamp_dt'] + pd.Timedelta(hours=label_window_hours)

    # Merge and filter
    merged = click[['user_id', 'streamer_id', 'live_id', 'timestamp_dt', 'label_end_dt']].merge(
        gift[['user_id', 'streamer_id', 'live_id', 'timestamp_dt', 'gift_price']],
        on=['user_id', 'streamer_id', 'live_id'],
        how='left',
        suffixes=('_click', '_gift')
    )

    # Filter: gift within window
    merged = merged[
        (merged['timestamp_dt_gift'] >= merged['timestamp_dt_click']) &
        (merged['timestamp_dt_gift'] <= merged['label_end_dt'])
    ]

    # Aggregate
    gift_agg = merged.groupby(
        ['user_id', 'streamer_id', 'live_id', 'timestamp_dt_click']
    )['gift_price'].sum().reset_index().rename(columns={
        'timestamp_dt_click': 'timestamp_dt',
        'gift_price': 'gift_price_label'
    })

    # Merge back
    click = click.merge(
        gift_agg,
        on=['user_id', 'streamer_id', 'live_id', 'timestamp_dt'],
        how='left'
    )
    click['gift_price_label'] = click['gift_price_label'].fillna(0)

    # æ¸…ç†: åˆ é™¤ watch_live_timeï¼ˆæ³„æ¼ç‰¹å¾ï¼‰
    if 'watch_live_time' in click.columns:
        click = click.drop(columns=['watch_live_time'])
        print("Removed watch_live_time (leakage feature)")

    print(f"Click-level labels: {len(click):,} records")
    print(f"Gift rate: {(click['gift_price_label'] > 0).mean()*100:.2f}%")

    return click


def split_by_days(df, train_days=7, val_days=7, test_days=7, gap_days=0):
    """æŒ‰å¤©åˆ’åˆ†æ•°æ®é›†ï¼ˆ7-7-7ï¼‰"""
    df = df.copy()
    df['_datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
    df['_date'] = df['_datetime'].dt.date

    dates = sorted(df['_date'].unique())
    min_date = dates[0]

    train_end = min_date + timedelta(days=train_days - 1)
    val_start = train_end + timedelta(days=gap_days + 1)
    val_end = val_start + timedelta(days=val_days - 1)
    test_start = val_end + timedelta(days=gap_days + 1)
    test_end = test_start + timedelta(days=test_days - 1)

    print(f"Data range: {dates[0]} ~ {dates[-1]} ({len(dates)} days)")
    print(f"Train: {min_date} ~ {train_end} ({train_days} days)")
    print(f"Val:   {val_start} ~ {val_end} ({val_days} days)")
    print(f"Test:  {test_start} ~ {test_end} ({test_days} days)")

    train_df = df[df['_date'] <= train_end].copy()
    val_df = df[(df['_date'] >= val_start) & (df['_date'] <= val_end)].copy()
    test_df = df[(df['_date'] >= test_start) & (df['_date'] <= test_end)].copy()

    for d in [train_df, val_df, test_df]:
        d.drop(columns=['_datetime', '_date'], inplace=True)

    print(f"Split: Train={len(train_df):,}, Val={len(val_df):,}, Test={len(test_df):,}")

    return train_df, val_df, test_df


def create_frozen_lookups(gift, train_end_ts):
    """åˆ›å»º Frozen ç‰¹å¾æŸ¥æ‰¾è¡¨"""
    gift_train = gift[gift['timestamp'] <= train_end_ts].copy()

    lookups = {}

    # Pair-level
    pair_stats = gift_train.groupby(['user_id', 'streamer_id']).agg({
        'gift_price': ['count', 'sum', 'mean'],
        'timestamp': 'max'
    }).reset_index()
    pair_stats.columns = ['user_id', 'streamer_id', 'count', 'sum', 'mean', 'last_ts']

    lookups['pair'] = {
        (r['user_id'], r['streamer_id']): {
            'count': r['count'], 'sum': r['sum'], 'mean': r['mean'], 'last_ts': r['last_ts']
        } for _, r in pair_stats.iterrows()
    }

    # User-level
    user_stats = gift_train.groupby('user_id')['gift_price'].agg(['count', 'sum', 'mean']).reset_index()
    lookups['user'] = {
        r['user_id']: {'count': r['count'], 'sum': r['sum'], 'mean': r['mean']}
        for _, r in user_stats.iterrows()
    }

    # Streamer-level
    str_stats = gift_train.groupby('streamer_id')['gift_price'].agg(['count', 'sum', 'mean']).reset_index()
    lookups['streamer'] = {
        r['streamer_id']: {'count': r['count'], 'sum': r['sum'], 'mean': r['mean']}
        for _, r in str_stats.iterrows()
    }

    print(f"Frozen lookups: {len(lookups['pair']):,} pairs, "
          f"{len(lookups['user']):,} users, {len(lookups['streamer']):,} streamers")

    return lookups


def apply_frozen_features(df, lookups):
    """åº”ç”¨ Frozen ç‰¹å¾"""
    df = df.copy()

    pair_keys = list(zip(df['user_id'], df['streamer_id']))

    # Pair features
    df['pair_gift_count_past'] = [lookups['pair'].get(k, {}).get('count', 0) for k in pair_keys]
    df['pair_gift_sum_past'] = [lookups['pair'].get(k, {}).get('sum', 0) for k in pair_keys]
    df['pair_gift_mean_past'] = [lookups['pair'].get(k, {}).get('mean', 0) for k in pair_keys]

    last_ts = np.array([lookups['pair'].get(k, {}).get('last_ts', np.nan) for k in pair_keys])
    df['pair_last_gift_gap_hours'] = np.where(
        ~np.isnan(last_ts),
        (df['timestamp'].values - last_ts) / (1000 * 3600),
        999
    )

    # User features
    df['user_gift_count_past'] = df['user_id'].map(lambda x: lookups['user'].get(x, {}).get('count', 0))
    df['user_gift_sum_past'] = df['user_id'].map(lambda x: lookups['user'].get(x, {}).get('sum', 0))

    # Streamer features
    df['str_gift_count_past'] = df['streamer_id'].map(lambda x: lookups['streamer'].get(x, {}).get('count', 0))
    df['str_gift_sum_past'] = df['streamer_id'].map(lambda x: lookups['streamer'].get(x, {}).get('sum', 0))

    return df


def add_static_features(df, user, streamer, room):
    """æ·»åŠ é™æ€ç‰¹å¾ï¼ˆæ— æ³„æ¼é£é™©ï¼‰"""
    # User features
    user_cols = ['user_id', 'age', 'gender', 'device_brand', 'device_price',
                 'fans_num', 'follow_num', 'accu_watch_live_cnt', 'accu_watch_live_duration']
    df = df.merge(user[user_cols], on='user_id', how='left')

    # Streamer features
    str_cols = ['streamer_id', 'fans_user_num', 'fans_group_fans_num', 'accu_live_cnt']
    df = df.merge(streamer[str_cols], on='streamer_id', how='left')

    # Room features
    room_cols = ['live_id', 'live_type', 'live_content_category']
    room_dedup = room[room_cols].drop_duplicates('live_id')
    df = df.merge(room_dedup, on='live_id', how='left')

    # Time features
    df['hour'] = pd.to_datetime(df['timestamp'], unit='ms').dt.hour
    df['day_of_week'] = pd.to_datetime(df['timestamp'], unit='ms').dt.dayofweek
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)

    return df


def prepare_leakage_free_dataset(train_days=7, val_days=7, test_days=7,
                                  gap_days=0, use_cache=True):
    """
    ä¸»å‡½æ•°: å‡†å¤‡æ— æ³„æ¼çš„æ•°æ®é›†

    Returns:
        train_df, val_df, test_df: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„ DataFrames
    """
    print("="*60)
    print("Preparing Leakage-Free Dataset")
    print("="*60)

    # Load data
    gift, click, user, streamer, room = load_raw_data()

    # Click-level labels (removes watch_live_time)
    click_with_labels = prepare_click_level_labels(gift, click)

    # 7-7-7 split
    train_df, val_df, test_df = split_by_days(
        click_with_labels, train_days, val_days, test_days, gap_days
    )

    # Get train end timestamp for frozen features
    train_end_ts = train_df['timestamp'].max()

    # Cache key
    cache_key = f"frozen_{train_days}_{val_days}_{test_days}_{gap_days}.pkl"
    cache_path = CACHE_DIR / cache_key

    if use_cache and cache_path.exists():
        print(f"Loading cached lookups from {cache_path}")
        with open(cache_path, 'rb') as f:
            lookups = pickle.load(f)
    else:
        lookups = create_frozen_lookups(gift, train_end_ts)
        with open(cache_path, 'wb') as f:
            pickle.dump(lookups, f)
        print(f"Cached lookups to {cache_path}")

    # Apply features
    train_df = apply_frozen_features(train_df, lookups)
    val_df = apply_frozen_features(val_df, lookups)
    test_df = apply_frozen_features(test_df, lookups)

    # Add static features
    train_df = add_static_features(train_df, user, streamer, room)
    val_df = add_static_features(val_df, user, streamer, room)
    test_df = add_static_features(test_df, user, streamer, room)

    # Create targets
    for df in [train_df, val_df, test_df]:
        df['target'] = np.log1p(df['gift_price_label'])
        df['target_raw'] = df['gift_price_label']
        df['is_gift'] = (df['gift_price_label'] > 0).astype(int)

    print("="*60)
    print("Dataset preparation complete!")
    print(f"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}")
    print(f"Train gift rate: {train_df['is_gift'].mean()*100:.2f}%")
    print("="*60)

    return train_df, val_df, test_df


# ç‰¹å¾åˆ—è·å–å‡½æ•°
def get_feature_columns(df):
    """è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ metadata å’Œ targetï¼‰"""
    exclude = {
        'user_id', 'live_id', 'streamer_id', 'timestamp', 'timestamp_dt',
        'gift_price_label', 'target', 'target_raw', 'is_gift',
        'label_end_dt', 'watch_live_time'  # ç¡®ä¿ watch_live_time è¢«æ’é™¤
    }
    return [c for c in df.columns if c not in exclude]
```

### 4.2 ä½¿ç”¨ç¤ºä¾‹

```python
# æ ‡å‡†ç”¨æ³•
from data_processing import prepare_leakage_free_dataset, get_feature_columns

# å‡†å¤‡æ•°æ®ï¼ˆ7-7-7 åˆ’åˆ†ï¼Œæ—  gapï¼‰
train_df, val_df, test_df = prepare_leakage_free_dataset(
    train_days=7, val_days=7, test_days=7, gap_days=0
)

# è·å–ç‰¹å¾åˆ—
feature_cols = get_feature_columns(train_df)
print(f"Features: {len(feature_cols)}")

# æ£€æŸ¥æ— æ³„æ¼
assert 'watch_live_time' not in feature_cols
assert all('past' in f for f in feature_cols if 'gift' in f)

# è®­ç»ƒæ¨¡å‹
X_train = train_df[feature_cols]
y_train = train_df['target']
# ...
```

---

## 5. éªŒè¯æ¸…å•

### 5.1 æ•°æ®åˆ’åˆ†éªŒè¯

```python
def verify_time_split(train_df, val_df, test_df):
    """éªŒè¯æ—¶é—´åˆ’åˆ†æ­£ç¡®æ€§"""
    train_max = train_df['timestamp'].max()
    val_min = val_df['timestamp'].min()
    val_max = val_df['timestamp'].max()
    test_min = test_df['timestamp'].min()

    assert train_max < val_min, f"Train/Val overlap: {train_max} >= {val_min}"
    assert val_max < test_min, f"Val/Test overlap: {val_max} >= {test_min}"

    print("Time split verification: PASSED")
    print(f"  Train max: {pd.to_datetime(train_max, unit='ms')}")
    print(f"  Val min:   {pd.to_datetime(val_min, unit='ms')}")
    print(f"  Val max:   {pd.to_datetime(val_max, unit='ms')}")
    print(f"  Test min:  {pd.to_datetime(test_min, unit='ms')}")
```

### 5.2 ç‰¹å¾æ³„æ¼éªŒè¯

```python
def verify_no_leakage(df, gift, n_samples=100):
    """éªŒè¯ç‰¹å¾æ— æ³„æ¼"""
    import numpy as np

    gift_sorted = gift.sort_values('timestamp')
    errors = []

    sample_idx = np.random.choice(len(df), min(n_samples, len(df)), replace=False)

    for idx in sample_idx:
        row = df.iloc[idx]
        click_ts = row['timestamp']
        user_id = row['user_id']
        streamer_id = row['streamer_id']

        # è®¡ç®—çœŸå®çš„ past-only count
        true_past = gift_sorted[
            (gift_sorted['user_id'] == user_id) &
            (gift_sorted['streamer_id'] == streamer_id) &
            (gift_sorted['timestamp'] < click_ts)  # ä¸¥æ ¼ <
        ]
        true_count = len(true_past)

        # å¯¹æ¯”ç‰¹å¾å€¼
        feature_count = row['pair_gift_count_past']

        if feature_count != true_count:
            errors.append({
                'idx': idx,
                'expected': true_count,
                'got': feature_count,
                'diff': feature_count - true_count
            })

    if errors:
        print(f"Leakage verification: FAILED ({len(errors)}/{n_samples} samples)")
        for e in errors[:3]:
            print(f"  idx={e['idx']}: expected={e['expected']}, got={e['got']}")
        return False
    else:
        print(f"Leakage verification: PASSED ({n_samples} samples)")
        return True
```

### 5.3 ç‰¹å¾åˆ—éªŒè¯

```python
def verify_feature_columns(feature_cols):
    """éªŒè¯ç‰¹å¾åˆ—ä¸åŒ…å«æ³„æ¼ç‰¹å¾"""
    forbidden = ['watch_live_time', 'watch_time_log', 'watch_time_ratio']

    for f in forbidden:
        assert f not in feature_cols, f"Forbidden feature found: {f}"

    # æ£€æŸ¥ gift ç›¸å…³ç‰¹å¾å¿…é¡»å¸¦ _past åç¼€
    gift_features = [f for f in feature_cols if 'gift' in f.lower()]
    for f in gift_features:
        if not f.endswith('_past') and 'label' not in f:
            print(f"WARNING: Gift feature without _past suffix: {f}")

    print("Feature column verification: PASSED")
```

### 5.4 å®Œæ•´éªŒè¯æµç¨‹

```python
def run_full_verification(train_df, val_df, test_df, gift, feature_cols):
    """è¿è¡Œå®Œæ•´éªŒè¯"""
    print("="*60)
    print("Running Full Verification")
    print("="*60)

    # 1. æ—¶é—´åˆ’åˆ†
    verify_time_split(train_df, val_df, test_df)

    # 2. ç‰¹å¾æ³„æ¼
    print("\nVerifying train set...")
    verify_no_leakage(train_df, gift, n_samples=100)

    print("\nVerifying val set...")
    verify_no_leakage(val_df, gift, n_samples=100)

    print("\nVerifying test set...")
    verify_no_leakage(test_df, gift, n_samples=100)

    # 3. ç‰¹å¾åˆ—
    verify_feature_columns(feature_cols)

    print("\n" + "="*60)
    print("All verifications PASSED!")
    print("="*60)
```

---

## é™„å½•: å¿«é€Ÿå‚è€ƒ

### å¿…é¡»éµå®ˆçš„è§„åˆ™

| # | è§„åˆ™ | æ£€æŸ¥æ–¹å¼ |
|---|------|---------|
| 1 | **ä¸ä½¿ç”¨ watch_live_time** | `'watch_live_time' not in features` |
| 2 | **7-7-7 æŒ‰å¤©åˆ’åˆ†** | `train_max < val_min < test_min` |
| 3 | **Frozen ç‰¹å¾ç”¨ Train æ•°æ®** | `gift_ts <= train_end_ts` |
| 4 | **Past-only ä¸¥æ ¼ <** | `gift_ts < click_ts` |
| 5 | **æ‰€æœ‰ gift ç‰¹å¾å¸¦ _past åç¼€** | `feature.endswith('_past')` |

### å¸¸è§é”™è¯¯

| é”™è¯¯ | åæœ | æ­£ç¡®åšæ³• |
|------|------|---------|
| ä½¿ç”¨ `groupby().agg()` å…¨é‡æ•°æ® | 100% æ ·æœ¬æ³„æ¼ | ä½¿ç”¨ frozen lookup æˆ– cumsum+searchsorted |
| ä½¿ç”¨ watch_live_time | ç»“æœæ³„æ¼ | å®Œå…¨ç§»é™¤ |
| æŒ‰æ ·æœ¬æ¯”ä¾‹åˆ’åˆ† | æ—¶é—´ç©¿è¶Š | æŒ‰å¤©åˆ’åˆ† |
| `searchsorted(side='right')` | åŒ…å«å½“å‰æ ·æœ¬ | ä½¿ç”¨ `side='left'` ç„¶å `-1` |

---

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0
> **æœ€åæ›´æ–°**: 2026-01-18
> **ç»´æŠ¤è€…**: Viska Wei
