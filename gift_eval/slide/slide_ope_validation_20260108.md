# OPE 验证：高方差打赏场景的离线策略评估 — 5min 汇报

> **Experiment**: EXP-20260108-gift-eval-01  \
> **Author**: Viska Wei  \
> **Date**: 2026-01-08  \
> **Language**: 中文

---

## SNIPS 在高方差场景达到 <10% 误差，Gate-1 关闭

- **目的**：验证 OPE 方法在重尾打赏场景能否达到 <10% RelErr，用于策略离线对比，减少 A/B 成本
- **X 一句话**：$X := \underbrace{\text{OPE (SNIPS)}}_{\text{离线策略评估}}\;\xrightarrow{\text{自归一化重要性采样}}\;\underbrace{\text{策略排序}}_{\text{无需上线}}$
  - Why: A/B 成本高，需要离线预筛
  - How: 打赏 Gini=0.94，IPS 权重爆炸，SNIPS 归一化控制方差
- **I/O**：日志 (s, a, r, propensity) → 策略价值估计 $\hat{V}(\pi_e)$ → 对比 Ground Truth

| 类型 | 符号 | 说明 | 示例 |
|------|------|------|------|
| 🫐 输入 | $\mathcal{D}$ | 行为日志 (含 propensity) | 5000 条 |
| 🫐 输入 | $\pi_e$ | 待评估策略 | Greedy/Softmax/Concave |
| 🫐 输出 | $\hat{V}(\pi_e)$ | 策略价值估计 | 预期收益 |
| 📊 指标 | RelErr | 相对误差 | <10% 通过 |
| 🍁 基线 | IPS | 标准重要性采样 | 方差大 |

<details>
<summary><b>note</b></summary>

大家好，今天分享 OPE 验证实验的结果。

一句话结论：**SNIPS 在高方差打赏场景达到 0.57%-9.97% 相对误差，Gate-1 关闭，可用于策略离线对比**。

这个实验要解决的痛点是：线上 A/B 成本很高，我们希望在上线前就能筛掉明显不好的策略。问题是打赏金额分布极度重尾，Gini 系数 0.94，传统的 IPS 方法权重容易爆炸。

我们的输入是行为策略产生的日志，必须包含 propensity（即行为策略选择这个动作的概率）；输出是对目标策略价值的估计；评估标准是相对误差小于 10%。

</details>

---

## 方法 + 流程 + 关键结果

- **算法**：
  - IPS: $\hat{V} = \frac{1}{n}\sum w_i r_i$，权重 $w_i = \pi_e(a)/\pi_b(a)$，高方差时爆炸
  - SNIPS: $\hat{V} = \frac{\sum w_i r_i}{\sum w_i}$，自归一化消除极端权重
  - DR: 结合 IPS + Q 函数，理论双稳健，但 Q 函数难估计

- **流程**：
```
实验流程
│
├── 1. 准备环境：SimulatorV2+ (500 users × 50 streamers)
├── 2. 生成日志：ε-greedy (ε=0.3) 收集 5000 条，含 propensity
├── 3. 对比方法：IPS / SNIPS / DR × 3 目标策略 × 20 重复
├── 4. 核心循环 ⭐：estimate = OPE(logs, target) → RelErr = |est - truth| / truth
├── 5. 评估：哪种方法 RelErr < 10%？最优探索率？最小样本量？
└── 6. 产出：6 张图 + 数值表 + Gate-1 决策
```

- **结果（只放胜负手）**：

| 策略 | IPS | SNIPS | DR |
|---:|---:|---:|---:|
| Softmax | 1.72% | **0.57%** ✅ | 18.78% |
| Concave | 8.79% | **4.31%** ✅ | 12.07% |
| Greedy | 15.34% | **9.97%** ✅ | 21.74% |

<details>
<summary><b>note</b></summary>

方法层面，我们对比了三类 OPE 方法：

IPS 是标准的重要性采样，用权重修正分布偏移，但在高方差场景权重容易爆炸；SNIPS 通过自归一化，把权重变成相对权重，有效控制方差；DR 理论上双稳健，但依赖 Q 函数质量，在我们场景下表现不好。

流程上，我们用 Simulator 生成日志，对 Greedy、Softmax、Concave 三种目标策略分别评估，每种方法重复 20 次。

关键结果：SNIPS 在所有策略上都达到了 <10% 相对误差。Softmax 策略最容易评估，因为它和行为策略相似，权重接近 1；Greedy 是确定性策略，权重极端，但 SNIPS 仍然控制在 9.97%。DR 表现不好，因为高方差奖励的 Q 函数难以准确估计。

</details>

---

## 结论 + 决策 + 下一步

- **金句**：「高方差场景，归一化比裁剪更优雅 —— SNIPS 0.57%-9.97%，Gate-1 关闭」
  - 机制层：自归一化消除极端权重的主导效应，牺牲少量偏差换取大幅方差降低
  - 证据层：跨 3 种策略、20 次重复稳定 <10%；样本量 ≥5000 后收敛
- **决策**：Gate-1 关闭，SNIPS 作为主 OPE 方法；条件：ε≥0.3，n≥5000，必须记录 propensity
- **权衡**：
  - 赚到：无需上线即可筛策略，节省 A/B 成本
  - 付出：需要日志系统改造（加 propensity），需保持探索率
- **配置建议**：行为策略 ε=0.3，日志量 ≥5000，差异 <5% 的策略需 Simulator 验证
- **下一步**：
  - 🔴 P0：线上日志添加 propensity 字段
  - 🟡 P1：实现 Bootstrap 置信区间估计
  - 🟡 P1：设计 OPE 粗筛 + Simulator 精筛流程

<details>
<summary><b>note</b></summary>

总结一下，核心结论是：**在高方差打赏场景，归一化比裁剪更优雅**。

SNIPS 达到 0.57%-9.97% 相对误差，Gate-1 可以关闭了。这意味着我们可以用 SNIPS 做策略离线对比，在上线前就筛掉明显不好的策略。

使用条件：行为策略探索率至少 0.3，日志量至少 5000，必须记录 propensity。

权衡：我们赚到的是无需上线就能评估策略；付出的是需要改造日志系统，而且线上需要保持一定探索率。

下一步：最紧急的是在线上日志里加 propensity 字段，这是 OPE 能用的前提；然后是实现置信区间估计，用于判断策略差异是否显著；最后是设计 OPE + Simulator 的联合评估流程。

</details>
