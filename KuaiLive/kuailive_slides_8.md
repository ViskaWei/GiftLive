---
# KuaiLive：打赏预测与全局分配优化

- 目标：最大化 **长期全局收益** = Revenue + 用户留存/满意度 + 主播生态健康
- 数据：KuaiLive（21天，多行为，动态候选集）
- 我做了什么：全方位 EDA → 明确关键结构性事实 → 推导可落地的“预测层→分配层→评估层”方案

Notes:
- 这次汇报不讲“能不能把某个主播做爆”，而讲如何在真实直播生态约束下做长期全局最优。
- 关键是：打赏极稀疏+重尾+高方差 + 在线分配是资源约束问题。
---
# 1. 数据与任务：直播与传统推荐的本质差异

- **动态候选集**：直播间有 start/end，某时刻可推荐集合在变化
- **多行为层次**：click → (like/comment) → gift（成本递增）
- **负反馈**：曝光未点/跳过可用于 CTR/停留建模
- 任务拆解：
  - 估计层：session 内 gift 概率/金额/EV
  - 决策层：在约束下分配高价值曝光（不是单点排序）

Notes:
- 如果不重建动态候选集，离线指标会虚高且策略不可迁移。
- 多行为让我们能用密集信号扶起稀疏打赏。
---
# 2. 数据事实①：打赏“即时冲动”，窗口极短

- **90.36%** 打赏发生在 session 前 **10%** 时间
- session 级 CVR **1.57%**（与 click-level 1.48% 接近）→ session 粒度可用
- 含义：
  - 预测：首 10% 窗口特征是主杠杆
  - 产品：进房早期的触发/排序/引导决定大部分收益

Notes:
- 这直接否定“打赏主要靠观看时长累积”的直觉。
- 也意味着很多复杂的延迟建模收益不大（后面会讲验证结果）。
---
# 3. 数据事实②：增长来自“转化效率”，不是挪存量

- 用户象限：**17.6%** 用户是“高观看低付费”（4,178）→ 最大待转化池
- 主播象限：**18.2%** 主播“高吸引低变现”（82,540）→ 供给侧效率低
- 含义：
  - 增量策略：提升“观看→付费”效率
  - 分配策略：别只堆头部；找“有观看但缺转化”的结构性改造点

Notes:
- 这两类人/供给是我们能做增量的地方：不是把大哥从A搬到B。
---
# 4. 数据事实③：极稀疏 + 极重尾 + 高度专一

- per-click gift rate **1.48%**；金额 P50=2，P99≈1488；User Gini≈0.94
- 付费高度专一：gift loyalty P50=100%（付费几乎只给 Top1 主播）
- 含义：
  - 学习：必须用 log1p / 负样本信息 / 方差控制
  - 决策：不能“硬迁移大哥”，需要分层：保底偏好 + 受控探索

Notes:
- 这解释了为什么简单“最大化短期EV”会导致极端集中，且生态风险很大。
---
# 5. 已验证的模型结论：两段式/延迟校正不是主线

- 公平对比（统一 click 全量候选集）：
  - Direct Regression：Top-1% capture **54.5%**
  - Two-Stage：Top-1% capture **35.8%**（但 NDCG@100 更高）
- 延迟校正：延迟中位数 **0s**，Chapelle 加权校准变差
- 含义：
  - 主干：Direct regression + 强特征 + 多任务
  - Two-stage：仅作为“金主细排”备选，不当默认

Notes:
- 关键点是“公平对比”：不同数据集/候选集下的对比都不可靠。
---
# 6. 解决方案：预测层→分配层→评估层（闭环）

- 预测层（session）：多任务输出
  - EV（短期收益）+ 满意度 proxy（停留/互动）+ 留存 proxy（复访）
- 分配层（全局）：凹收益 + 影子价格 + 约束
  - 凹收益防止堆头部；约束处理“大哥稀缺/主播承接上限/生态护栏”
- 评估层：Simulator + OPE
  - 没线上AB也能做低风险策略迭代

Notes:
- 把“把分数排出来”升级为“资源分配”。
- 这个框架能把目标、约束、风险都显式化。
---
# 7. 下一步实验：最快闭环路径（2周）

- Week1：Gate-1（估计层可用）
  - Direct regression（session+首10%窗口+pair特征）+ ablation
  - 指标：Top-1% capture + NDCG@K + ECE
- Week2：Gate-2/3（分配层原型+评估层）
  - Simulator V1：只校准3个事实（即时、重尾、专一）
  - 凹收益分配：先无复杂约束，证明“更好生态/相近收益”

Notes:
- 我优先把“能不能控制 trade-off”跑通，再精细化模型。
---
# 8. 你需要我拍板/资源支持的点

- 目标函数权重：Revenue vs 留存proxy vs 生态指标（护栏阈值）
- 线上实验策略：是否能拿到小比例随机桶/propensity（支撑 OPE）
- 约束定义：主播承接上限/用户频控的业务口径

Notes:
- 如果这些口径确定，我们就能把“策略迭代”变成可控工程，而不是玄学调参。
