# 软约束冷启动：收益+32%，成功率+263% — 5min 汇报

> **Experiment**: VIT-20260108-gift_allocation-09  \
> **Author**: Viska Wei  \
> **Date**: 2026-01-08  \
> **Language**: 中文

---

## 软约束冷启动同时提升收益(+32%)和成功率(+263%)

- **目的**：验证冷启动约束能否让新主播获得曝光，同时不损失收益？成功标准：收益损失<5%，成功率提升>20%
- **X 一句话**：$X := \underbrace{\text{Soft Cold-Start}}_{\text{拉格朗日软约束}}\;\xrightarrow{\text{对偶更新 }\lambda}\;\underbrace{\text{新主播曝光保障}}_{\text{避免马太效应}}$
  - Why: 新主播无历史，Greedy 永远不选他们
  - How: 软约束通过 λ 自动调节约束强度，比硬约束更灵活
- **I/O（带符号）**：输入模拟环境配置，输出分配策略，核心决策是「新主播是否获得足够曝光」

| 类型 | 符号 | 说明 | 示例 |
|------|------|------|------|
| 🫐 输入 | $\mathcal{E}$ | 模拟环境 | 10k 用户 × 500 主播，20% 新主播 |
| 🫐 输入 | $\widehat{EV}(u,s)$ | 期望收益估计 | 基于偏好相似度 |
| 🫐 输出 | $\pi$ | 分配策略 | Greedy + 软约束冷启动 |
| 📊 指标 | $R, CSR, G$ | 收益/成功率/Gini | +32% / +263% / +0.7% |
| 🍁 基线 | $\pi_0$ | Greedy 无约束 | Revenue=41,594 |

<details>
<summary><b>note</b></summary>

大家好，今天分享的是冷启动约束验证实验。

核心结论是：**软约束冷启动同时提升收益32%和成功率263%**。这个结果出乎意料——我们本来担心约束会损失收益，结果反而赚了。

我们要解决的痛点是：新主播没有历史数据，纯 Greedy 策略永远不会选他们。这导致马太效应——老主播越来越强，新主播没机会。

方法是拉格朗日软约束：通过对偶变量 λ 自动调节。违反约束时 λ 增大，下一轮更倾向于分配给新主播。

看 I/O 表：输入是 10k 用户和 500 主播的模拟环境，输出是分配策略。成功率从 16.8% 提升到 61.1%。

</details>

---

## 拉格朗日对偶更新 + 实验流程 + 关键结果

- **算法**：目标函数加入约束惩罚项，对偶更新自动调节 λ

$$\mathcal{L} = \sum_s g(V_s) + \sum_c \lambda_c \cdot \text{slack}_c$$
$$\lambda_c^{(t+1)} = \max\left(0, \lambda_c^{(t)} + \eta \cdot \text{violation}_c^{(t)}\right)$$

- **流程（6 步）**：
```
实验流程
├── 1. 准备环境
│   └── SimulatorV1: 10k 用户 × 500 主播，20% 新主播
├── 2. 构建对比组
│   └── 无约束 Greedy / 软约束冷启动 / 硬约束冷启动 / 全约束
├── 3. 核心循环
│   ├── × 100 次模拟
│   └── × 200 轮: 用户到达 → 策略分配 → 观察收益 → 更新 λ
├── 4. 评估
│   └── 计算 Revenue / Cold Start Success / Gini
├── 5. 汇总
│   └── 均值/方差
└── 6. 产出
    └── 对照表 + 决策：采用软约束
```

- **结果**：

| Metric | Baseline | Soft Cold-Start | Δ |
|---|---:|---:|---:|
| Revenue | 41,594 | **54,882** | **+32%** |
| Cold Start Success | 16.8% | **61.1%** | **+263%** |
| Gini | 0.813 | 0.820 | +0.7% |

<details>
<summary><b>note</b></summary>

先讲算法直觉：拉格朗日目标函数把约束变成惩罚项。违反冷启动约束时，λ 自动增大，分配器就更倾向于给新主播机会。

实验流程：4 种策略对比，每种跑 100 次模拟，每次 200 轮。关键差异在「软约束更新 λ」这一步。

看结果表：这是实验最重要的发现——软约束冷启动不仅没损失收益，反而提升 32%。原因是**探索发现了高潜力新主播**。Cold Start Success 从 16.8% 飙升到 61.1%。Gini 只增加 0.7%，完全可接受。

硬约束效果差很多：收益损失 29%，成功率只提升 19%。这说明软约束的灵活性很重要。

</details>

---

## 探索发现高价值新主播 — 决策：采用软约束冷启动

- **结论**：「软约束冷启动同时提升收益和成功率，因为探索发现了高潜力新主播」
  - 机制层：软约束强制探索，意外发现被埋没的优质新主播
  - 证据层：收益 +32%（54,882 vs 41,594），成功率 +263%（61.1% vs 16.8%）
- **决策**：✅ **采用 Greedy + 软约束冷启动**
  - Gate-2 通过理由：收益增加（非损失）+ 成功率大幅提升
- **权衡**：赚到 +32% 收益 + +263% 成功率，付出 Gini +0.7%（可接受）
- **配置建议**：
  - `min_allocation = 10`（新主播最低曝光）
  - `λ_init = 0.5`（对偶变量初始值）
- **下一步**：
  - 🔴 P0: 生产部署 Greedy + Soft Cold-Start
  - 🟡 P1: MVP-3.1 OPE 离线验证
  - 🟢 P2: 在真实数据上微调 λ

<details>
<summary><b>note</b></summary>

金句是：**软约束冷启动同时提升收益和成功率，因为探索发现了高潜力新主播**。

这个发现很反直觉。我们本来以为约束会损失收益，结果反而赚了。为什么？因为新主播里有被埋没的优质主播，Greedy 永远不会发现他们。软约束强制探索，就把这些"宝藏主播"挖出来了。

决策很明确：**采用软约束冷启动**。收益增加 32%，成功率提升 263%，Gini 只增加 0.7%——这是稳赚不赔的 trade-off。

配置建议：min_allocation=10，λ=0.5。这组参数在实验中表现稳定。

下一步：P0 是生产部署，P1 是离线验证，P2 是真实数据微调。

</details>
