# 🧠 Gift Allocation Hub

> **Status:** ✅ Phase 0-3 完成 | 🔄 Phase 5 规划中 | **Data:** KuaiLive | **Date:** 2026-01-09

---

## 🚨 战略转向 (2026-01-09)

> **核心判断：从"预测更准"转向"分配更对"**

**已有结论表明**：
- 估计层已相对够用（Direct Regression Top-1%=54.5%）
- 分配策略（Greedy vs Random）带来 **~3x** 收益差异
- 软冷启动约束额外 **+32%** 收益
- **资源约束 + 生态目标**才是决定上线收益与风险的核心

**不值得继续的方向**（节省时间）：
1. ~~延迟反馈建模~~ — 86.3%礼物立即发生，Chapelle校正反而让ECE变差
2. ~~两段式 p×m 作为主排序器~~ — Top-1% 35.7% vs 54.5%，差距巨大
3. ~~单纯凹收益分配~~ — Δ=-1.17%，仅靠边际递减假设不足以解决真实约束
4. ~~多任务学习~~ — 极稀疏场景(<2%)无优势，性价比不高

**下一阶段主线**：
> 把分配问题当成"带约束、带外部性、带风险"的在线资源分配/匹配问题来做

---

## 🔑 核心结论 (10条)

| # | 结论 | 证据 | 决策 |
|---|------|------|------|
| K1 | **直接回归优于两段式** | Top-1%: 54.5% vs 35.7% | 保留简单架构 |
| K2 | **延迟反馈不是问题** | 86.3%礼物立即发生 | 简单负例处理足够 |
| K3 | **凹收益分配无显著优势** | Δ=-1.17%, Gini改善-0.018 | 简化为Greedy |
| K4 | **软约束冷启动有效** | 收益+32%, 成功率+263% | 采用λ=0.5 |
| K5 | **多任务学习无优势** | Δ PR-AUC=-1.76pp | 保留单任务 |
| K6 | **Two-Stage适合精排但召回覆盖率不足** | gift子集Spearman=0.89，但Recall Coverage仅5.5% | ❌ 分工架构不可行，保留Direct-only |
| K7 | **交互历史是最强特征** | 重要性是第二名3倍 | 优先维护交互特征 |
| K8 | **SNIPS是最佳OPE方法** | RelErr<10% | 探索率≥0.3, 日志≥5000 |
| K9 | **分配策略是最大杠杆** | Greedy 3x Random | 优化分配层 > 优化估计层 |
| K10 | **高并发存在边际递减** | Revenue/User 下降24.4% | 需考虑容量约束 |
| K11 | **简单规则优于复杂框架** | Greedy+Rules +4.36% vs Shadow Price +2.74% | 保留Greedy+软约束 |

## 🏗️ 推荐架构 (Phase 5 目标)

| 层 | 当前方法 | 目标方法 | 预期收益 |
|----|---------|---------|---------|
| **召回层** | ~~Direct Top-M~~ | ❌ 不可行 (Recall Coverage仅5.5%) | - |
| **精排层** | ~~Two-Stage Stage2~~ | ❌ 不可行 (无足够召回) | - |
| **排序层** | Direct Regression | **保留 Direct-only** | Top-1%=54.5% |
| **分配层** | Greedy + 软约束 | **保留 Greedy+Rules** | Shadow Price FAIL (+2.74%<5%) |
| **风险层** | - | UCB/CVaR | 波动↓，尾部保护 |
| **评估层** | SNIPS OPE | SNIPS + Simulator | RelErr<10% |

## 📊 关键数字速查

| 类别 | 指标 | 值 | 来源 |
|------|------|-----|------|
| **数据特征** | 打赏率 (per-click) | 1.48% | MVP-0.1 |
| | 金额 P50/P90/P99 | 2/88/1,488 | MVP-0.1 |
| | 金额 Mean/Max | 82.7/56,246 | MVP-0.1 |
| | User Gini | 0.942 | MVP-0.1 |
| | Streamer Gini | 0.930 | MVP-0.1 |
| | Top 1% User贡献 | 59.9% | MVP-0.1 |
| | Matrix Density | 0.0064% | MVP-0.1 |
| | Cold Start Streamer | 92.2% | MVP-0.1 |
| **Baseline** | Top-1% Capture | 56.2% | MVP-0.2 |
| | Spearman | 0.891 | MVP-0.2 |
| | MAE(log) | 0.263 | MVP-0.2 |
| **公平对比** | Direct Top-1% (click全量) | 54.5% | MVP-1.1-fair |
| | Two-Stage Top-1% | 35.7% | MVP-1.1-fair |
| | Two-Stage NDCG@100 | 0.359 | MVP-1.1-fair |
| | Oracle_p 增益 | +54.9pp | MVP-1.4 |
| | Oracle_m 增益 | +3.1pp | MVP-1.4 |
| | Stage2 gift子集 Spearman | 0.892 | MVP-1.4 |
| **延迟建模** | 延迟中位数 | 0 秒 | MVP-1.2 |
| | 86.3%礼物立即发生 | delay=0 | MVP-1.2-audit |
| | pct_late_50 | 13.3% | MVP-1.2-audit |
| | Chapelle ECE改善 | -0.010 (变差) | MVP-1.2 |
| **多任务** | Single-Task PR-AUC | 0.182 | MVP-1.3 |
| | Multi-Task PR-AUC | 0.165 | MVP-1.3 |
| | Δ PR-AUC | -1.76pp | MVP-1.3 |
| **分配层** | Greedy/Random 收益比 | 2.94x | MVP-0.3 |
| | Concave vs Greedy Δ | -1.17% | MVP-2.1 |
| | Concave Gini改善 | -0.018 | MVP-2.1 |
| | Soft Cold-Start Revenue Δ | +32% | MVP-2.2 |
| | Soft Cold-Start Success Δ | +263% | MVP-2.2 |
| | Hard Cold-Start Revenue Δ | -29% | MVP-2.2 |
| | 推荐λ | 0.5 | MVP-2.2 |
| **评估层** | SNIPS RelErr (Softmax) | 0.57% | MVP-3.1 |
| | SNIPS RelErr (Concave) | 4.31% | MVP-3.1 |
| | SNIPS RelErr (Greedy) | 9.97% | MVP-3.1 |
| | 最优探索率 | 0.3 | MVP-3.1 |
| | 最小日志量 | 5000+ | MVP-3.1 |
| | Simulator Gini误差 | <5% | MVP-0.3 |
| **Simulator V2+** | V2+ P50误差 | 0% | MVP-4.1+ |
| | V2+ P90误差 | 13.2% | MVP-4.1+ |
| | V2+ P99误差 | 26.0% | MVP-4.1+ |
| | V2+ Mean误差 | 24.0% | MVP-4.1+ |
| | V2+ User Gini | 0.887 | MVP-4.1+ |
| **并发容量** | Revenue/User下降 | 24.4% | MVP-4.2 |
| | 拥挤率@800用户 | 67.9% | MVP-4.2 |
| | 容量约束收益差 | 2.2% | MVP-4.2 |
| | 推荐惩罚强度β | 0.5 | MVP-4.2 |
| **召回-精排** | Recall Coverage @1000 | 5.5% | MVP-5.1 |
| | RR@1000 Top-1% | 54.5% (无提升) | MVP-5.1 |
| | RR@1000 NDCG@100 | 0.335 | MVP-5.1 |
| | 真实Top-1%样本数 | 13,354 | MVP-5.1 |
| **影子价格** | Shadow Price All Δ收益 | +2.74% | MVP-5.2 |
| | Greedy+Rules Δ收益 | +4.36% | MVP-5.2 |
| | Shadow Price 容量满足率 | 82.8% | MVP-5.2 |
| | Shadow Price 冷启动率 | 91.5% | MVP-5.2 |
| | 推荐学习率η | 0.05 | MVP-5.2 |
| | Cold Start λ_final | 1.25 | MVP-5.2 |

## 🌲 核心假设树

```
🌲 核心: 如何在直播场景下最大化全局打赏收益？
│
├── Q1: 估计层 - 如何准确预测极稀疏、重尾、延迟反馈的打赏行为？
│   ├── Q1.1: 两段式(是否打赏+金额)是否优于直接回归？ → ❌ 整体不优，但精排场景有优势
│   │   ├── H1: Stage2数据量/信息量不足 → ❌ 否定：Oracle_m增益仅+3.1pp
│   │   ├── H2: p×m乘法放大误差 → ❌ 否定：Stage1-only≈Two-Stage
│   │   ├── H3: Stage2 OOD问题 → ❌ 否定：gift子集Spearman=0.89
│   │   └── **主因**: ✅ Stage1分类不足（Oracle_p增益+54.9pp），推荐召回-精排分工
│   ├── Q1.2: 延迟反馈建模能否提升预测准确性？ → ❌ 审计通过，延迟不是问题
│   │   ├── ✅ 样本数差异已解释: 77,824 = gift×click配对（1.14x膨胀正常）
│   │   ├── ✅ pct_late_50=13.3%（代码bug已修复，之前单位错误）
│   │   └── ✅ 86.3%礼物delay=0，简单负例处理已足够
│   ├── Q1.3: 多任务学习能否用密集信号扶起稀疏信号？ → ❌ 无优势 (Δ=-1.76pp)
│   └── Q1.4: 重尾金额用log(1+Y)还是分位数回归？ → ⏳待验证
│
├── Q2: 决策层 - 如何在全局最优下分配高价值用户？
│   ├── Q2.1: 凹收益分配 vs 贪心分配的收益差异？ → ❌ Δ=-1.17%，无显著优势但公平性改善(Gini -0.018)
│   ├── Q2.2: 边际收益递减系数g'(V)如何设定/学习？ → ❌ 模拟环境下影响不大
│   ├── Q2.3: 冷启动/公平约束如何嵌入分配层？ → ✅ **软约束：收益+32%，成功率+263%**
│   └── Q2.4: 羊群效应/攀比效应是否存在？如何建模？ → ⏳待验证
│
└── Q3: 评估层 - 如何可靠评估分配策略？
    ├── Q3.1: OPE(IPS/DR)在高方差场景的有效性？ → ✅ **SNIPS可用，RelErr<10%，Gate-3关闭**
    │   ├── ✅ SNIPS最佳方法：Softmax 0.57%, Concave 4.31%, Greedy 9.97%
    │   ├── ❌ DR表现不如预期：Q函数偏差大于减少的方差
    │   └── ✅ 最优探索率~0.3，日志量≥5000
    ├── Q3.2: Simulator能否复现真实场景的关键特性？ → ✅ Gini可校准(<5%误差)，金额分布误差大
    │   ├── Q3.2.1: Simulator变量控制实验 - Two-Stage何时赢？ → ⏳ 待验证
    │   └── Q3.2.2: Simulator V2 能否修复金额分布失真？ → ✅ V2+ PASS (MVP-4.1+)
    │       ├── H4.1.1: 离散金额档位能否修复 P50/P90 误差？ → ✅ P50=0%, P90=23%
    │       ├── H4.1.2: V2+ 是否优于 V2 (lognormal+pareto)？ → ✅ Mean误差27% vs 375%
    │       └── H4.1.3: Gate-4A 通过？ → ✅ PASS → 继续 MVP-4.2
    │   └── Q3.2.3: 并发容量能否建模边际递减？ → ✅ PASS (MVP-4.2)
    │       ├── H4.2.1: Revenue/User 是否随负载下降？ → ✅ 24.4% 下降
    │       ├── H4.2.2: 拥挤率是否显著？ → ✅ 67.9% @800用户
    │       └── H4.2.3: Gate-4B 通过？ → ✅ PASS → 继续 MVP-4.3
    └── Q3.3: 评估窗口H如何设定？对结论有多敏感？ → ⏳待验证

Legend: ✅ 已验证 | ❌ 已否定 | ⏳ 待验证
```

## 💡 洞见汇合 (多实验共识)

| # | 洞见 | 观察 | 决策影响 | 证据 |
|---|------|------|---------|------|
| I1 | 交互历史决定打赏 | pair_gift_mean 重要性是第二名的3倍 | 优先维护交互特征；冷启动是瓶颈 | MVP-0.2 |
| I2 | Baseline 已有较高性能 | Top-1%=56.2%远超30%基准 | 两段式需证明显著提升才值得复杂度 | MVP-0.2 |
| I3 | 模型对比需统一候选集 | MVP-1.1两段式与Baseline在不同数据集训练 | 必须在相同候选集上公平对比 | MVP-1.1 |
| I4 | 分类层和回归层学到不同信号 | Stage1用count特征，Stage2用mean特征 | 两阶段设计有合理性 | MVP-1.1 |
| I5 | 公平对比下直接回归更优 | Direct Top-1%=54.5% vs Two-Stage=35.7% | 保留简单架构，优化特征 | MVP-1.1-fair |
| I6 | p(x)×m(x)乘法组合引入排序噪声 | Two-Stage(35.7%) < 二分类上限(51.7%) | 高稀疏场景慎用两阶段 | MVP-1.1-fair |
| I7 | Two-Stage 在 NDCG@100 上更优 | NDCG@100: 35.9% vs 21.7% (+14.2pp) | 召回用Direct，精排用Two-Stage | MVP-1.1-fair |
| I8 | 延迟校正无效（审计后确认） | 86.3%礼物立即发生，pct_late_50=13.3% | 简单负例处理足够 | MVP-1.2+audit |
| I9 | Two-Stage输的主因是Stage1分类不足 | Oracle_p增益+54.9pp >> Oracle_m+3.1pp | 考虑优化Stage1或采用分工策略 | MVP-1.4 |
| I10 | Stage2在精排场景有明显优势 | gift子集Spearman: Stage2=0.89 > Direct=0.74 | 召回-精排分工 | MVP-1.4 |
| I11 | 多任务学习在极稀疏场景无优势 | Multi PR-AUC=0.165 < Single=0.182 | 极稀疏(<2%)场景慎用多任务 | MVP-1.3 |
| I12 | Simulator Gini可校准，金额分布误差大 | User Gini误差4.9% | Simulator可用于分配策略相对比较 | MVP-0.3 |
| I13 | Greedy分配策略收益远高于Random | Greedy=60,651 vs Random=20,643 (2.94x) | 分配策略对总收益影响显著 | MVP-0.3 |
| I20 | 并发容量能有效建模边际递减 | Revenue/User 从 0.49 降至 0.37 (24.4%) | 高并发损害边际收益 | MVP-4.2 |
| I21 | 低容量设置必要以触发拥挤 | 原 capacity=100 无效果，需降至 15/8/3 | 参数需根据并发量校准 | MVP-4.2 |
| I14 | 凹收益分配无显著优势但公平性改善 | Δ=-1.17%, Gini改善-0.018 | 简化为Greedy+约束 | MVP-2.1 |
| I15 | 软约束冷启动是explore-exploit平衡 | 收益+32%，成功率+263%同时提升 | 冷启动约束本质是有价值的探索 | MVP-2.2 |
| I16 | 硬约束过于激进 | 收益-29%，成功率仅+19% | 避免硬预留，使用软约束λ | MVP-2.2 |
| I17 | V1 Simulator 金额分布严重失真 | P50误差2163%，Mean误差322% | 需V2+校准 | 问题分析 |
| I18 | V2+ 离散档位成功校准 | P50=0%, P90=23%, Mean=27% | Gate-4A PASS | MVP-4.1+ |
| I19 | 离散档位优于连续分布 | V2+ Mean误差27% vs V2 375% | 避免pareto重尾 | MVP-4.1+ |
| I22 | 档位概率是金额校准关键 | tier_probs调整直接影响P50/P90 | 65%概率落在tier≤2才能匹配P50=2 | MVP-4.1+ |
| I23 | 容量参数需根据并发量校准 | capacity=100无效果，降至15/8/3才触发拥挤 | 低估并发时需降低容量 | MVP-4.2 |
| I24 | 高并发场景存在边际递减 | 800用户时Revenue/User下降24.4% | 分配策略需考虑容量约束 | MVP-4.2 |
| I25 | 召回覆盖率是分工架构前提 | Top-M=1000仅覆盖5.5%真实Top-1% | 无足够召回覆盖率，精排无法发挥作用 | MVP-5.1 |
| I26 | Direct头部识别能力不足 | 需Top-M>13K才能覆盖Top-1%样本 | 当前Direct不适合作为召回模型 | MVP-5.1 |
| I27 | 简单规则优于复杂框架 | Greedy+Rules +4.36% vs Shadow Price +2.74% | 复杂框架需更好的约束校准 | MVP-5.2 |
| I28 | 对偶变量收敛性差异大 | cold_start收敛良好，whale_spread收敛差 | 约束惩罚函数设计影响收敛 | MVP-5.2 |

## ✅ Gate 状态

| Gate | 状态 | 结论 |
|------|------|------|
| Gate-1 估计层 | ✅ 完成 | 直接回归胜出；延迟/多任务无效 |
| Gate-2 分配层 | ✅ 关闭 | Greedy + 软约束冷启动 |
| Gate-3 评估层 | ✅ 关闭 | SNIPS RelErr<10% |
| Gate-4A 金额校准 | ✅ 通过 | V2+ 离散档位: P50=0%, P90=13%, Mean=24% |
| Gate-4B 并发容量 | ✅ 通过 | 边际递减24.4%, 拥挤率68% @800用户 |
| Gate-5A 召回精排 | ❌ FAIL | Recall Coverage仅5.5%@Top-1000，保留Direct-only |
| Gate-5B 影子价格 | ❌ FAIL | 收益仅+2.74%(<5%)，容量满足率82.8%(<90%)。保留Greedy+Rules |
| Gate-5C 鲸鱼分散 | ⏳ 待验证 | 超载率<10%，Gini改善 |
| Gate-5D 风险控制 | ⏳ 待验证 | CVaR@5% 改善 |

---

## 🔴 决策空白 (Phase 5 要回答的问题)

| DG | 我们缺的答案 | 为什么重要 | 什么结果能关闭它 | 决策规则 |
|----|-------------|-----------|-----------------|---------|
| DG6 | 召回-精排分工能否同时保住Top-1%和提升NDCG？ | 决定估计层架构 | ❌ **FAIL**: Recall Coverage仅5.5%@Top-1000 | **关闭**: 保留Direct-only |
| DG7 | 影子价格能否统一处理多约束（容量/冷启动/头部cap）？ | 决定分配框架 | ❌ **FAIL**: 收益+2.74%<5%，容量82.8%<90% | **关闭**: 保留Greedy+Rules |
| DG8 | 鲸鱼分散能否降低生态集中度？ | 解决马太效应 | 超载率<10% & Gini↓ | If Y → 鲸鱼单独匹配; Else → 统一分配 |
| DG9 | 不确定性排序能否降低策略波动？ | 上线稳定性 | CVaR@5%改善 & 波动↓ | If Y → UCB/LCB; Else → 纯EV排序 |
| DG10 | 多目标权重扫描的Pareto前沿形状如何？ | 生态护栏定阈值 | 可解释的权衡曲线 | 为上线设定$\alpha$权重 |

---

## 📐 设计原则

### 6.1 已确认原则

| # | 原则 | 建议 | 适用范围 | 证据 |
|---|------|------|---------|------|
| P1 | 高稀疏场景优先直接回归 | ✅ 做 | 打赏率<5% | MVP-1.1-fair |
| P2 | 极稀疏(<2%)场景慎用多任务 | ✅ 做 | 辅助信号不够强时 | MVP-1.3 |
| P3 | 模型对比需统一候选集 | ✅ 做 | 所有对比实验 | MVP-1.1 |
| P4 | 金额回归用log(1+Y) | ✅ 做 | 重尾金额 | 文献共识 |
| P5 | 稀疏信号用PR-AUC | ✅ 做 | 正例<5% | 文献共识 |
| P6 | 金额模拟用离散档位 | ✅ 做 | Simulator | MVP-4.1+ |
| P7 | 避免Pareto重尾建模 | ✅ 做 | 金额分布 | MVP-4.1+ |
| P8 | 容量参数需按并发量校准 | ✅ 做 | Simulator | MVP-4.2 |
| P9 | 高并发场景需考虑边际递减 | ✅ 做 | 分配策略 | MVP-4.2 |
| P10 | 软约束优于硬约束 | ✅ 做 | 冷启动/头部cap | MVP-2.2 |

### 6.2 待验证原则 (Phase 5)

| # | 原则 | 初步建议 | 需要验证 |
|---|------|---------|---------|
| ~~P11~~ | ~~召回用Direct，精排用Two-Stage~~ | ❌ 否定 | Gate-5A FAIL |
| ~~P12~~ | ~~多约束用影子价格统一框架~~ | ❌ 否定 | Gate-5B FAIL |
| P13 | 鲸鱼单独做匹配层 | 推荐 | Gate-5C |
| P14 | 上线初期用LCB保守排序 | 推荐 | Gate-5D |
| P15 | 探索预算集中在冷启动/不确定性高的候选 | 推荐 | MVP-5.x |

### 6.3 已关闭方向

| 方向 | 否定证据 | 关闭原因 | 教训 |
|------|---------|---------|------|
| ~~延迟反馈建模~~ | 86.3%立即发生, ECE变差 | 不是主要矛盾 | 先审计数据再建模 |
| ~~两段式主排序~~ | Top-1% 35.7% vs 54.5% | Stage1分类不足 | 乘法放大误差 |
| ~~凹收益替代Greedy~~ | Δ=-1.17% | 无真实约束 | 边际递减需配合约束 |
| ~~多任务学习~~ | Δ PR-AUC=-1.76pp | 极稀疏无共享收益 | 需更强结构(序列/图) |
| ~~召回-精排分工~~ | Recall Coverage仅5.5%@Top-1000 | Direct召回能力不足 | 分工需先保证召回覆盖率≥80% |
| ~~影子价格统一框架~~ | 收益+2.74%<5%，容量82.8%<90% | Greedy+Rules更优(+4.36%) | 简单规则在现阶段足够有效 |

## 📋 实验完成度

| Phase | 完成 | MVP |
|-------|------|-----|
| Phase 0 | 3/3 | 0.1, 0.2, 0.3 |
| Phase 1 | 6/7 | 1.1, 1.1-fair, 1.2, 1.2-audit, 1.3, 1.4 |
| Phase 2 | 2/2 | 2.1, 2.2 |
| Phase 3 | 1/1 | 3.1 |
| Phase 4 | 2/7 | 4.1+, 4.2 |
| Phase 5 | 2/5 | 5.1 (FAIL), 5.2 (FAIL) |
| **总计** | **16/25** | Phase 5 进行中 |

## 🚀 Phase 5 规划：分配优化

> **Status:** 📋 规划中 | **目标:** 从"预测更准"转向"分配更对"

### 优先级排序（建议执行顺序）

| 优先级 | MVP | 名称 | 目标 | Gate |
|--------|-----|------|------|------|
| ~~🔴 P0~~ | ~~5.1~~ | ~~召回-精排分工~~ | ❌ Gate-5A FAIL | ~~Gate-5A~~ |
| ~~🔴 P0~~ | ~~5.2~~ | ~~影子价格/供需匹配~~ | ❌ Gate-5B FAIL | ~~Gate-5B~~ |
| 🟡 P1 | 5.3 | 鲸鱼分散 (b-matching) | 超载率<10%，Gini↓ | Gate-5C |
| 🟡 P1 | 5.4 | 风险控制 (UCB/CVaR) | CVaR@5% 改善 | Gate-5D |
| 🟢 P2 | 5.5 | 多目标生态调度 | Pareto前沿可视化 | - |

### Phase 4 完成状态

| MVP | 名称 | 状态 | 结论 |
|-----|------|------|------|
| 4.1+ | Simulator V2 - 增强版 | ✅ PASS | P50=0%, P90=13%, Mean=24% |
| 4.2 | Simulator V2 - 并发容量 | ✅ PASS | 边际递减24.4%, 拥挤率68% |

**详见**: [Phase 5 Roadmap](./gift_allocation_roadmap.md)

## 🔗 导航

| 文件 | 用途 |
|------|------|
| [Roadmap](./gift_allocation_roadmap.md) | MVP规格、进度追踪 |
| [Phase 4 立项](./gift_allocation_phase4_charter.md) | 下一阶段规划 |
| [exp/](./exp/) | 15个实验报告 |
| [results/](./results/) | 数值结果JSON |
