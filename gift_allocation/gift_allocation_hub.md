# 🧠 Gift Allocation Hub
> **ID:** EXP-20260108-gift-allocation-hub | **Status:** 🌱探索 |  
> **Date:** 2026-01-08 | **Update:** 2026-01-08  

| # | 💡 共识[抽象洞见] | 证据 | 决策 |
|---|----------------------------|----------------|------|
| K1 | ❌ ~~两段式优于直接回归~~ → **公平对比下直接回归更优** | MVP-1.1-fair: Direct Top-1%=54.5% vs Two-Stage=35.8% | 保留直接回归，优化特征工程 |
| K2 | 延迟反馈不能简单当负例，会系统性低估 | 广告CVR延迟校正文献 | 需用生存分析或延迟校正 |
| K3 | 金主堆叠单间不优于分散分配（凹收益假设） | ⏳待验证 | 需设计边际递减分配层 |
| K4 | 高稀疏场景(98% Y=0)下，直接回归学会了"谁不送礼" | MVP-1.1-fair 公平对比 | 数据稀疏性影响模型选择 |

**🦾 现阶段信念 [≤10条，写"所以呢"]**
- **信念1**：~~两段式更优~~ → **公平对比下直接回归更优** (Top-1% 54.5% vs 35.8%) → 保留简单架构，优化特征
- **信念2**：延迟反馈导致标签污染是核心风险 → 优先验证延迟建模的增益
- **信念3**：单点最优推荐≠全局最优分配 → 需引入凹收益分配层+影子价格机制
- **信念4**：密集代理信号(停留、互动)可扶起稀疏打赏信号 → 多任务学习是关键技术路线
- **信念5**：高稀疏(98% Y=0)时直接回归能学会"谁不送礼" → 大量负样本有信息价值

**👣 下一步最有价值  [≤2条，直接可进 Roadmap Gate]**
- ✅ ~~**P0 (MVP-1.1-fair)**~~：已完成，Direct Reg 胜出 (Δ=-18.7pp)，DG1 关闭
- 🔴 **P0 (MVP-1.2)**：验证延迟反馈建模增益 → If ECE改善≥0.02 then 纳入主模型 else 简单窗口截断
- 🟡 **P1**：优化 Direct Regression 特征工程，目标 Top-1% ≥ 60%

> **权威数字（一行即可）**：User Gini=0.942; **公平对比: Direct Top-1%=54.5% > Two-Stage=35.8% (Δ=-18.7pp)**; 条件=KuaiLive click全量

| 模型/方法 | 指标值 | 配置 | 备注 |
|-----------|--------|------|------|
| 数据特征 | Gini=0.94, P99/P50=744x | KuaiLive EDA | MVP-0.1 ✅ |
| Baseline (直接回归 gift-only) | Top-1%=56.2%, Spearman=0.891, MAE(log)=0.263 | LightGBM gift-only | MVP-0.2 ✅ |
| 两段式 (p×m) | PR-AUC=0.65, ECE=0.018 (Stage1) | LightGBM click | MVP-1.1 ✅ ⚠️不可对比 |
| **Direct Reg (click全量)** | **Top-1%=54.5%, Spearman=0.331** | LightGBM click全量 | ✅ MVP-1.1-fair **胜出** |
| **Two-Stage (click全量)** | Top-1%=35.8%, Spearman=0.247 | LightGBM click全量 | ✅ MVP-1.1-fair |
| Upper bound | TODO | Oracle | 理论上限 |

---

## 1) 🌲 核心假设树
```
🌲 核心: 如何在直播场景下最大化全局打赏收益？
│
├── Q1: 估计层 - 如何准确预测极稀疏、重尾、延迟反馈的打赏行为？
│   ├── Q1.1: 两段式(是否打赏+金额)是否优于直接回归？ → ❌ 公平对比下直接回归更优
│   ├── Q1.2: 延迟反馈建模(生存分析)能否提升预测准确性？ → ⏳待验证
│   ├── Q1.3: 多任务学习能否用密集信号扶起稀疏信号？ → ⏳待验证
│   └── Q1.4: 重尾金额用log(1+Y)还是分位数回归？ → ⏳待验证
│
├── Q2: 决策层 - 如何在全局最优下分配高价值用户？
│   ├── Q2.1: 凹收益分配 vs 贪心分配的收益差异？ → ⏳待验证
│   ├── Q2.2: 边际收益递减系数g'(V)如何设定/学习？ → ⏳待验证
│   ├── Q2.3: 冷启动/公平约束如何嵌入分配层？ → ⏳待验证
│   └── Q2.4: 羊群效应/攀比效应是否存在？如何建模？ → ⏳待验证
│
└── Q3: 评估层 - 如何可靠评估分配策略？
    ├── Q3.1: OPE(IPS/DR)在高方差场景的有效性？ → ⏳待验证
    ├── Q3.2: Simulator能否复现真实场景的关键特性？ → ⏳待验证
    └── Q3.3: 评估窗口H如何设定？对结论有多敏感？ → ⏳待验证

Legend: ✅ 已验证 | ❌ 已否定 | 🔆 进行中 | ⏳ 待验证 | 🗑️ 已关闭
```

---

## 2) 口径冻结（唯一权威）
| 项目 | 规格 |
|---|---|
| Dataset / Version | KuaiLive v1.0 / VTuber 1B |
| Train / Val / Test | 按天切分，最后7天test，前7天val |
| Metric | Gift: PR-AUC, ECE; Amount: MAE(log), P90误差; EV: Top-1%捕获率 |
| Seed / Repeats | seed=42, repeat=3 |
> 规则：任何口径变更必须写入 §8 变更日志。

---

## 3) 当前答案 & 战略推荐（对齐问题树）

### 3.1 战略推荐（只保留"当前推荐"）
- **推荐路线：Route B**（理由：先做扎实估计层，再叠加分配层，最后验证评估闭环）
- 需要 Roadmap 关闭的 Gate：Gate-1（估计层验证）, Gate-2（分配层验证）

| Route | 一句话定位 | 当前倾向 | 关键理由 | 需要的 Gate |
|---|---|---|---|---|
| Route A | 端到端学习 (E2E) | 🔴 | 数据稀疏，端到端可能难收敛 | - |
| **Route B** | 分层建模 (估计→分配) | 🟢 推荐 | 可解释、可调试、先验知识可嵌入 | Gate-1, Gate-2 |
| Route C | 纯Bandit探索 | 🟡 | 需大量在线流量，冷启动困难 | Gate-3 |

### 3.2 分支答案表（每行必须回答"所以呢"）
| 分支 | 当前答案（1句话） | 置信度 | 决策含义（So what） | 证据（exp/MVP） |
|---|---|---|---|---|
| Q1 估计层 | 两段式+延迟校正是主路线 | 🟡 | 优先验证两段式增益 | TODO |
| Q2 决策层 | 凹收益分配层是核心创新点 | 🟡 | 需设计可学习的g(V) | TODO |
| Q3 评估层 | OPE+Simulator双保险 | 🟡 | 需保留随机化流量桶 | TODO |

---

## 4) 洞见汇合（多实验 → 共识）
> 只收录"会改变决策"的洞见，建议 5–8 条。

| # | 洞见（标题） | 观察（What） | 解释（Why） | 决策影响（So what） | 证据 |
|---|---|---|---|---|---|
| I1 | 交互历史决定打赏 | pair_gift_mean 重要性是第二名的3倍 | 打赏行为有强用户-主播绑定 | 优先维护交互特征；冷启动是瓶颈 | MVP-0.2 |
| I2 | Baseline 已有较高性能 | Top-1%=56.2%远超30%基准 | 直接回归+交互特征已捕捉核心信号 | 两段式需证明显著提升才值得复杂度 | MVP-0.2 |
| I3 | 模型对比需统一候选集 | MVP-1.1两段式与Baseline在不同数据集训练 | Baseline只看gift-only，Two-Stage看click全量 | 必须在相同候选集上公平对比 | MVP-1.1 |
| I4 | 分类层和回归层学到不同信号 | Stage1用count特征，Stage2用mean特征 | 是否打赏和打赏多少是不同问题 | 两阶段设计有合理性 | MVP-1.1 |
| **I5** | **公平对比下直接回归更优** | Direct Top-1%=54.5% vs Two-Stage=35.8% | 98% Y=0 时直接回归学会"谁不送礼"；Stage1 仅9轮early stop | 保留简单架构，优化特征 | MVP-1.1-fair |
| I6 | Two-Stage 在高稀疏场景失效 | Stage1 只训练9轮 | 正样本太少(1.82%)，分类器快速饱和 | 高稀疏场景慎用两阶段 | MVP-1.1-fair |

---

## 5) 决策空白（Decision Gaps）
> 写"要回答什么"，不写"怎么做实验"。建议 3–6 条。

| DG | 我们缺的答案 | 为什么重要（会改哪个决策） | 什么结果能关闭它 | 决策规则 |
|---|---|---|---|---|
| DG1 | ~~两段式 vs 直接回归的增益有多大？~~ | 决定估计层架构 | ✅ **已关闭** Δ=-18.7pp | Direct Reg 胜出，保留简单架构 |
| DG2 | 延迟校正的增益有多大？ | 决定是否引入生存建模 | 校准误差对比 | If ECE改善>0.02 → 延迟建模；Else → 简单窗口 |
| DG3 | 凹收益分配 vs 贪心的总收益差异？ | 决定是否需要分配层 | Simulator收益对比 | If Δ>10% → 分配层；Else → 贪心 |
| DG4 | 边际递减vs羊群效应哪个主导？ | 决定g(V)的形式 | Simulator消融实验 | If 递减>羊群 → 凹g；Else → 先增后凹 |
| DG5 | 密集信号对打赏预测的迁移效果？ | 决定多任务权重 | 多任务vs单任务对比 | If多任务>单任务 → 联合训练 |

---

## 6) 设计原则（可复用规则）

### 6.1 已确认原则
| # | 原则 | 建议（做/不做） | 适用范围 | 证据 |
|---|---|---|---|---|
| P1 | 稀疏信号用PR-AUC而非ROC-AUC | ✅ 做 | 打赏预测评估 | 文献共识 |
| P2 | 金额回归用log(1+Y)变换 | ✅ 做 | 重尾分布 | 文献共识 |
| P3 | 时间切分避免数据泄漏 | ✅ 做 | 所有离线实验 | 文献共识 |

### 6.2 待验证原则
| # | 原则 | 初步建议 | 需要验证（MVP/Gate） |
|---|---|---|---|
| P4 | 新数据近期样本不能当负例 | 延迟校正或删失处理 | MVP-1.2 |
| P5 | 分配层用影子价格g'(V)加权 | 实时更新累计V | MVP-2.1 |
| P6 | 冷启动主播设最低探索流量 | 约束/拉格朗日形式 | MVP-2.2 |

### 6.3 关键数字速查（只留会反复用到的）
| 指标 | 值 | 条件 | 来源 |
|---|---|---|---|
| KuaiLive打赏率(per-click) | 1.48% | 72,646/4,909,515 | MVP-0.1 |
| 金额P50/P90/P99 | 2/88/1,488 | 打赏样本 | MVP-0.1 |
| 金额Mean/Max | 82.7/56,246 | 打赏样本 | MVP-0.1 |
| User Gini | 0.942 | 用户维度总打赏 | MVP-0.1 |
| Streamer Gini | 0.930 | 主播维度总收益 | MVP-0.1 |
| Top 1% User贡献 | 59.9% | 收益占比 | MVP-0.1 |
| Top 1% Streamer贡献 | 53.4% | 收益占比 | MVP-0.1 |
| Matrix Density | 0.0064% | User-Streamer | MVP-0.1 |
| Cold Start Streamer | 92.2% | 无打赏主播占比 | MVP-0.1 |
| **Baseline MAE(log)** | **0.263** | Test set | MVP-0.2 |
| **Baseline Top-1% Capture** | **56.2%** | Test set | MVP-0.2 |
| **Baseline Spearman** | **0.891** | Test set | MVP-0.2 |
| Baseline NDCG@100 | 0.716 | Test set | MVP-0.2 |

### 6.4 已关闭方向（避免重复踩坑）
| 方向 | 否定证据 | 关闭原因 | 教训 |
|---|---|---|---|
| - | - | - | - |

---

## 7) 指针（详细信息在哪）
| 类型 | 路径 | 说明 |
|---|---|---|
| 🗺️ Roadmap | `./gift_allocation_roadmap.md` | Decision Gates + MVP 执行 |
| 📋 Kanban | `status/kanban.md` | 进度中控 |
| 📗 Experiments | `./exp/exp_*.md` | 单实验报告 |
| 📊 Consolidated | `./gift_allocation_consolidated_*.md` | 跨实验综合 |

---

## 8) 变更日志（只记录"知识改变"）
| 日期 | 变更 | 影响 |
|---|---|---|
| 2026-01-08 | 创建 | 初始化问题树、战略路线 |
| 2026-01-08 | MVP-0.1 完成 | 确认关键数字: Gini=0.94, Top1%=60%, Density=0.0064%; 验证两段式必要性 |
| 2026-01-08 | MVP-0.2 完成 | Baseline LightGBM: Top-1%=56.2%, Spearman=0.891; 交互特征主导预测 |
| 2026-01-08 | MVP-1.1 完成 | 揭示Two-Stage与Baseline在不同数据集，不可直接对比；Stage1分类有效(PR-AUC=0.65) |
| 2026-01-08 | MVP-1.1-fair 立项 | P0：在click全量上公平对比，关闭DG1 |
| 2026-01-08 | MVP-1.2 立项 | P1：延迟反馈建模，关闭DG2 |
| 2026-01-08 | **MVP-1.1-fair 完成** | ❌ Direct Reg 胜出 (Top-1%=54.5% vs 35.8%)；DG1 关闭 |

---

<details>
<summary><b>附录：术语表 / 背景</b></summary>

### 术语表
| 术语 | 定义 |
|---|---|
| 两段式建模 | 先预测是否打赏p(x)，再预测条件金额m(x)，期望收益v(x)=p(x)·m(x) |
| 凹收益分配 | 收益函数g(V)是凹函数，边际收益递减，避免流量堆叠 |
| 影子价格 | g'(V)，表示直播间当前的边际价值折扣 |
| OPE | Off-Policy Evaluation，离线策略评估 |
| IPS | Inverse Propensity Scoring，逆倾向加权 |
| DR | Doubly Robust，双重稳健估计 |
| 延迟反馈 | Delayed Feedback，转化事件在曝光后延迟发生 |
| 删失 | Censoring，用户离开导致未观测到潜在打赏 |

### 背景
- 直播打赏场景：用户进入直播间后可能打赏，但绝大多数不打赏（极稀疏），少数大额贡献大部分营收（重尾）
- 核心挑战：估计层的稀疏+重尾+延迟；决策层的全局最优分配；评估层的反事实推断
- 数据源：KuaiLive（快手直播，含gift price）、VTuber 1B（YouTube SuperChat）

### 核心公式

**两段式期望收益**：
$$v(x) = \mathbb{E}[Y|x] = p(x) \cdot m(x)$$
其中 $p(x) = \Pr(Y>0|x)$，$m(x) = \mathbb{E}[Y|Y>0,x]$

**凹收益分配目标**：
$$\max \sum_s g_s(V_s)$$
其中 $g_s(\cdot)$ 是凹函数，$V_s = \sum_{u \to s} v_{u,s}$

**边际增益决策**：
$$\Delta(u \to s) \approx g'_s(V_s) \cdot v_{u,s}$$

**常用凹函数**：
- 对数型：$g(V) = \log(1+V)$
- 饱和型：$g(V) = B(1 - e^{-V/B})$

</details>
