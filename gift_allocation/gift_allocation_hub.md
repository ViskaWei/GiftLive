# 🧠 Gift Allocation Hub
> **ID:** EXP-20260108-gift-allocation-hub | **Status:** ✅阶段完成 |  
> **Date:** 2026-01-08 | **Update:** 2026-01-08 ✅  
> **Gates:** Gate-1 🔄 | Gate-2 ✅ | Gate-3 ✅  

| # | 结论 | 证据 | 决策 |
|---|------|------|------|
| K1 | **直接回归优于两段式** | Direct Top-1%=54.5% vs Two-Stage=35.7% | 保留简单架构 |
| K2 | **延迟反馈不是问题** | 86.3%礼物立即发生，Chapelle ECE变差-0.010 | 简单负例处理足够 |
| K3 | **凹收益分配无显著优势** | Δ=-1.17%, 但Gini改善-0.018 | 简化为Greedy |
| K4 | **软约束冷启动有效** | 收益+32%, 成功率+263% | 采用λ=0.5 |
| K5 | **多任务学习无优势** | Multi PR-AUC=0.165 < Single=0.182 | 保留单任务 |
| K6 | **Two-Stage适合精排** | gift子集Spearman=0.89 > Direct=0.74 | 可用于召回-精排分工 |
| K7 | **交互历史是最强特征** | pair_gift_mean重要性是第二名3倍 | 优先维护交互特征 |
| K8 | **SNIPS是最佳OPE方法** | RelErr: Softmax 0.57%, Greedy 9.97% | 探索率≥0.3, 日志≥5000 |

**🦾 已验证结论**
- **预测层**: 直接回归优于两段式；Two-Stage适合精排场景；多任务无优势；延迟校正无效
- **分配层**: Greedy + 软约束冷启动最优；凹收益分配收益提升不显著但公平性改善
- **评估层**: SNIPS OPE可用于策略离线对比 (RelErr<10%)

**👣 当前状态**
- ✅ **Phase 0-3 完成**: 12/13 MVP 已完成
- ✅ **Gate-2 关闭**: 分配层验证完成
- ✅ **Gate-3 关闭**: 评估层验证完成
- 🟡 **可选**: MVP-1.5 召回-精排分工策略

> **权威数字（一行即可）**：User Gini=0.942; **公平对比: Direct Top-1%=54.5% > Two-Stage=35.8% (Δ=-18.7pp)**; 条件=KuaiLive click全量

| 模型/方法 | 指标值 | 配置 | 备注 |
|-----------|--------|------|------|
| 数据特征 | Gini=0.94, P99/P50=744x | KuaiLive EDA | MVP-0.1 ✅ |
| Baseline (直接回归 gift-only) | Top-1%=56.2%, Spearman=0.891, MAE(log)=0.263 | LightGBM gift-only | MVP-0.2 ✅ |
| 两段式 (p×m) | PR-AUC=0.65, ECE=0.018 (Stage1) | LightGBM click | MVP-1.1 ✅ ⚠️不可对比 |
| **Direct Reg (click全量)** | **Top-1%=54.5%, Spearman=0.331, MAE(log)=0.044** | LightGBM click全量 | ✅ MVP-1.1-fair **胜出** |
| **Two-Stage V2 (click全量)** | Top-1%=35.7%, NDCG@100=0.359, ROC-AUC=0.991 | LightGBM click全量 | ✅ MVP-1.1-fair V2 验证 |
| 二分类上限 (Y>0) | Top-1%=51.7% | 理论参考 | gift占1.93%，Top-1%最多命中52% |
| Upper bound | Top-1%=100% | Oracle | 完美预测 |

---

## 1) 🌲 核心假设树
```
🌲 核心: 如何在直播场景下最大化全局打赏收益？
│
├── Q1: 估计层 - 如何准确预测极稀疏、重尾、延迟反馈的打赏行为？
│   ├── Q1.1: 两段式(是否打赏+金额)是否优于直接回归？ → ❌ 整体不优，但精排场景有优势
│   │   ├── H1: Stage2数据量/信息量不足 → ❌ 否定：Oracle_m增益仅+3.1pp
│   │   ├── H2: p×m乘法放大误差 → ❌ 否定：Stage1-only≈Two-Stage
│   │   ├── H3: Stage2 OOD问题 → ❌ 否定：gift子集Spearman=0.89
│   │   └── **主因**: ✅ Stage1分类不足（Oracle_p增益+54.9pp），推荐召回-精排分工
│   ├── Q1.2: 延迟反馈建模能否提升预测准确性？ → ❌ 审计通过，延迟不是问题
│   │   ├── ✅ 样本数差异已解释: 77,824 = gift×click配对（1.14x膨胀正常）
│   │   ├── ✅ pct_late_50=13.3%（代码bug已修复，之前单位错误）
│   │   └── ✅ 86.3%礼物delay=0，简单负例处理已足够
│   ├── Q1.3: 多任务学习能否用密集信号扶起稀疏信号？ → ⏳待验证
│   └── Q1.4: 重尾金额用log(1+Y)还是分位数回归？ → ⏳待验证
│
├── Q2: 决策层 - 如何在全局最优下分配高价值用户？
│   ├── Q2.1: 凹收益分配 vs 贪心分配的收益差异？ → ❌ Δ=-1.17%，无显著优势但公平性改善(Gini -0.018)
│   ├── Q2.2: 边际收益递减系数g'(V)如何设定/学习？ → ❌ 模拟环境下影响不大
│   ├── Q2.3: 冷启动/公平约束如何嵌入分配层？ → ✅ **软约束：收益+32%，成功率+263%**
│   └── Q2.4: 羊群效应/攀比效应是否存在？如何建模？ → ⏳待验证
│
└── Q3: 评估层 - 如何可靠评估分配策略？
    ├── Q3.1: OPE(IPS/DR)在高方差场景的有效性？ → ✅ **SNIPS可用，RelErr<10%，Gate-3关闭**
    │   ├── ✅ SNIPS最佳方法：Softmax 0.57%, Concave 4.31%, Greedy 9.97%
    │   ├── ❌ DR表现不如预期：Q函数偏差大于减少的方差
    │   └── ✅ 最优探索率~0.3，日志量≥5000
    ├── Q3.2: Simulator能否复现真实场景的关键特性？ → ✅ Gini可校准(<5%误差)，金额分布误差大
    │   └── Q3.2.1: Simulator变量控制实验 - Two-Stage何时赢？ → 🔆 待P2验证
    └── Q3.3: 评估窗口H如何设定？对结论有多敏感？ → ⏳待验证

Legend: ✅ 已验证 | ❌ 已否定 | 🔆 进行中 | ⏳ 待验证 | 🗑️ 已关闭 | 🚩 红旗/待审计
```

---

## 2) 口径冻结（唯一权威）
| 项目 | 规格 |
|---|---|
| Dataset / Version | KuaiLive v1.0 / VTuber 1B |
| Train / Val / Test | 按天切分，最后7天test，前7天val |
| Metric | Gift: PR-AUC, ECE; Amount: MAE(log), P90误差; EV: Top-1%捕获率 |
| Seed / Repeats | seed=42, repeat=3 |
> 规则：任何口径变更必须写入 §8 变更日志。

---

## 3) 当前答案 & 战略推荐（对齐问题树）

### 3.1 战略推荐（只保留"当前推荐"）
- **推荐路线：Route B**（理由：先做扎实估计层，再叠加分配层，最后验证评估闭环）
- 需要 Roadmap 关闭的 Gate：Gate-1（估计层验证）, Gate-2（分配层验证）

| Route | 一句话定位 | 当前倾向 | 关键理由 | 需要的 Gate |
|---|---|---|---|---|
| Route A | 端到端学习 (E2E) | 🔴 | 数据稀疏，端到端可能难收敛 | - |
| **Route B** | 分层建模 (估计→分配) | 🟢 推荐 | 可解释、可调试、先验知识可嵌入 | Gate-1, Gate-2 |
| Route C | 纯Bandit探索 | 🟡 | 需大量在线流量，冷启动困难 | Gate-3 |

### 3.2 分支答案表（每行必须回答"所以呢"）
| 分支 | 当前答案（1句话） | 置信度 | 决策含义（So what） | 证据（exp/MVP） |
|---|---|---|---|---|
| Q1 估计层 | 两段式+延迟校正是主路线 | 🟡 | 优先验证两段式增益 | TODO |
| Q2 决策层 | 凹收益分配层是核心创新点 | 🟡 | 需设计可学习的g(V) | TODO |
| Q3 评估层 | OPE+Simulator双保险 | 🟡 | 需保留随机化流量桶 | TODO |

---

## 4) 洞见汇合（多实验 → 共识）
> 只收录"会改变决策"的洞见，建议 5–8 条。

| # | 洞见（标题） | 观察（What） | 解释（Why） | 决策影响（So what） | 证据 |
|---|---|---|---|---|---|
| I1 | 交互历史决定打赏 | pair_gift_mean 重要性是第二名的3倍 | 打赏行为有强用户-主播绑定 | 优先维护交互特征；冷启动是瓶颈 | MVP-0.2 |
| I2 | Baseline 已有较高性能 | Top-1%=56.2%远超30%基准 | 直接回归+交互特征已捕捉核心信号 | 两段式需证明显著提升才值得复杂度 | MVP-0.2 |
| I3 | 模型对比需统一候选集 | MVP-1.1两段式与Baseline在不同数据集训练 | Baseline只看gift-only，Two-Stage看click全量 | 必须在相同候选集上公平对比 | MVP-1.1 |
| I4 | 分类层和回归层学到不同信号 | Stage1用count特征，Stage2用mean特征 | 是否打赏和打赏多少是不同问题 | 两阶段设计有合理性 | MVP-1.1 |
| **I5** | **公平对比下直接回归更优（V2验证）** | Direct Top-1%=54.5% vs Two-Stage=35.7% | V2修复Stage1(68轮,ROC=0.991)后仍落后18.8pp；问题在Stage2信息量不足(34k vs 1.87M) | 保留简单架构，优化特征 | MVP-1.1-fair V1+V2 |
| I6 | p(x)×m(x)乘法组合引入排序噪声 | Two-Stage(35.7%) < 二分类上限(51.7%) | p偏高→v等比例放大；Stage2外推不准 | 高稀疏场景慎用两阶段 | MVP-1.1-fair V2 |
| I7 | Two-Stage 在 NDCG@100 上更优 | NDCG@100: 35.9% vs 21.7% (+14.2pp) | 在金主内部，m(x)能更好区分金额大小 | 召回用Direct，精排用Two-Stage | MVP-1.1-fair V2 |
| **I8** | **延迟校正无效（审计后确认）** | 86.3%礼物立即发生，pct_late_50=13.3%（代码bug已修复） | 直播打赏是即时冲动行为，不像广告CVR | **DG2关闭有效，简单负例处理足够** | MVP-1.2+audit ✅ |
| I9 | 打赏延迟分布符合Weibull | shape=0.88, scale=57.6min, median=34.5min | 大部分打赏在1小时内完成 | 延迟不是核心瓶颈 | MVP-1.2 |
| **I10** | **离线数据无截止截断时Chapelle不适用** | Chapelle假设"尚未观测到的转化"，离线数据已完整回放 | 除非人为截断，否则不存在"未成熟标签" | 需伪在线截断实验验证真实价值 | 方法论 |
| **I11** | **Simulator Gini可校准，金额分布误差大** | User Gini误差4.9%, 金额median误差285% | 用户财富不平等可模拟，但金额需要混合分布 | Simulator可用于分配策略相对比较，不适合绝对值校准 | MVP-0.3 ✅ |
| **I12** | **Greedy分配策略收益远高于Random** | Greedy=60,651 vs Random=20,643 (2.94x) | 高匹配用户-主播对贡献更多收益 | 分配策略对总收益影响显著，值得优化 | MVP-0.3 ✅ |
| **I13** | **凹收益分配无显著优势但公平性改善** | Concave Exp Δ=-1.17%, Δ Gini=-0.018 | 凹收益边际收益小(-1.17%)但分配更公平(Gini降0.018) | 简化为Greedy+约束；若需公平性可用concave_exp | MVP-2.1 ✅ |
| **I14** | **软约束冷启动是explore-exploit平衡** | 收益+32%，成功率+263%同时提升 | λ引导探索新主播，发现高潜力主播 | 冷启动约束本质是有价值的探索，非"损失"换"公平" | MVP-2.2 ✅ |
| **I15** | **硬约束过于激进** | 收益-29%，成功率仅+19% | 预留分配牺牲了高价值匹配 | 避免硬预留，使用软约束λ | MVP-2.2 ✅ |
| **I16** | **Two-Stage输的主因是Stage1分类不足** | Oracle_p增益+54.9pp >> Oracle_m+3.1pp | Stage1 p(x)排序弱于Direct | 考虑优化Stage1或采用分工策略 | MVP-1.4 ✅ |
| **I17** | **Stage2在精排场景有明显优势** | gift子集Spearman: Stage2=0.89 > Direct=0.74 | m(x)能更好区分金额大小 | **召回-精排分工**：Direct召回+Stage2精排 | MVP-1.4 ✅ |
| **I16** | **多任务学习在极稀疏场景无优势** | Multi PR-AUC=0.165 < Single=0.182 (Δ=-1.76pp) | 密集任务(watch)梯度压制稀疏任务(gift)；任务异质性大(回归vs极稀疏分类) | 极稀疏(<2%正例)场景慎用多任务 | MVP-1.3 ✅ |

### 4.1 ✅ 红旗已解决（MVP-1.2-audit）

| # | 原红旗 | 审计结果 | 状态 |
|---|--------|----------|------|
| ~~🚩1~~ | 样本数不一致 (72,646 vs 77,824) | 正常：gift×click配对导致1.14x膨胀，92.6%是一对一 | ✅ 已解释 |
| ~~🚩2~~ | pct_late_50=68.8% 矛盾 | **代码bug**：watch_time单位ms未转秒，修复后=13.3% | ✅ 已修复 |
| ~~🚩3~~ | 延迟中位数=0 vs Weibull | 正常：86.3%立即发生(0点质量)，正延迟部分用Weibull | ✅ 已解释 |

---

## 5) 决策空白（Decision Gaps）
> 写"要回答什么"，不写"怎么做实验"。建议 3–6 条。

| DG | 我们缺的答案 | 为什么重要（会改哪个决策） | 什么结果能关闭它 | 决策规则 |
|---|---|---|---|---|
| DG1 | ~~两段式 vs 直接回归的增益有多大？~~ | 决定估计层架构 | ✅ **已关闭** Δ=-18.7pp | Direct Reg 胜出，保留简单架构 |
| **DG1.1** | ~~Two-Stage输的主因是什么？~~ | 决定是否用召回-精排分工策略 | ✅ **已关闭** | **主因=Stage1分类不足**(Oracle_p+54.9pp)；推荐召回-精排分工 |
| DG2 | ~~延迟校正的增益有多大？~~ | 决定是否引入生存建模 | ✅ **已关闭** | ECE改善=-0.010(变差)，86.3%礼物立即发生 |
| **DG2.1** | ~~延迟数据口径是否正确？~~ | 决定DG2结论是否成立 | ✅ **已通过** | 审计发现代码bug(单位错误)，修复后pct_late_50=13.3% |
| ~~DG2.2~~ | ~~伪在线截断下Chapelle是否有效？~~ | 决定生产环境是否需要延迟校正 | ❌ **取消** | 延迟问题不存在(86.3%立即发生)，无需验证 |
| DG3 | ~~凹收益分配 vs 贪心的总收益差异？~~ | 决定是否需要分配层 | ✅ **已关闭** Δ=-1.17%, Gini -0.018 | **简化为贪心+约束** |
| DG4 | 边际递减vs羊群效应哪个主导？ | 决定g(V)的形式 | ✅ 边际递减影响<1% | 模拟环境下影响不显著 |
| **DG7** | **软约束参数λ最优值？** | 决定探索强度 | 真实数据调优 | 当前λ=0.5有效 |
| ~~DG5~~ | ~~密集信号对打赏预测的迁移效果？~~ | ~~决定多任务权重~~ | ✅ **已关闭** Δ PR-AUC=-1.76pp | ❌ 多任务无优势，保留单任务 |
| **DG6** | **Two-Stage何时应优于Direct？(稀疏度/正样本量/噪声)** | 指导未来场景选型 | Simulator变量控制实验 | 产出相图：明确场景边界 |

---

## 6) 设计原则（可复用规则）

### 6.1 已确认原则
| # | 原则 | 建议（做/不做） | 适用范围 | 证据 |
|---|---|---|---|---|
| P1 | 稀疏信号用PR-AUC而非ROC-AUC | ✅ 做 | 打赏预测评估 | 文献共识 |
| P2 | 金额回归用log(1+Y)变换 | ✅ 做 | 重尾分布 | 文献共识 |
| P3 | 时间切分避免数据泄漏 | ✅ 做 | 所有离线实验 | 文献共识 |
| P7 | 高稀疏(>95% Y=0)场景优先直接回归 | ✅ 做 | 打赏/转化预测 | MVP-1.1-fair |
| P8 | 模型对比需统一数据集/候选集 | ✅ 做 | 所有对比实验 | MVP-1.1 教训 |
| P9 | 极稀疏(<2%正例)场景慎用多任务 | ✅ 做 | 打赏/转化预测 | MVP-1.3 ✅ |
| P10 | 多任务异质性大时需 GradNorm | 🟡 可选 | 回归+稀疏分类混合 | MVP-1.3 |

### 6.2 待验证原则
| # | 原则 | 初步建议 | 需要验证（MVP/Gate） |
|---|---|---|---|
| ~~P4~~ | ~~新数据近期样本不能当负例~~ | ❌ 已否定：简单截断足够 | MVP-1.2 ✅ |
| P5 | 分配层用影子价格g'(V)加权 | 实时更新累计V | MVP-2.1 |
| P6 | 冷启动主播设最低探索流量 | 约束/拉格朗日形式 | MVP-2.2 |

### 6.3 关键数字速查（只留会反复用到的）
| 指标 | 值 | 条件 | 来源 |
|---|---|---|---|
| KuaiLive打赏率(per-click) | 1.48% | 72,646/4,909,515 | MVP-0.1 |
| 金额P50/P90/P99 | 2/88/1,488 | 打赏样本 | MVP-0.1 |
| 金额Mean/Max | 82.7/56,246 | 打赏样本 | MVP-0.1 |
| User Gini | 0.942 | 用户维度总打赏 | MVP-0.1 |
| Streamer Gini | 0.930 | 主播维度总收益 | MVP-0.1 |
| Top 1% User贡献 | 59.9% | 收益占比 | MVP-0.1 |
| Top 1% Streamer贡献 | 53.4% | 收益占比 | MVP-0.1 |
| Matrix Density | 0.0064% | User-Streamer | MVP-0.1 |
| Cold Start Streamer | 92.2% | 无打赏主播占比 | MVP-0.1 |
| **Baseline MAE(log)** | **0.263** | gift-only test | MVP-0.2 |
| **Baseline Top-1% Capture** | **56.2%** | gift-only test | MVP-0.2 |
| **Baseline Spearman** | **0.891** | gift-only test | MVP-0.2 |
| Baseline NDCG@100 | 0.716 | gift-only test | MVP-0.2 |
| **Direct Reg Top-1% (click全量)** | **54.5%** | click全量 1.33M test | MVP-1.1-fair ✅ |
| Direct Reg Spearman | 0.331 | click全量 | MVP-1.1-fair |
| Direct Reg MAE(log) | 0.044 | click全量 | MVP-1.1-fair |
| Two-Stage Top-1% | 35.7% | click全量 | MVP-1.1-fair V2 |
| Two-Stage NDCG@100 | 0.359 | click全量，精细排序更好 | MVP-1.1-fair V2 |
| **延迟中位数** | **0 秒** | 86.3%礼物立即发生 | MVP-1.2 ✅ |
| **pct_late_50** | **13.3%** | 只有13%礼物在观看后半段 | MVP-1.2-audit ✅ |
| 延迟分布 | Weibull(0.94, 3253s) | P90=17min, P99=144min | MVP-1.2 |
| Baseline ECE | **0.0179** | click全量二分类 | MVP-1.2 ✅ |
| Baseline PR-AUC (click) | 0.651 | click全量二分类 | MVP-1.2 |
| Chapelle ECE | 0.0276 | 软标签加权，**变差** | MVP-1.2 ❌ |
| Chapelle ECE改善 | **-0.010** | 变差！<< 0.02阈值 | MVP-1.2 ❌ DG2关闭 |
| **Sim User Gini 误差** | **4.9%** | 模拟 vs 真实 | MVP-0.3 ✅ |
| **Sim Streamer Gini 误差** | **5.0%** | 模拟 vs 真实 | MVP-0.3 ✅ |
| **Greedy/Random 收益比** | **2.94x** | 50次模拟平均 | MVP-0.3 ✅ |
| **边际递减影响** | **-0.4%** | gamma 0→0.2 | MVP-0.3 |
| **Concave vs Greedy Δ** | **-1.17%** | 凹收益无显著优势(concave_exp), Gini -0.018改善 | MVP-2.1 ✅ |
| **Single-Task PR-AUC** | **0.182** | has_gift + gift_amount, MLP | MVP-1.3 ✅ |
| **Multi-Task PR-AUC** | 0.165 | 4 tasks, MLP | MVP-1.3 ✅ |
| **Multi vs Single Δ PR-AUC** | **-1.76pp** | < 3pp 阈值，多任务无优势 | MVP-1.3 ✅ DG5关闭 |
| **Soft Cold-Start Revenue Δ** | **+32%** | 探索发现高潜力新主播 | MVP-2.2 ✅ |
| **Soft Cold-Start Success Δ** | **+263%** | 61.1% vs 16.8% | MVP-2.2 ✅ |
| **Hard Cold-Start Revenue Δ** | **-29%** | 预留分配损失收益 | MVP-2.2 |
| **推荐分配策略** | **Greedy+软约束** | λ=0.5, min_alloc=10 | Gate-2 ✅ |
| **Oracle_p 增益** | **+54.9pp** | 完美分类下Top-1%提升 | MVP-1.4 ✅ |
| **Oracle_m 增益** | +3.1pp | 完美回归下Top-1%提升 | MVP-1.4 ✅ |
| **Stage2 gift子集 Spearman** | **0.892** | 远超Direct的0.737 | MVP-1.4 ✅ |
| **推荐预测策略** | **召回-精排分工** | Direct召回+Stage2精排 | DG1.1 ✅ |
| **SNIPS RelErr (Softmax)** | **0.57%** | OPE最佳表现 | MVP-3.1 ✅ |
| **SNIPS RelErr (Concave)** | **4.31%** | OPE良好表现 | MVP-3.1 ✅ |
| **SNIPS RelErr (Greedy)** | 9.97% | 确定性策略较难评估 | MVP-3.1 ✅ |
| **推荐OPE方法** | **SNIPS** | 自归一化IPS | MVP-3.1 ✅ |
| **最优探索率** | **0.3** | epsilon-greedy | MVP-3.1 ✅ |
| **最小日志量** | **5000+** | OPE可靠性要求 | MVP-3.1 ✅ |

### 6.4 已关闭/待验证方向
| 方向 | 状态 | 证据 | 关闭原因/待验证内容 | 教训 |
|---|---|---|---|---|
| 两段式建模 (p×m) 全面替代Direct | ❌ 关闭 | MVP-1.1-fair: Δ Top-1%=-18.8pp | Stage2样本不足(34k vs 1.87M) | 高稀疏场景慎用两阶段 |
| 两段式建模用于精排 | 🔆 待验证 | NDCG@100: +14.2pp | 需验证召回-精排分工策略 | 两阶段可能在局部场景有价值 |
| 延迟反馈校正 (Chapelle) | ❌ 关闭 | MVP-1.2+audit: ECE=-0.010, 86.3%立即发生 | 延迟问题不存在 | 打赏是即时冲动行为，不像广告CVR |
| 伪在线截断验证 | ❌ 取消 | 延迟问题不存在 | 无需验证 | 86.3%礼物立即发生，简单负例处理足够 |
| 凹收益分配 | ❌ 关闭 | MVP-2.1: Δ=-1.17%, Gini -0.018 | 收益提升不显著(<10%)，但公平性有改善 | 简化为Greedy+约束 |
| 硬约束冷启动 | ❌ 关闭 | MVP-2.2: Revenue -29% | 预留分配过于激进 | 使用软约束 |
| **软约束冷启动** | ✅ **采用** | MVP-2.2: +32%收益, +263%成功率 | 探索价值 > 约束成本 | **推荐策略** |
| **OPE (SNIPS)** | ✅ **可用** | MVP-3.1: RelErr<10%, Gate-3关闭 | SNIPS + ε=0.3 + N≥5000 | **可用于策略离线评估** |

---

## 7) 指针（详细信息在哪）
| 类型 | 路径 | 说明 |
|---|---|---|
| 🗺️ Roadmap | `./gift_allocation_roadmap.md` | Decision Gates + MVP 执行 |
| 📋 Kanban | `status/kanban.md` | 进度中控 |
| 📗 Experiments | `./exp/exp_*.md` | 单实验报告 |
| 📊 Consolidated | `./gift_allocation_consolidated_*.md` | 跨实验综合 |

---

## 8) 变更日志（只记录"知识改变"）
| 日期 | 变更 | 影响 |
|---|---|---|
| 2026-01-08 | 创建 | 初始化问题树、战略路线 |
| 2026-01-08 | MVP-0.1 完成 | 确认关键数字: Gini=0.94, Top1%=60%, Density=0.0064%; 验证两段式必要性 |
| 2026-01-08 | MVP-0.2 完成 | Baseline LightGBM: Top-1%=56.2%, Spearman=0.891; 交互特征主导预测 |
| 2026-01-08 | MVP-1.1 完成 | 揭示Two-Stage与Baseline在不同数据集，不可直接对比；Stage1分类有效(PR-AUC=0.65) |
| 2026-01-08 | MVP-1.1-fair 立项 | P0：在click全量上公平对比，关闭DG1 |
| 2026-01-08 | MVP-1.2 立项 | P1：延迟反馈建模，关闭DG2 |
| 2026-01-08 | **MVP-1.1-fair 完成** | ❌ Direct Reg 胜出 (Top-1%=54.5% vs 35.8%)；DG1 关闭 |
| 2026-01-08 | MVP-1.2 完成 | 初步结论：延迟中位数=0s，ECE改善=-0.010(变差) |
| 2026-01-08 | **问题拆解 + 新假设H1-H4** | 🚩 发现数据红旗；DG2暂缓关闭；新增DG1.1/DG2.1/DG2.2/DG6 |
| 2026-01-08 | 新增MVP-1.4/1.5/1.2-audit/1.2-pseudo | P0诊断拆解 + 数据审计；P1召回-精排分工 + 伪在线验证 |
| 2026-01-08 | **MVP-0.3 完成** | ✅ Simulator V1：Gini误差<5%可校准；Greedy策略收益3x；分配策略评估就绪 |
| 2026-01-08 | **MVP-2.1 完成** | ❌ DG3关闭：concave_exp Δ=-1.17%无显著优势，但公平性改善(Gini -0.018)；Gate-2决策：简化为Greedy+约束 |
| 2026-01-08 | **MVP-2.2 完成** | ✅ 软约束冷启动：收益+32%，成功率+263%；决策：**采用Greedy+软约束** |
| 2026-01-08 | **MVP-1.3 完成** | ❌ 多任务无优势，Δ PR-AUC=-1.76pp；DG5 关闭；新增 I16+P9/P10 |
| 2026-01-08 | **Gate-2 关闭** | 分配层验证完成；推荐策略：Greedy + Soft Cold-Start (λ=0.5) |
| 2026-01-08 | **MVP-1.2-audit 完成** | ✅ 发现并修复代码bug(单位错误)；pct_late_50=13.3%；DG2.1通过 |
| 2026-01-08 | **DG2 关闭确认** | 审计后确认：86.3%礼物立即发生，延迟校正无价值，简单负例处理足够 |
| 2026-01-08 | **MVP-3.1 完成** | OPE验证：SNIPS最佳(RelErr<10%)，Gate-3关闭 |
| 2026-01-08 | **阶段总结** | 12/13 MVP完成；推荐策略：Direct Reg + Greedy + Soft Cold-Start + SNIPS |

---

<details>
<summary><b>附录：术语表 / 背景</b></summary>

### 术语表
| 术语 | 定义 |
|---|---|
| 两段式建模 | 先预测是否打赏p(x)，再预测条件金额m(x)，期望收益v(x)=p(x)·m(x) |
| 凹收益分配 | 收益函数g(V)是凹函数，边际收益递减，避免流量堆叠 |
| 影子价格 | g'(V)，表示直播间当前的边际价值折扣 |
| OPE | Off-Policy Evaluation，离线策略评估 |
| IPS | Inverse Propensity Scoring，逆倾向加权 |
| DR | Doubly Robust，双重稳健估计 |
| 延迟反馈 | Delayed Feedback，转化事件在曝光后延迟发生 |
| 删失 | Censoring，用户离开导致未观测到潜在打赏 |

### 背景
- 直播打赏场景：用户进入直播间后可能打赏，但绝大多数不打赏（极稀疏），少数大额贡献大部分营收（重尾）
- 核心挑战：估计层的稀疏+重尾+延迟；决策层的全局最优分配；评估层的反事实推断
- 数据源：KuaiLive（快手直播，含gift price）、VTuber 1B（YouTube SuperChat）

### 核心公式

**两段式期望收益**：
$$v(x) = \mathbb{E}[Y|x] = p(x) \cdot m(x)$$
其中 $p(x) = \Pr(Y>0|x)$，$m(x) = \mathbb{E}[Y|Y>0,x]$

**凹收益分配目标**：
$$\max \sum_s g_s(V_s)$$
其中 $g_s(\cdot)$ 是凹函数，$V_s = \sum_{u \to s} v_{u,s}$

**边际增益决策**：
$$\Delta(u \to s) \approx g'_s(V_s) \cdot v_{u,s}$$

**常用凹函数**：
- 对数型：$g(V) = \log(1+V)$
- 饱和型：$g(V) = B(1 - e^{-V/B})$

</details>
