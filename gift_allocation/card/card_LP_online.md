工业界一般不会“每来一个请求就把完整 LP/MILP 从头解一遍”。常见做法是把问题拆成 **3 层：离线规划（solve）→ 近线控参（update dual）→ 在线决策（fast policy）**。核心思想：**在线用对偶变量（影子价格/拉格朗日乘子）把约束变成一个“可打分”的惩罚项**，这样实时只需要做一次打分/排序/贪心。

下面按你这个“资源约束 + 利润最大化”的范式讲（也适用于广告预算、流量分配、主播/大哥分配等）。

---

## 1）工业界怎么解：离线用 Solver，线上用“对偶价格”做快速决策

### A. 离线（Planning / Batch）：解一个大 LP/MILP

**场景**：供应链/排产、预算分配、每日流量规划、活动资源排期……
**做法**：

* 把未来一段时间（比如一天/一小时）的需求预测、产能、成本写成 **LP / MILP**；
* 用成熟求解器（商业/开源）跑出：

  * **Primal**：计划量/分配量（要做什么）
  * **Dual**：资源影子价格（约束有多“贵”）

**为什么离线适合 Solver**：变量和约束可能几十万/百万级，求最优或近似最优，Solver 很强，但不适合毫秒级响应。

> 你在例子里得到的 (u=0, v=10)（木材不稀缺、工时稀缺）就是最典型的“离线算影子价格”。

---

### B. 在线（Serving / Real-time）：不解 LP，用“对偶价格”打一分就决策

在线到来一个请求（一个订单/一个曝光/一个用户-主播候选集合），你只做：

[
\text{score}(a)=\widehat{r}(a\mid x);-;\sum_i \lambda_i;c_i(a\mid x)
]

* (a)：一个动作（生产一个桌子/给用户分配某主播/投放某广告）
* (\widehat{r})：预测收益（利润/EV/长期价值）
* (c_i)：消耗第 (i) 个资源（工时、预算、曝光、主播承接容量、公平约束…）
* (\lambda_i)：**对偶价格/惩罚系数**（资源紧张就变大，不紧张就接近 0）

然后选：
[
a^*=\arg\max_a \text{score}(a)
]

这一步就是一次 **打分 + 取最大/排序**，可以做到 **毫秒级**。

> 你例子里在线策略会变成：
> 桌子“净收益” = (50 - 0\cdot20 - 10\cdot5 = 0)（边际刚好打平）
> 椅子“净收益” = (30 - 0\cdot10 - 10\cdot4 = -10)（负的，不做）
> 所以在线会一直选桌子，直到工时打满。

---

## 2）在线如何“实时更新”对偶价格：pacing / primal-dual / MPC

在线最关键不是“选哪个动作”（打分很快），而是 **(\lambda) 怎么更新**，保证约束长期不被打爆。

### 方法 1：Pacing（最常见的工业闭环控制）

**广告预算/流量约束/产能约束**里极常用。
思想：如果资源用太快 → 提高 (\lambda)（变贵）→ 系统自然少用；反之降低。

一个简单可用的更新：
[
\lambda_i \leftarrow \Big[\lambda_i + \eta\cdot(\text{usage}_i-\text{target}*i)\Big]*+
]

* (\text{usage}_i)：最近窗口实际消耗（如过去 1 分钟用掉多少工时/预算）
* (\text{target}_i)：该窗口应该消耗多少（按总预算/总工时均匀或按预测曲线分配）
* (\eta)：步长
* ([\cdot]_+)：投影到非负

**工程实现**：

* 在线请求：只读当前 (\lambda)（内存里一个小向量）→ 打分/排序（<10ms）
* 另一个“控制线程/服务”：每 1～60 秒更新一次 (\lambda)

这就是“实时”的核心：**决策超快，参数慢更新**。

---

### 方法 2：Online Primal-Dual（理论味更重，但思想很工业）

每次来一个请求，像在线匹配/在线广告那类问题，会用“边做边更新对偶变量”的在线算法。实现上和 pacing 很像，只是更新规则可能更讲究（镜像下降/乘子更新/带正则）。

你可以把它理解成：
**在线近似解一个“随机到达”的大 LP**，对偶变量是系统的“状态”。

---

### 方法 3：MPC / Receding Horizon（近线重优化 + 在线执行）

**做法**：

* 每隔一小段时间（比如 5 分钟）重新解一次“未来 30 分钟”的 LP（warm start）
* 在接下来 5 分钟内用当前计划或对偶价在线执行

优点：能更好处理预测变化、突发峰值；缺点：需要更复杂的系统和稳定性设计。

---

## 3）如果约束里有整数/离散（MILP），在线怎么办？

很多工业分配是离散的（要么投放要么不投放、一个用户只能分配一个主播、一个订单只能给一个工厂），这会变成 **MILP**。

常见策略：

* **离线解 MILP / 用 LP 松弛得到 dual 价**；
* 在线用 dual 价打分做 **贪心/排序 + 约束过滤**；
* 必要时加一个很轻量的“修复器”（repair）：违反约束就回退到次优动作。

**关键点**：在线不追求严格全局最优，追求 **稳定、可控、可解释、能打 SLA**。

---

## 4）把这套话翻译成你更关心的“推荐/分配”语言

如果你的动作是“把某个大哥分配给某个主播/某个房间”，常见在线形式就是：

* 模型给 (\widehat{EV}(u, s))（给用户 u 推主播 s 的期望收益/长期价值）
* 约束/生态项（主播承接能力、曝光上限、集中度惩罚、公平、风险…）用 (c_i(u,s)) 表示
* 在线打分：
  [
  \widehat{EV}(u,s);-;\sum_i \lambda_i c_i(u,s)
  ]
* (\lambda) 由 pacing/控制器实时调，保证：

  * 不把曝光全灌给头部
  * 不让某主播过载
  * 预算/流量/生态指标在目标范围内

这就是工业界把“对偶”落地成“实时可用策略”的方式。

---

## 你要的是“实时”：一句话总结

* **离线/近线**：用 LP/MILP 求一遍（或松弛）拿到 **对偶价 (\lambda)**
* **在线**：每个请求只做 **一次打分/排序（带 (\lambda) 的惩罚项）**
* **实时控制**：每隔几秒/几分钟用 **pacing / primal-dual 更新 (\lambda)**

---

